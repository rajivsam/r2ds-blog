[
  {
    "objectID": "notebooks/coffee_prices_discretization.html",
    "href": "notebooks/coffee_prices_discretization.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import pandas as pd\nfp = \"../data/regimed_coffee_prices.csv\"\ndf = pd.read_csv(fp)\ndf\n\n\n\n\n\n\n\n\ndate\ncents_per_lb\nregime\n\n\n\n\n0\n1990-01-01\n75.830\nR-1\n\n\n1\n1990-02-01\n84.010\nR-1\n\n\n2\n1990-03-01\n93.960\nR-1\n\n\n3\n1990-04-01\n93.730\nR-1\n\n\n4\n1990-05-01\n92.020\nR-1\n\n\n...\n...\n...\n...\n\n\n417\n2024-10-01\n276.777\nR-8\n\n\n418\n2024-11-01\n304.953\nR-8\n\n\n419\n2024-12-01\n344.119\nR-8\n\n\n420\n2025-01-01\n353.933\nR-8\n\n\n421\n2025-02-01\n409.516\nR-8\n\n\n\n\n422 rows × 3 columns\n\n\n\n\nregimes = df[\"regime\"].unique().tolist()\n\nfor r in regimes:\n    reg_select = (df[\"regime\"] == r) # select each regime\n    df_reg = df[reg_select]\n    # the index contains the indices of each regime, discretize the prices with qcut and set\n    df.loc[df_reg.index, \"price\"] = pd.qcut(df_reg[\"cents_per_lb\"], 3, labels=[\"L\", \"M\", \"H\"])\n\n    \n\n    \n\n\nfor r in regimes:\n    reg_select = (df[\"regime\"] == r) # select each regime\n    df_reg = df[reg_select]\n    # maintain regime point count to mark the previous price for the first entry in each regime as na\n    rpc = 0\n    for ri, row in df_reg.iterrows():\n        if rpc == 0 :\n            rpc += 1\n            continue\n        else:\n            df.loc[ri, \"previous_price\"] = df.loc[ (ri -1), \"price\"]\n            rpc += 1\n\n\nsum(df[\"previous_price\"].isna()) \n\n8\n\n\n\nfp = \"../data/regimed_coffee_prices.csv\"\ndf.to_csv(fp, index=False)\n\n\nmatrix_dict = {}\nfor r in regimes:\n    reg_select = (df[\"regime\"] == r) # select each regime\n    df_reg = df[reg_select]\n    # the index contains the indices of each regime, discretize the prices with qcut and set\n    df_sm = pd.crosstab(df_reg.price, df_reg.previous_price)\n    # the next step normalizes the entry in each row by the row sum\n    df_sm = df_sm.div(df_sm.sum(axis=1), axis=0).round(3)\n    matrix_dict[r] = df_sm\n    fp = \"../data/stochastic_matrix_coffee_price-regime-\" + r + \".csv\"\n    df_sm.to_csv(fp, index=True)\n    \n\n\nmatrix_dict[\"R-5\"]\n\n\n\n\n\n\n\nprevious_price\nH\nL\nM\n\n\nprice\n\n\n\n\n\n\n\nL\n0.000\n0.778\n0.222\n\n\nM\n0.111\n0.278\n0.611\n\n\nH\n0.833\n0.000\n0.167\n\n\n\n\n\n\n\n\nimport plotly.express as px\n\nfig = px.imshow(matrix_dict[\"R-5\"], text_auto=True)\nfig.update_layout(\n    title={\n        'text': \"Stochastic Matrix for Region 5\",\n        'y':.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show()"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html",
    "href": "notebooks/retail_transaction_data.html",
    "title": "Data Preparation Notebook",
    "section": "",
    "text": "The data for this notebook is from Kaggle. The data looks like it is synthetic, the characteristics observed in the dataset also seem to suggest that. The objectives for this notebook are to:\nimport pandas as pd\nfp = \"../data/Retail_Transactions_Dataset.csv\"\ndf = pd.read_csv(fp)\ndf.head()\n\n\n\n\n\n\n\n\nTransaction_ID\nDate\nCustomer_Name\nProduct\nTotal_Items\nTotal_Cost\nPayment_Method\nCity\nStore_Type\nDiscount_Applied\nCustomer_Category\nSeason\nPromotion\n\n\n\n\n0\n1000000000\n2022-01-21 06:27:29\nStacey Price\n['Ketchup', 'Shaving Cream', 'Light Bulbs']\n3\n71.65\nMobile Payment\nLos Angeles\nWarehouse Club\nTrue\nHomemaker\nWinter\nNone\n\n\n1\n1000000001\n2023-03-01 13:01:21\nMichelle Carlson\n['Ice Cream', 'Milk', 'Olive Oil', 'Bread', 'P...\n2\n25.93\nCash\nSan Francisco\nSpecialty Store\nTrue\nProfessional\nFall\nBOGO (Buy One Get One)\n\n\n2\n1000000002\n2024-03-21 15:37:04\nLisa Graves\n['Spinach']\n6\n41.49\nCredit Card\nHouston\nDepartment Store\nTrue\nProfessional\nWinter\nNone\n\n\n3\n1000000003\n2020-10-31 09:59:47\nMrs. Patricia May\n['Tissues', 'Mustard']\n1\n39.34\nMobile Payment\nChicago\nPharmacy\nTrue\nHomemaker\nSpring\nNone\n\n\n4\n1000000004\n2020-12-10 00:59:59\nSusan Mitchell\n['Dish Soap']\n10\n16.42\nDebit Card\nHouston\nSpecialty Store\nFalse\nYoung Adult\nWinter\nDiscount on Selected Items"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#profile-the-categorical-columns",
    "href": "notebooks/retail_transaction_data.html#profile-the-categorical-columns",
    "title": "Data Preparation Notebook",
    "section": "Profile the categorical columns",
    "text": "Profile the categorical columns\n\ndf.columns\n\nIndex(['Transaction_ID', 'Date', 'Customer_Name', 'Product', 'Total_Items',\n       'Total_Cost', 'Payment_Method', 'City', 'Store_Type',\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion'],\n      dtype='object')\n\n\n\ncategory_cols = ['Payment_Method', 'City', 'Store_Type',\\\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion']\n\n\ndf[category_cols] = df[category_cols].astype('category')\n\n\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n\ndf.dtypes\n\nTransaction_ID                int64\nDate                 datetime64[ns]\nCustomer_Name                object\nProduct                      object\nTotal_Items                   int64\nTotal_Cost                  float64\nPayment_Method             category\nCity                       category\nStore_Type                 category\nDiscount_Applied           category\nCustomer_Category          category\nSeason                     category\nPromotion                  category\ndtype: object\n\n\n\ndf[\"Customer_Category\"].value_counts()\n\nSenior Citizen    125485\nHomemaker         125418\nTeenager          125319\nRetiree           125072\nStudent           124842\nProfessional      124651\nMiddle-Aged       124636\nYoung Adult       124577\nName: Customer_Category, dtype: int64\n\n\n\ndf[\"City\"].value_counts()\n\nBoston           100566\nDallas           100559\nSeattle          100167\nChicago          100059\nHouston          100050\nNew York         100007\nLos Angeles       99879\nMiami             99839\nSan Francisco     99808\nAtlanta           99066\nName: City, dtype: int64"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#observation",
    "href": "notebooks/retail_transaction_data.html#observation",
    "title": "Data Preparation Notebook",
    "section": "Observation",
    "text": "Observation\nThe counts for each of the metro areas are very similar, the counts for each of the customer categories are very similar, so this dataset was probably synthetically generated.\n\ndf[\"Product\"] = df[\"Product\"].apply(eval)\n\n\npurchase_summ = {}\nfor index, row in df[\"Product\"].items():\n    for p in row:\n        if p in purchase_summ:\n            purchase_summ[p] += 1\n        else:\n            purchase_summ[p] = 1"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#extract-the-ice-cream-transactions",
    "href": "notebooks/retail_transaction_data.html#extract-the-ice-cream-transactions",
    "title": "Data Preparation Notebook",
    "section": "Extract the ice cream transactions",
    "text": "Extract the ice cream transactions\n\ndef is_ice_cream(row):\n    for p in row:\n        if p == \"Ice Cream\":\n            return True\n    return False\ndf[\"is_ice_cream\"] = df[\"Product\"].apply(is_ice_cream)\n\n\ndf_ice_cream_trans = df[df[\"is_ice_cream\"]].reset_index()\nreq_cols = [\"Date\"]\ndf_ice_cream_trans = df_ice_cream_trans[req_cols]\n\n\ndf_ice_cream_trans[\"ice_cream_purchases\"] = 1\n\n\ndf_daily_ice_cream_sales = df_ice_cream_trans.set_index(\"Date\").resample(\"D\").sum()\n\n\ndf_weekly_max_ice_cream_sales = df_daily_ice_cream_sales.assign(year=df_daily_ice_cream_sales.index.year,\\\n                                week = df_daily_ice_cream_sales.index.isocalendar().week).groupby(['year','week']).max()\n\n\ndf_weekly_mean_ice_cream_sales = df_daily_ice_cream_sales.assign(year=df_daily_ice_cream_sales.index.year,\\\n                                week = df_daily_ice_cream_sales.index.isocalendar().week).groupby(['year','week']).mean()\n\n\ndf_weekly_max_ice_cream_sales = pd.DataFrame(df_weekly_max_ice_cream_sales.to_records()) \n\n\ndf_weekly_max_ice_cream_sales\n\n\n\n\n\n\n\n\nyear\nweek\nice_cream_purchases\n\n\n\n\n0\n2020\n1\n23\n\n\n1\n2020\n2\n26\n\n\n2\n2020\n3\n36\n\n\n3\n2020\n4\n31\n\n\n4\n2020\n5\n23\n\n\n...\n...\n...\n...\n\n\n225\n2024\n16\n33\n\n\n226\n2024\n17\n32\n\n\n227\n2024\n18\n30\n\n\n228\n2024\n19\n30\n\n\n229\n2024\n20\n30\n\n\n\n\n230 rows × 3 columns\n\n\n\n\ndf_weekly_mean_ice_cream_sales\n\n\n\n\n\n\n\n\n\nice_cream_purchases\n\n\nyear\nweek\n\n\n\n\n\n2020\n1\n21.800000\n\n\n2\n21.857143\n\n\n3\n26.000000\n\n\n4\n26.142857\n\n\n5\n19.571429\n\n\n...\n...\n...\n\n\n2024\n16\n21.571429\n\n\n17\n23.000000\n\n\n18\n20.857143\n\n\n19\n23.285714\n\n\n20\n23.000000\n\n\n\n\n230 rows × 1 columns"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#write-the-extracted-files-for-further-analysis",
    "href": "notebooks/retail_transaction_data.html#write-the-extracted-files-for-further-analysis",
    "title": "Data Preparation Notebook",
    "section": "Write the extracted files for further analysis",
    "text": "Write the extracted files for further analysis\n\nfpdaily = \"../data/daily_ice_cream_sales.csv\"\nfpmaxweekly = \"../data/max_weekly_ice_cream_sales.csv\"\nfpmeanweekly = \"../data/mean_weekly_ice_cream_sales.csv\"\ndf_daily_ice_cream_sales.to_csv(fpdaily, index=True)\ndf_weekly_max_ice_cream_sales.to_csv(fpmaxweekly, index=True)\ndf_weekly_mean_ice_cream_sales.to_csv(fpmeanweekly, index=True)"
  },
  {
    "objectID": "notebooks/coffee_prices_change_point.html",
    "href": "notebooks/coffee_prices_change_point.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nfp = \"../data/cp_data.csv\"\ndf = pd.read_csv(fp)\ncols = [\"date\", \"cents_per_lb\"]\ndf = df[cols]\n\n\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 422 entries, 0 to 421\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   date          422 non-null    datetime64[ns]\n 1   cents_per_lb  422 non-null    float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 6.7 KB\n\n\n\ndf\n\n\n\n\n\n\n\n\ndate\ncents_per_lb\n\n\n\n\n0\n1990-01-01\n75.830\n\n\n1\n1990-02-01\n84.010\n\n\n2\n1990-03-01\n93.960\n\n\n3\n1990-04-01\n93.730\n\n\n4\n1990-05-01\n92.020\n\n\n...\n...\n...\n\n\n417\n2024-10-01\n276.777\n\n\n418\n2024-11-01\n304.953\n\n\n419\n2024-12-01\n344.119\n\n\n420\n2025-01-01\n353.933\n\n\n421\n2025-02-01\n409.516\n\n\n\n\n422 rows × 2 columns\n\n\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.line(df, x='date', y=\"cents_per_lb\")\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\ncoffee prices have cycles as is evident from the above chart.\nEach hill like pattern we observe in the graph is a cycle. From visual inspection we can count 6 to 7 cycles in coffee prices between 1990 and the present.\nWe will have to apply rigorous statistical methodology to determine the end points of each cycle. The visual data exploration provides us a basic estimate of how many cycles we can expect to see in this data. The statistical modeling technique provides us the number of cycles.\nThe statistical methodology applied assumes that prices are a Gaussian Process. It applies the change point detection method developed by Killick and available in the ruptures python package. Please see this page for the gaussian process cost function implementation.\n\n\nimport ruptures as rpt\n#model = \"l1\"  # \"l2\", \"rbf\"\nalgo = rpt.Pelt(custom_cost=rpt.costs.CostNormal(), min_size=10).fit(df[\"cents_per_lb\"].values)\nmy_bkps = algo.predict(pen=200)\n\nConduct an experiment to determine how the number of change points changes with regularization penalty parameter. Killick suggests that this is ideally \\(k \\log(n)\\) where \\(n\\) is the number of data points (which is 422 here) and \\(k\\) is a small number. By running the experiment below and visually inspecting the number of cycles, a penalty value of 6 seems about right for this dataset. Edit: Now use a GP to model\n\nimport numpy as np\npen = np.arange(2,62,2)\npen_res = {}\nmodel = \"rbf\"  # \"l2\", \"rbf\"\nfor p in pen:\n    algo = rpt.Pelt(custom_cost=rpt.costs.CostNormal(), min_size=12).fit(df[\"cents_per_lb\"].values)\n    my_bkps = algo.predict(pen=p)\n    pen_res[p] = my_bkps\n\n    \n\n\nnum_bks = { int(p): len(pen_res[p]) for p in pen}\n\n\ndf_bkps = pd.DataFrame.from_dict(num_bks,orient='index').reset_index()\ndf_bkps.columns = [\"penalty\", \"num_breaks\"]\n\n\nfig = px.line(df_bkps, x='penalty', y='num_breaks', markers=True)\nfig.show()\n\n                            \n                                            \n\n\n\nnp.log2(df.shape[0])\n\nnp.float64(8.721099188707186)\n\n\nKillick’s paper recommends \\(k \\log(n)\\) for the penalty. Here \\(k\\) is a small constant and \\(n\\) is the number of datapoints. A plot of penalty versus the number of break points detected is shown in the above plot. A penalty of about \\(6\\) is consistent with what we observe visually in the plot of the time series.\n\nif 'pen_res' in globals():\n    del pen_res\nalgo = rpt.Pelt(custom_cost=rpt.costs.CostNormal(), min_size=12).fit(df[\"cents_per_lb\"].values)\nmy_bkps = algo.predict(pen=55)\n\n\nlen(my_bkps)\n\n8\n\n\n\nmy_bkps\n\n[50, 100, 125, 180, 235, 305, 375, 422]\n\n\n\nfor i in range(len(my_bkps)):\n    if i == 0:\n        df.loc[: my_bkps[i], \"regime\"] = \"R-\" + str(i+1)\n    elif ( ( i &gt; 0) and (i &lt;= (len(my_bkps) - 1))):\n        df.loc[ my_bkps[ (i - 1)] : my_bkps[i], \"regime\"] = \"R-\" + str(i+1)\n    else:\n        print(\"last breakpoint, do nothing!\")\n\n\ndf\n\n\n\n\n\n\n\n\ndate\ncents_per_lb\nregime\n\n\n\n\n0\n1990-01-01\n75.830\nR-1\n\n\n1\n1990-02-01\n84.010\nR-1\n\n\n2\n1990-03-01\n93.960\nR-1\n\n\n3\n1990-04-01\n93.730\nR-1\n\n\n4\n1990-05-01\n92.020\nR-1\n\n\n...\n...\n...\n...\n\n\n417\n2024-10-01\n276.777\nR-8\n\n\n418\n2024-11-01\n304.953\nR-8\n\n\n419\n2024-12-01\n344.119\nR-8\n\n\n420\n2025-01-01\n353.933\nR-8\n\n\n421\n2025-02-01\n409.516\nR-8\n\n\n\n\n422 rows × 3 columns\n\n\n\nA line plot that marks each regime with a separate color\n\nfig = px.line(df, x='date', y='cents_per_lb', color='regime', markers=True)\nfig.show()\n\n                            \n                                            \n\n\nA violin plot to confirm that each regime corresponds to a single density (not too multi modal) and summarize the ranges for each regime.\n\nfig = px.violin(df, y=\"cents_per_lb\", x=\"regime\", color=\"regime\", box=True, points=\"all\",\n          hover_data=df.columns)\nfig.show()\n\n                            \n                                            \n\n\nWrite the prepared datafile to disk.\n\ndf.groupby(\"regime\").agg( size = pd.NamedAgg(column=\"regime\", aggfunc=\"size\"),\\\n                         min_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"min\"),\\\n                        max_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"max\"),\\\n                        mean_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"mean\"),\\\n                        std_dev_price =  pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"std\")).round(2)\n\n\n\n\n\n\n\n\nsize\nmin_price\nmax_price\nmean_price\nstd_dev_price\n\n\nregime\n\n\n\n\n\n\n\n\n\nR-1\n50\n50.83\n94.92\n76.94\n12.83\n\n\nR-2\n50\n87.02\n265.61\n156.20\n37.78\n\n\nR-3\n25\n84.52\n138.94\n108.44\n12.72\n\n\nR-4\n55\n54.36\n104.39\n68.45\n9.94\n\n\nR-5\n55\n99.49\n157.29\n124.97\n14.43\n\n\nR-6\n70\n122.02\n300.48\n193.37\n46.80\n\n\nR-7\n70\n120.55\n184.12\n147.23\n15.16\n\n\nR-8\n47\n168.65\n409.52\n241.02\n47.27\n\n\n\n\n\n\n\n\nfp = \"../data/regimed_coffee_prices.csv\"\ndf.to_csv(fp, index=False, header=True)"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html",
    "href": "notebooks/daily_ice_cream_sales_exploration.html",
    "title": "Analysis Notebook",
    "section": "",
    "text": "The purpose of this notebook is to analyze the daily sales of ice creams at metro areas in the USA. Subsets corresponding to yearly sales are also profiled. Plots of auto-correlation and maximum daily sales per week are also provided.\nimport pandas as pd\nimport numpy as np\nfp = \"../data/daily_ice_cream_sales.csv\"\ndf = pd.read_csv(fp)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"log_daily_sales\"] = np.log(df[\"ice_cream_purchases\"])\ndf[\"Date\"].max()\n\nTimestamp('2024-05-18 00:00:00')\ndf[\"Date\"].min()\n\nTimestamp('2020-01-01 00:00:00')\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#extract-yearly-data-subsets",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#extract-yearly-data-subsets",
    "title": "Analysis Notebook",
    "section": "Extract yearly data subsets",
    "text": "Extract yearly data subsets\n\ndf_2020 = df[df.Date.dt.year == 2020]\ndf_2021 = df[df.Date.dt.year == 2021]\ndf_2022 = df[df.Date.dt.year == 2022]\ndf_2023 = df[df.Date.dt.year == 2023]\n\n\n%matplotlib inline\nimport statsmodels.api as sm\n\n\ndf_2020[\"log_daily_sales\"].plot()"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#plot-autocorrelation",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#plot-autocorrelation",
    "title": "Analysis Notebook",
    "section": "Plot Autocorrelation",
    "text": "Plot Autocorrelation\nThe auto correlation plots are shown below.\n\nsm.graphics.tsa.plot_acf(df_2020[\"log_daily_sales\"], lags=40)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsm.graphics.tsa.plot_pacf(df_2020[\"log_daily_sales\"], lags=40, method=\"ywm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsm.graphics.tsa.plot_pacf(df[\"log_daily_sales\"], lags=40, method=\"ywm\")"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#observation",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#observation",
    "title": "Analysis Notebook",
    "section": "Observation",
    "text": "Observation\nIt appears that daily sales are not correlated, but independent draws from a distribution. This looks like a white noise process. This is consistent with synthetically generated data. A dickey fuller test with no regression (constant, this is the default) also suggests the same.\n\nfrom statsmodels.tsa.stattools import adfuller\nresult = adfuller(df[\"log_daily_sales\"])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\n\nADF Statistic: -40.221392\np-value: 0.000000\n\n\n\ndf_2020\n\n\n\n\n\n\n\n\nDate\nice_cream_purchases\nlog_daily_sales\n\n\n\n\n0\n2020-01-01\n20\n2.995732\n\n\n1\n2020-01-02\n22\n3.091042\n\n\n2\n2020-01-03\n23\n3.135494\n\n\n3\n2020-01-04\n23\n3.135494\n\n\n4\n2020-01-05\n21\n3.044522\n\n\n...\n...\n...\n...\n\n\n361\n2020-12-27\n31\n3.433987\n\n\n362\n2020-12-28\n30\n3.401197\n\n\n363\n2020-12-29\n25\n3.218876\n\n\n364\n2020-12-30\n22\n3.091042\n\n\n365\n2020-12-31\n29\n3.367296\n\n\n\n\n366 rows × 3 columns\n\n\n\n\n\ndf[\"log_daily_sales\"] = np.log(df[\"ice_cream_purchases\"])\n\n\ndf_yearly_ice_cream_sales = df.set_index(\"Date\").resample(\"Y\").sum()\n\n\ndf_yearly_ice_cream_sales\n\n\n\n\n\n\n\n\nice_cream_purchases\nlog_daily_sales\n\n\nDate\n\n\n\n\n\n\n2020-12-31\n8488\n1142.662667\n\n\n2021-12-31\n8224\n1128.462655\n\n\n2022-12-31\n8206\n1127.813921\n\n\n2023-12-31\n8379\n1135.628790\n\n\n2024-12-31\n3188\n432.632764\n\n\n\n\n\n\n\n\ndf[\"ice_cream_purchases\"].plot()\n\n\n\n\n\n\n\n\n\nfpmeanweekly = \"../data/mean_weekly_ice_cream_sales.csv\"\ndfwmics = pd.read_csv(fpmeanweekly)\n\n\ndfwmics[\"ice_cream_purchases\"].plot()\n\n\n\n\n\n\n\n\n\ndfwmics[\"weekno\"] = range(1,dfwmics.shape[0] + 1)\n\n\ndfwmics\n\n\n\n\n\n\n\n\nyear\nweek\nice_cream_purchases\nweekno\n\n\n\n\n0\n2020\n1\n21.800000\n1\n\n\n1\n2020\n2\n21.857143\n2\n\n\n2\n2020\n3\n26.000000\n3\n\n\n3\n2020\n4\n26.142857\n4\n\n\n4\n2020\n5\n19.571429\n5\n\n\n...\n...\n...\n...\n...\n\n\n225\n2024\n16\n21.571429\n226\n\n\n226\n2024\n17\n23.000000\n227\n\n\n227\n2024\n18\n20.857143\n228\n\n\n228\n2024\n19\n23.285714\n229\n\n\n229\n2024\n20\n23.000000\n230\n\n\n\n\n230 rows × 4 columns"
  },
  {
    "objectID": "notebooks/coffee_prices.html",
    "href": "notebooks/coffee_prices.html",
    "title": "Observations",
    "section": "",
    "text": "import pandas as pd\nfp = \"../data/coffee_prices_index.csv\"\ndf = pd.read_csv(fp)\ndf.columns = [\"date\", \"cindex\"]\ndf[\"date\"] = pd.to_datetime(df.date)\ndf[\"cindex\"] = df[\"cindex\"].astype(float).round(3)\n%matplotlib inline\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nplt.plot(df.date,df.cindex)\nplt.grid(True)\nplt.xlabel(\"Year\")\nplt.ylabel(\"cents per pound\")\nplt.title (\"Price of mild arabica coffee\") # from https://fred.stlouisfed.org/series/PCOFFOTMUSDM\n#plt.xlabel(df[\"date\"])\n\nText(0.5, 1.0, 'Price of mild arabica coffee')\nsm.graphics.tsa.plot_acf(df.cindex, lags=24);\ncoffee_prices = pd.Series(df.cindex.values, index=df.date)\ncoffee_prices\n\ndate\n2014-12-01    193.386\n2015-01-01    189.626\n2015-02-01    178.888\n2015-03-01    160.736\n2015-04-01    163.998\n               ...   \n2024-08-01    261.438\n2024-09-01    278.760\n2024-10-01    276.777\n2024-11-01    304.953\n2024-12-01    344.119\nLength: 121, dtype: float64"
  },
  {
    "objectID": "notebooks/coffee_prices.html#observations",
    "href": "notebooks/coffee_prices.html#observations",
    "title": "Observations",
    "section": "Observations",
    "text": "Observations\nPost-covid, there has been a stronger trend and larger seasonal variations. We have been experiencing this at the grocery stores and every where else. It looks like pre-covid, though prices did have trend-cycles and seasonality, they were gradual and similar. I am not economist, so I don’t know the answers. The point here is that the right data tools can surface the problems that need analysis. It also provides a basis for determing the right characteristics we need to account for in downstream analysis like forecasting. Building predictive models without rigorous data analysis to document evidence for sources of variation we need to account for is like carpet bombing or driving blind. You are either using too much of computational sophistication, or, if you get a reasonable answer, you are just lucky that you picked a model that had the right features.\n\nfrom statsmodels.tsa.seasonal import STL\n\nstl = STL(coffee_prices, period=12)\nres = stl.fit()\n#fig = res.plot()\n\n\n\ndecomp_res = {\"Trend\": res._trend, \"Seasonality\": res._seasonal, \"Noise\": res._resid}\ndf_res = pd.DataFrame.from_dict(decomp_res, orient=\"columns\")\n\n\ndf_res = df_res.reset_index()\ndf_res\n\n\n\n\n\n\n\n\ndate\nTrend\nSeasonality\nNoise\n\n\n\n\n0\n2014-12-01\n177.846834\n7.022492\n8.516674\n\n\n1\n2015-01-01\n175.257465\n6.336066\n8.032469\n\n\n2\n2015-02-01\n172.751562\n1.939044\n4.197394\n\n\n3\n2015-03-01\n170.331065\n-5.153896\n-4.441169\n\n\n4\n2015-04-01\n167.995177\n-3.830315\n-0.166862\n\n\n...\n...\n...\n...\n...\n\n\n116\n2024-08-01\n268.822150\n-6.844100\n-0.540050\n\n\n117\n2024-09-01\n277.118703\n-1.991550\n3.632847\n\n\n118\n2024-10-01\n285.549310\n-10.645750\n1.873441\n\n\n119\n2024-11-01\n294.113028\n2.619736\n8.220236\n\n\n120\n2024-12-01\n302.806405\n25.075745\n16.236849\n\n\n\n\n121 rows × 4 columns\n\n\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.line(df_res, x='date', y=\"Trend\", title=\"Trend Cycle Component of Coffee Prices\",\n             labels = {\"Trend\": \"cents per pound\", \"date\": \"date\"})# Using plotly.express\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nfig = px.line(df_res, x='date', y=\"Seasonality\", title=\"Seasonality Component of Coffee Prices\",\n             labels = {\"Seasonality\": \"cents per pound\", \"date\": \"date\"})# Using plotly.express\nfig.show()\n\n                            \n                                            \n\n\n\nfig = px.line(df_res, x='date', y=\"Noise\", title=\"Noise Component of Coffee Prices\",\n             labels = {\"Noise\": \"cents per pound\", \"date\": \"date\"})# Using plotly.express\nfig.show()"
  },
  {
    "objectID": "notebooks/coffee_prices.html#errors-post-decomposition",
    "href": "notebooks/coffee_prices.html#errors-post-decomposition",
    "title": "Observations",
    "section": "Errors Post Decomposition",
    "text": "Errors Post Decomposition\n\nfrom matplotlib import pyplot as plt\nres.resid.plot.kde() # looks reasonable\nplt.grid(True)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog, where I explore a variety of topics that pique my interest, including:\n\nAnalyzing, summarizing, and optimizing large datasets for business machine learning tasks.\nDeveloping discrete and continuous optimization models for business applications.\nLeveraging graph data models and algorithms to address business challenges.\nApplying statistical modeling techniques to solve business problems."
  },
  {
    "objectID": "posts/markov_analysis_coffee_prices/index.html",
    "href": "posts/markov_analysis_coffee_prices/index.html",
    "title": "Understanding Coffee Prices",
    "section": "",
    "text": "The question of whether I will continue paying these high prices for coffee was a “nerd itch” I simply had to scratch. Below is a plot showing the coffee prices (in cents per pound) reported at a monthly frequency.\n\n\n\nRaw Coffee Prices\n\n\nAs mentioned in the previous post on coffee prices, it’s evident that these prices follow distinct cycles in which prices rise to a peak and then decline. While some peaks are more pronounced than others, focusing on the major ones reveals seven to eight clear cycles between 1990 and the present. To identify the beginning and end of these cycles, we can use a change point detection algorithm. In time series analysis, the process of dividing a series into segments with similar behavior is known as segmentation.\n\n\n\n\n\n\nNote\n\n\n\nI have not really explained how I arrive at the number of \\(7-8\\) changes in this post. This comes from an analysis of the decomposition of the time series into trend and seasonal components and a subsequent review of these components. If you are interested in the details, please check out the next post. An explaination is provided there.\n\n\nThe modeling approach we choose depends largely on how certain we are about the number of segments in the data. In some cases, this number is known with confidence; in others, we start with an estimate that must be refined through modeling. For the coffee price series, we have a fairly narrow estimate—between seven and eight segments. To determine the exact number and placement of these segments, we use a model selection technique. In essence, model selection helps us refine our estimate and identify the segmentation that best fits the data.\nTo identify segments in the coffee price time series, we use the PELT algorithm (Pruned Exact Linear Time)(Killick, Fearnhead, and and 2012). For a broader overview of change point detection methods, see (Truong, Oudre, and Vayatis 2020). The change point detection problem can be described as follows: we have a sequence of signal values—in this case, monthly coffee prices. Typically, prices from one month to the next are relatively consistent, but occasionally, a significant shift occurs, indicating the start of a new pattern. The goal of change point detection is to identify these moments of transition. Each segment between change points represents a distinct period of similar price behavior, also known as a price regime. When a change point is detected, it marks the beginning of a new regime.\nA statistical model is assigned to each segment—for example, a probabilistic model where each segment is assumed to be a Gaussian Process. To prevent overfitting, a regularization or penalty term is included in the model. The parameters, along with the optimal segmentation, are determined by solving an optimization problem. For a comprehensive overview of this approach, see (Truong, Oudre, and Vayatis 2020), and for details on the specific change point detection method used here, refer to (Killick, Fearnhead, and and 2012). The method is briefly summarized in the equation below.\n\\(\\sum_{i=1}^{i=m} \\text{segment cost}_i + \\beta * \\text{penalty}\\)\nIf we have \\(m\\) change points in the time series, then we have \\((m+1)\\) segments. Each seqment contributes a model component. This is the segment cost. To prevent overfitting we introduce a penalty, actually, a penalty function, \\(f(n)\\), where \\(n\\) is the length of the time series. One suggestion for \\(f(n)\\) is \\(\\log(n)\\). (Killick, Fearnhead, and and 2012) suggests \\(\\beta\\) to be a small number.\nWe need to adjust the penalty parameter to be consistent with our data. This is the crux of the model selection problem. The plot below shows the number of breakpoints (and consequently the number of cycles) as a function of the penalty. A penalty of \\(6\\) produces a set of breakpoints that is consistent with our data.\n\n\n\nNumber of Break Points vs Penalty\n\n\n\n\n\n\n\n\nCorrection\n\n\n\nThere is an edit to the previous version of the post. In the previous version I had mentioned that a Gaussian kernel is used as model for the segments. This is not a reasonable choice because the points in the series are not IID (identical, independently distributed), a Gaussian Process model is a much more reasonable choice. This model captures the correlation of values between data points. As a result of this change, the results from the modeling are different from what resulted from the Gaussian kernel.\n\n\nUsing a Gaussian Process for each segment and a penalty value of \\(55\\), we get a segmentation that is shown below.\n\n\n\nChange Point Plot\n\n\nEach segment, or regime, is a sequence of months with similar prices. The summary statistics of the segments is shown below. The size column represents the length of the segment in months.\n\n\n\nRegime Summary\n\n\nA violin plot of the prices during each of the cycles is shown below\n\n\n\nViolin Plot of Prices\n\n\nIt is evident that each regime has a unimodal distribution and the Gaussian assumption seems reasonable. Strictly speaking, a Q-Q plot and a goodness of fit test are warranted.\nSegmentation gives us time periods where prices can be explained by a particular random component, say a straight line with a particular slope, or, as in this example, samples from a particular Gaussian family. The goal here is to understand behavior and identify characteristics of coffee prices, as apposed to an overt task like forecasting (Can we predict next month’s price accurately?). In other words, this is an analysis exercise as opposed to a strict model development exercise. We need a modeling method that summarize the stochastic characteristics of each regime or segment and in the process reveal insights about how prices behaved in that segment. Markov models can do this for us.\nThe approach taken here is to model prices as a markov chain. In fact, a simplification is applied. The prices for each regime are first quantized into three discrete levels: Low, Medium and High, by applying a n-tile function to the prices for that regime. As a consequence, the sequence for each regime is a sequence of discretized labels. These are the states of the markov chain for each regime. We can compute a transition matrix corresponding to these states for each regime. The transition matrix is a contingency table between successive states in each regime. Such a table can be represented as a matrix. The \\((i,j)^{\\text{th}}\\) entry of this table represents the count of transitions between predecessor state \\(i\\) and successor state \\(j\\). When we normalize the entries of this matrix by the row sum, we have the transition probablities for the regime. This is a stochastic matrix.\nMarkov chains are widely used to study, understand, and model stochastic processes across a range of fields, including economics, science, and engineering. Their versatility and broad applicability make them a powerful tool for analyzing systems governed by probabilistic behavior. For a comprehensive introduction to Markov chains, see:\n\nThe QuantEcon website(QuantEcon 2025)\nMatthew Aldridge’s course (Aldridge 2025), “Introduction to Markov Processes”\nJason Bramburger’s course (Bramburger 2025), “Introduction to Mathematical Modeling”\n\nThe stochastic matrix for the current period (now) is shown below.\n\n\n\nStochastic Matrix, R8\n\n\nIf the stochastic matrix is irreducible —meaning that it is possible to transition from any state to any other state in a finite number of steps—then the corresponding regime has a stationary distribution. This distribution represents the long-run probabilities of the prices being in each of the three states within that regime. For the current period, the stochastic matrix is indeed irreducible, and its associated stationary distribution is as follows:\n\n\n\nStationary Probablities, R8\n\n\nNow that the modeling has been described, what do the results suggest. Here is what I was able to pick.\n\nCycle Lengths Vary Widely: Coffee price cycles can range from as short as 25 months to as long as 70 months.\nRising Volatility Since 2005: The variability in prices within each cycle—measured by the standard deviation—has been increasing since around 2005 (beginning with regime R5). This trend aligns with major global disruptions such as the financial crisis, the COVID-19 pandemic, and the Ukraine conflict. The rise in variance is also clearly visible in the violin plots. Increased volatility implies that businesses could potentially reduce costs by stockpiling coffee when demand is stable, though practical constraints and regulations may limit this strategy.\nCoffee Is Currently Expensive: We are presently in a high-price regime—bad news for coffee drinkers.\nStationary Distributions Provide Insight: The stationary distribution of a regime captures the long-term probability of prices falling into different tiers. In the current regime, for example, prices are expected to be in the lowest third of the historical range 47% of the time, in the middle third 31% of the time, and in the highest third 22% of the time.\nLarge Price Swings Are Rare: Within a regime, large jumps between low and high price levels are uncommon. Transitions typically occur gradually—for example, from low to medium before reaching high. This pattern means sudden, sharp price changes from one day to the next are unlikely.\n\nThese are some of the observations I could make. If you can identify others, please let me know along with a justification and rationale for your suggestion.\nThe code for this post is available. These are grouped as follows:\n\nDownload Data\nIdentify Changepoints\nDiscretize Prices in Regimes\nMarkov Analysis"
  },
  {
    "objectID": "posts/markov_analysis_coffee_prices/index.html#analysis-of-coffee-prices-with-segmentation-and-markov-chains",
    "href": "posts/markov_analysis_coffee_prices/index.html#analysis-of-coffee-prices-with-segmentation-and-markov-chains",
    "title": "Understanding Coffee Prices",
    "section": "",
    "text": "The question of whether I will continue paying these high prices for coffee was a “nerd itch” I simply had to scratch. Below is a plot showing the coffee prices (in cents per pound) reported at a monthly frequency.\n\n\n\nRaw Coffee Prices\n\n\nAs mentioned in the previous post on coffee prices, it’s evident that these prices follow distinct cycles in which prices rise to a peak and then decline. While some peaks are more pronounced than others, focusing on the major ones reveals seven to eight clear cycles between 1990 and the present. To identify the beginning and end of these cycles, we can use a change point detection algorithm. In time series analysis, the process of dividing a series into segments with similar behavior is known as segmentation.\n\n\n\n\n\n\nNote\n\n\n\nI have not really explained how I arrive at the number of \\(7-8\\) changes in this post. This comes from an analysis of the decomposition of the time series into trend and seasonal components and a subsequent review of these components. If you are interested in the details, please check out the next post. An explaination is provided there.\n\n\nThe modeling approach we choose depends largely on how certain we are about the number of segments in the data. In some cases, this number is known with confidence; in others, we start with an estimate that must be refined through modeling. For the coffee price series, we have a fairly narrow estimate—between seven and eight segments. To determine the exact number and placement of these segments, we use a model selection technique. In essence, model selection helps us refine our estimate and identify the segmentation that best fits the data.\nTo identify segments in the coffee price time series, we use the PELT algorithm (Pruned Exact Linear Time)(Killick, Fearnhead, and and 2012). For a broader overview of change point detection methods, see (Truong, Oudre, and Vayatis 2020). The change point detection problem can be described as follows: we have a sequence of signal values—in this case, monthly coffee prices. Typically, prices from one month to the next are relatively consistent, but occasionally, a significant shift occurs, indicating the start of a new pattern. The goal of change point detection is to identify these moments of transition. Each segment between change points represents a distinct period of similar price behavior, also known as a price regime. When a change point is detected, it marks the beginning of a new regime.\nA statistical model is assigned to each segment—for example, a probabilistic model where each segment is assumed to be a Gaussian Process. To prevent overfitting, a regularization or penalty term is included in the model. The parameters, along with the optimal segmentation, are determined by solving an optimization problem. For a comprehensive overview of this approach, see (Truong, Oudre, and Vayatis 2020), and for details on the specific change point detection method used here, refer to (Killick, Fearnhead, and and 2012). The method is briefly summarized in the equation below.\n\\(\\sum_{i=1}^{i=m} \\text{segment cost}_i + \\beta * \\text{penalty}\\)\nIf we have \\(m\\) change points in the time series, then we have \\((m+1)\\) segments. Each seqment contributes a model component. This is the segment cost. To prevent overfitting we introduce a penalty, actually, a penalty function, \\(f(n)\\), where \\(n\\) is the length of the time series. One suggestion for \\(f(n)\\) is \\(\\log(n)\\). (Killick, Fearnhead, and and 2012) suggests \\(\\beta\\) to be a small number.\nWe need to adjust the penalty parameter to be consistent with our data. This is the crux of the model selection problem. The plot below shows the number of breakpoints (and consequently the number of cycles) as a function of the penalty. A penalty of \\(6\\) produces a set of breakpoints that is consistent with our data.\n\n\n\nNumber of Break Points vs Penalty\n\n\n\n\n\n\n\n\nCorrection\n\n\n\nThere is an edit to the previous version of the post. In the previous version I had mentioned that a Gaussian kernel is used as model for the segments. This is not a reasonable choice because the points in the series are not IID (identical, independently distributed), a Gaussian Process model is a much more reasonable choice. This model captures the correlation of values between data points. As a result of this change, the results from the modeling are different from what resulted from the Gaussian kernel.\n\n\nUsing a Gaussian Process for each segment and a penalty value of \\(55\\), we get a segmentation that is shown below.\n\n\n\nChange Point Plot\n\n\nEach segment, or regime, is a sequence of months with similar prices. The summary statistics of the segments is shown below. The size column represents the length of the segment in months.\n\n\n\nRegime Summary\n\n\nA violin plot of the prices during each of the cycles is shown below\n\n\n\nViolin Plot of Prices\n\n\nIt is evident that each regime has a unimodal distribution and the Gaussian assumption seems reasonable. Strictly speaking, a Q-Q plot and a goodness of fit test are warranted.\nSegmentation gives us time periods where prices can be explained by a particular random component, say a straight line with a particular slope, or, as in this example, samples from a particular Gaussian family. The goal here is to understand behavior and identify characteristics of coffee prices, as apposed to an overt task like forecasting (Can we predict next month’s price accurately?). In other words, this is an analysis exercise as opposed to a strict model development exercise. We need a modeling method that summarize the stochastic characteristics of each regime or segment and in the process reveal insights about how prices behaved in that segment. Markov models can do this for us.\nThe approach taken here is to model prices as a markov chain. In fact, a simplification is applied. The prices for each regime are first quantized into three discrete levels: Low, Medium and High, by applying a n-tile function to the prices for that regime. As a consequence, the sequence for each regime is a sequence of discretized labels. These are the states of the markov chain for each regime. We can compute a transition matrix corresponding to these states for each regime. The transition matrix is a contingency table between successive states in each regime. Such a table can be represented as a matrix. The \\((i,j)^{\\text{th}}\\) entry of this table represents the count of transitions between predecessor state \\(i\\) and successor state \\(j\\). When we normalize the entries of this matrix by the row sum, we have the transition probablities for the regime. This is a stochastic matrix.\nMarkov chains are widely used to study, understand, and model stochastic processes across a range of fields, including economics, science, and engineering. Their versatility and broad applicability make them a powerful tool for analyzing systems governed by probabilistic behavior. For a comprehensive introduction to Markov chains, see:\n\nThe QuantEcon website(QuantEcon 2025)\nMatthew Aldridge’s course (Aldridge 2025), “Introduction to Markov Processes”\nJason Bramburger’s course (Bramburger 2025), “Introduction to Mathematical Modeling”\n\nThe stochastic matrix for the current period (now) is shown below.\n\n\n\nStochastic Matrix, R8\n\n\nIf the stochastic matrix is irreducible —meaning that it is possible to transition from any state to any other state in a finite number of steps—then the corresponding regime has a stationary distribution. This distribution represents the long-run probabilities of the prices being in each of the three states within that regime. For the current period, the stochastic matrix is indeed irreducible, and its associated stationary distribution is as follows:\n\n\n\nStationary Probablities, R8\n\n\nNow that the modeling has been described, what do the results suggest. Here is what I was able to pick.\n\nCycle Lengths Vary Widely: Coffee price cycles can range from as short as 25 months to as long as 70 months.\nRising Volatility Since 2005: The variability in prices within each cycle—measured by the standard deviation—has been increasing since around 2005 (beginning with regime R5). This trend aligns with major global disruptions such as the financial crisis, the COVID-19 pandemic, and the Ukraine conflict. The rise in variance is also clearly visible in the violin plots. Increased volatility implies that businesses could potentially reduce costs by stockpiling coffee when demand is stable, though practical constraints and regulations may limit this strategy.\nCoffee Is Currently Expensive: We are presently in a high-price regime—bad news for coffee drinkers.\nStationary Distributions Provide Insight: The stationary distribution of a regime captures the long-term probability of prices falling into different tiers. In the current regime, for example, prices are expected to be in the lowest third of the historical range 47% of the time, in the middle third 31% of the time, and in the highest third 22% of the time.\nLarge Price Swings Are Rare: Within a regime, large jumps between low and high price levels are uncommon. Transitions typically occur gradually—for example, from low to medium before reaching high. This pattern means sudden, sharp price changes from one day to the next are unlikely.\n\nThese are some of the observations I could make. If you can identify others, please let me know along with a justification and rationale for your suggestion.\nThe code for this post is available. These are grouped as follows:\n\nDownload Data\nIdentify Changepoints\nDiscretize Prices in Regimes\nMarkov Analysis"
  },
  {
    "objectID": "posts/coffee_prices/index.html",
    "href": "posts/coffee_prices/index.html",
    "title": "Predictive Modeling without Supporting Data Analysis",
    "section": "",
    "text": "In my opinion, relying on models without prior data analysis is problematic. This is particularly true for business applications that work with regular tabular data. While it’s common practice to develop separate models for prediction and explanation, preliminary data analysis is crucial for justifying: 1. Specific modeling approaches 2. The necessity of certain features\nThis isn’t just about generating summary statistics or assessing data quality; it’s about understanding the sources of variation relevant to our predictions or estimates.\nAs a coffee connoisseur, you may have noticed the rising costs of your favorite brew. Imagine you own a small restaurant chain and need to plan your coffee purchases to manage production costs effectively. To forecast coffee prices for the next six months, you hire a data science team to build a predictive model. However, it’s essential to first understand the primary sources of price variation and any recent changes in price patterns before developing an accurate model.\nOver the past decade, the quality of libraries for statistical modeling and optimization has significantly improved. Coupled with code generation features like those provided by GitHub Copilot, the effort and cost of conducting data analysis have decreased substantially—provided you have the data analysis skills. Unfortunately, it is quite common to run into conversations today where people believe that throwing the data at an “automatic modeling” tool will give you the answer and there is really no need to understand how the result was obtained.\nFor example, here is how easy tools make this for the coffee example. Here is the raw series  Here is the decomposition of this series with LOWESS. The details of the notebook with the observations are here."
  },
  {
    "objectID": "posts/coffee_prices/index.html#the-black-box-model",
    "href": "posts/coffee_prices/index.html#the-black-box-model",
    "title": "Predictive Modeling without Supporting Data Analysis",
    "section": "",
    "text": "In my opinion, relying on models without prior data analysis is problematic. This is particularly true for business applications that work with regular tabular data. While it’s common practice to develop separate models for prediction and explanation, preliminary data analysis is crucial for justifying: 1. Specific modeling approaches 2. The necessity of certain features\nThis isn’t just about generating summary statistics or assessing data quality; it’s about understanding the sources of variation relevant to our predictions or estimates.\nAs a coffee connoisseur, you may have noticed the rising costs of your favorite brew. Imagine you own a small restaurant chain and need to plan your coffee purchases to manage production costs effectively. To forecast coffee prices for the next six months, you hire a data science team to build a predictive model. However, it’s essential to first understand the primary sources of price variation and any recent changes in price patterns before developing an accurate model.\nOver the past decade, the quality of libraries for statistical modeling and optimization has significantly improved. Coupled with code generation features like those provided by GitHub Copilot, the effort and cost of conducting data analysis have decreased substantially—provided you have the data analysis skills. Unfortunately, it is quite common to run into conversations today where people believe that throwing the data at an “automatic modeling” tool will give you the answer and there is really no need to understand how the result was obtained.\nFor example, here is how easy tools make this for the coffee example. Here is the raw series  Here is the decomposition of this series with LOWESS. The details of the notebook with the observations are here."
  },
  {
    "objectID": "posts/hmm_r8/index.html",
    "href": "posts/hmm_r8/index.html",
    "title": "Using Hidden Markov Models with the Coffee Prices Data",
    "section": "",
    "text": "I started writing about coffee prices kind of on a whim—mostly because I wanted to try out this tool called quarto that I’d heard good things about. At the same time, I was curious to see how useful ChatGPT would be when dealing with tabular data, since most of the hype is around text and image/video stuff.\nAlong the way, I ended up realizing just how much the open source community has given to the data science world—there’s a ton of amazing tools out there. For most projects, I’ve found that the bulk of my time goes into exploring the data and evaluating different ways to frame the problem. Trying out different approaches and prepping the data for each one—that’s what really eats up modeling time.\nI’m not really in the “one model to rule them all” camp—first it was deep learning, now it’s generative AI. I’m definitely curious to see what gen AI can do, but I’m keeping a healthy dose of skepticism.\nAnyway, while I was digging into all this, I stumbled across some pretty neat libraries—no regrets. One of them was hmmlearn, which is great for working with Hidden Markov Models. This post is really just me wrapping up what I started—gotta close the loop!\nHere’s a brief recap of the story so far: we can use time series decomposition to break down the coffee price data into three main components — trend-cycle, seasonal, and noise.\nIt’s worth noting that people often confuse cycles with seasonality. The key difference is that cycles don’t follow a fixed or precisely defined period, whereas seasonal patterns do. For example, a weekly seasonal pattern repeats exactly every week.\nThis distinction is why some authors — such as Hyndman (Hyndman and Athanasopoulos 2018) — prefer the term trend-cycle instead of simply trend. The decomposition process typically captures both long-term trends and irregular cycles as a single trend-cycle component. In contrast, the seasonal component represents consistent, repeating patterns and is treated separately.\n\n\n\n\n\n\n\n\n\nRaw Prices (monthly)\n\n\n\n\n\n\n\nTrend-Cycle\n\n\n\n\n\n\n\n\n\nSeasonality\n\n\n\n\n\nThe decomposition of the coffee time series using LOESS(Cleveland et al. 1990) is shown in the panel above. A review of the trend-cycle component reveals approximately \\(4\\) major cycles. The seasonal component reflects noticeable shifts beginning around 2020, likely due to the impact of COVID. I was able to identify at least \\(3\\) distinct seasonal patterns during the period.\nAltogether, this led me to expect around \\(7\\) significant changes in the series — \\(4\\) from the trend-cycle and \\(3\\)–\\(4\\) from seasonal variations — dating back to the start of the series in 1990.\nIf you’re a commodities pricing expert and you see additional changes beyond these, I’d be very interested in learning more about your interpretation.\nThe residuals — the remaining variation after removing both trend and seasonal effects — are shown in the panel below.\n\n\n\nErrors\n\n\nRunning change point analysis gave us the changepoints represented in the series below\n\n\n\nChange Points\n\n\nIn the previous post, I mentioned that Markov Models are powerful tools for modeling stochastic processes. By discretizing regime prices into three levels — (low, medium, high) — we can apply the theory of Markov chains to estimate the probability of being in each state, known as the stationary probability.\nIn this post, I’d like to complete the loop by introducing a practical extension for monitoring sequential data like this: the Hidden Markov Model.\nInstead of discretizing prices into fixed categories like (low, medium, high), we can let the data guide the analysis using a kernel density estimator (or a histogram — here, I’ve used a KDE). In other words, the discretization is data-driven rather than predefined.\nThis analysis focuses on Regime 8, which corresponds to the current period in coffee prices. A kernel density plot of these prices is shown below.\n\n\n\nCoffee Prices, R8\n\n\nA look at the KDE above reveals that coffee prices tend to concentrate around specific values. In this case, the distribution appears bi-modal — suggesting two distinct clusters of prices. Rather than arbitrarily discretizing prices into three levels, it’s more effective to first histogram the data and then estimate the number of underlying components.\nWhat we uncover here is a 2-state Hidden Markov Model. Coffee prices are clustering around two primary levels, but the cluster membership is latent — we don’t observe it directly. Instead, what we observe at each time step is a noisy sample from one of these clusters.\nYou can think of this like monitoring patients in a hospital ward where only two health conditions are possible: condition A or condition B. As a healthcare worker doing daily rounds, you don’t get to see a label revealing a patient’s condition — instead, you observe noisy indicators like temperature and vital signs, and from those, you infer the likely condition.\nSimilarly, each month we observe a single coffee price reported by the time series publisher. Based on that value — a noisy sample — we can infer which price cluster (or state) the market is in.\nI’m not going into the details of how the Hidden Markov Model is estimated from the data here. For that, I referred to Bishop’s book (Bishop 2006), which offers a solid conceptual introduction. For those eager to dive deeper, I recommend (Rabiner 1989) and (Bilmes et al. 1998).\nBy the way, if you’re interested in Graphical Models, Bishop’s book and Jeff Bilmes’ lecture series on YouTube are both excellent resources.\nThe HMM model estimates these two clusters for us. The kernel density estimates are shown below\n\n\n\n\n\n\n\n\ncluster 0, size = 27\n\n\n\n\n\n\n\ncluster 1, size = 13\n\n\n\n\n\n\nFigure 1: Clusters from HMM Estimation\n\n\n\nThe density plots reveal that cluster 0 is characterized by a lower mean and higher variance, while cluster 1 has a higher mean and lower variance.\nIn total, there are \\(47\\) data points in the current regime (Regime 8). The first \\(40\\) points were used to estimate the Hidden Markov Model, and the fitted model was then applied to predict the cluster membership for the remaining \\(7\\) data points.\nThis is illustrated in the figure below.\n\n\n\nHMM Prediction of Price Clusters\n\n\nWhile Hidden Markov Models may not be state-of-the-art for prediction tasks, they remain incredibly useful for monitoring and understanding the underlying structure of a time series. Personally, I believe the insights they offer make them well worth the time investment.\nThat’s a wrap on the coffee prices series — thanks for following along!\n\n\n\n\n\nReferences\n\nBilmes, Jeff A et al. 1998. “A Gentle Tutorial of the EM Algorithm and Its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models.” International Computer Science Institute 4 (510): 126.\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning. Vol. 4. Springer New York. http://scholar.google.com/scholar.bib?q=info:jYxggZ6Ag1YJ:scholar.google.com/&output=citation&hl=en&as_sdt=0,5&as_vis=1&ct=citation&cd=0.\n\n\nCleveland, Robert B, William S Cleveland, Jean E McRae, Irma Terpenning, et al. 1990. “STL: A Seasonal-Trend Decomposition.” J. Off. Stat 6 (1): 3–73.\n\n\nDiebold, F. X. 2007. Elements of Forecasting. Thomson/South-Western. https://books.google.co.in/books?id=j2_HtQEACAAJ.\n\n\nDiebold, Francis X, and Glenn D Rudebusch. 2020. “Business Cycles: Durations, Dynamics, and Forecasting.”\n\n\nHyndman, Rob J, and George Athanasopoulos. 2018. Forecasting: Principles and Practice. OTexts.\n\n\nRabiner, Lawrence R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” Proceedings of the IEEE 77 (2): 257–86.\n\n\nShumway, Robert H, David S Stoffer, and David S Stoffer. 2000. Time Series Analysis and Its Applications. Vol. 3. Springer.\n\nCitationBibTeX citation:@online{sambasivan2025,\n  author = {Sambasivan, Rajiv},\n  title = {Using {Hidden} {Markov} {Models} with the {Coffee} {Prices}\n    {Data}},\n  date = {2025-04-24},\n  url = {https://rajivsam.github.io/r2ds-blog/posts/markov_analysis_coffee_prices/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSambasivan, Rajiv. 2025. “Using Hidden Markov Models with the\nCoffee Prices Data.” April 24, 2025. https://rajivsam.github.io/r2ds-blog/posts/markov_analysis_coffee_prices/."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html",
    "href": "posts/ice_cream_sales/index.html",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "For a while now I have been meaning to explore how Copilot and Chat GPT are useful for data analysis. I have also been meaning to transition to Quarto for a while, so I grabbed a dataset from Kaggle, picked some interesting questions to analyze, and threw it at Copilot and Chat GPT. Here’s my experience. I used Chat GPT to generate the content initially, so what you see here is my own writing interlaced with generated content. I used VSCode with extensions for quarto and copilot for this article.\nBy leveraging Chat GPT, we can automate and enhance various aspects of time series analysis, including data preprocessing, model selection, and interpretation of results.\n\n\n\n\n\n\nNote\n\n\n\nWhile the above statement is true, it can be somewhat misleading. The assistance we get for the tasks like data preprocessing in comparison to tasks like result interpretation and model selection is very different. Code generation for manipulating and transforming pandas dataframes was useful, I did’t really find anything useful for interpretation or model selection.\n\n\n\n\nThe data analyzed in this post is a collection of transactions from a national retailer. The transactions span major metropolitan cities in the US and cover a four and half year period. The dataset is available on Kaggle. The objective was to analyze this data to get answers to a representative set of questions. The questions addressed in this post represent a subset of a comprehensive analysis exercise, and it is not intended to be exhaustive or rigorous. Some example questions include:\n\nWhat does the plot of daily ice cream sales nationwide over the analysis period look like?\nHow do ice cream sales vary across the four years within the analysis period? Are there significant differences in the summary statistics year over year?\nIs there variability in ice cream sales based on location? In other words, is there heterogeneity in consumption patterns based on location?\nWhat does the autocorrelation function of the ice cream sales indicate? Is the time series stationary?\nWhat are the key components of the time series (trend, seasonality, etc.)?\nHow does the plot of the maximum daily ice cream sales for each week of the year look? Can we model these maximum values using a Gumbel distribution to understand the distribution of peak daily demand nationally?\n\n\n\n\n\n\n\nNote\n\n\n\nThe relevant analysis tasks (the list above) is something the analyst has to generate. In my view, this comes with experience with similar data and use cases. Asking Chat GPT on how to do time series analysis got me very general guidelines. I did not find the response very useful (see below). The ACF suggests a white noise process (which is the ground truth), Chat GPT was trying find patterns in it. A notebook implementation of most of these questions is available\n\n\n\n\n\nChat GPT Analysis\n\n\n\n\n\nTransforming the raw dataset to a format that was suitable to answer the analysis questions required an understanding of the various fields in the data. To get the daily ice cream sales from the raw transaction data, I had the identify the transactions that contained ice creams and then aggregate the transactions on a daily basis. Doing this required an analysis of how the line items for each transaction is encoded. Once the data description was analyzed, providing the prompts to Chat GPT generated code that was pretty close to what I developed manually.\n\n\n\n\n\n\nNote\n\n\n\nThe real effort in the data preparation task went into developing an understanding of the various elements of the raw dataset and then coming up with the process of identifying the transactions containing ice cream. Aggregating these transactions to a daily or weekly cadence was fairly straight forward. Chat GPT was helpful in generating the specific pandas code for each step. The manual and the generated were pretty close.\n\n\n\n\n\n\nChat GPT was definitely very helpful in developing and organizing the content.\nIdentifying the key questions that are relevant to the analysis requires formal training and experience - I don’t really see this replacing a data scientist at this point.\nChat GPT was definitely useful for various code generation tasks.\nIn summary, it was impressive as an assistant, but at this point, it is an assistant not a data scientist replacement."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#the-time-series-analysis-task",
    "href": "posts/ice_cream_sales/index.html#the-time-series-analysis-task",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "The data analyzed in this post is a collection of transactions from a national retailer. The transactions span major metropolitan cities in the US and cover a four and half year period. The dataset is available on Kaggle. The objective was to analyze this data to get answers to a representative set of questions. The questions addressed in this post represent a subset of a comprehensive analysis exercise, and it is not intended to be exhaustive or rigorous. Some example questions include:\n\nWhat does the plot of daily ice cream sales nationwide over the analysis period look like?\nHow do ice cream sales vary across the four years within the analysis period? Are there significant differences in the summary statistics year over year?\nIs there variability in ice cream sales based on location? In other words, is there heterogeneity in consumption patterns based on location?\nWhat does the autocorrelation function of the ice cream sales indicate? Is the time series stationary?\nWhat are the key components of the time series (trend, seasonality, etc.)?\nHow does the plot of the maximum daily ice cream sales for each week of the year look? Can we model these maximum values using a Gumbel distribution to understand the distribution of peak daily demand nationally?\n\n\n\n\n\n\n\nNote\n\n\n\nThe relevant analysis tasks (the list above) is something the analyst has to generate. In my view, this comes with experience with similar data and use cases. Asking Chat GPT on how to do time series analysis got me very general guidelines. I did not find the response very useful (see below). The ACF suggests a white noise process (which is the ground truth), Chat GPT was trying find patterns in it. A notebook implementation of most of these questions is available\n\n\n\n\n\nChat GPT Analysis"
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#data-preparation",
    "href": "posts/ice_cream_sales/index.html#data-preparation",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "Transforming the raw dataset to a format that was suitable to answer the analysis questions required an understanding of the various fields in the data. To get the daily ice cream sales from the raw transaction data, I had the identify the transactions that contained ice creams and then aggregate the transactions on a daily basis. Doing this required an analysis of how the line items for each transaction is encoded. Once the data description was analyzed, providing the prompts to Chat GPT generated code that was pretty close to what I developed manually.\n\n\n\n\n\n\nNote\n\n\n\nThe real effort in the data preparation task went into developing an understanding of the various elements of the raw dataset and then coming up with the process of identifying the transactions containing ice cream. Aggregating these transactions to a daily or weekly cadence was fairly straight forward. Chat GPT was helpful in generating the specific pandas code for each step. The manual and the generated were pretty close."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#summary-of-chat-gpts-utility-for-this-particular-exercise",
    "href": "posts/ice_cream_sales/index.html#summary-of-chat-gpts-utility-for-this-particular-exercise",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "Chat GPT was definitely very helpful in developing and organizing the content.\nIdentifying the key questions that are relevant to the analysis requires formal training and experience - I don’t really see this replacing a data scientist at this point.\nChat GPT was definitely useful for various code generation tasks.\nIn summary, it was impressive as an assistant, but at this point, it is an assistant not a data scientist replacement."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "Using Hidden Markov Models with the Coffee Prices Data\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 24, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Coffee Prices\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Modeling without Supporting Data Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT and Data Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data/sba_heterogeneity_analysis.html",
    "href": "data/sba_heterogeneity_analysis.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import pandas as pd\nfp = \"../data/sba_7a_loans.csv\"\ndf = pd.read_csv(fp)\n\n/var/folders/fb/t_m5qpcj6qq85rvkh73vbxh40000gn/T/ipykernel_61763/3705498770.py:3: DtypeWarning: Columns (34,35,39) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(fp)\n\n\n\nsubset_cols = [\"BorrName\", \"BankFDICNumber\", \"BankZip\", \"BorrZip\", \"NaicsCode\", \"FranchiseCode\",\\\n               \"BusinessAge\", \"LoanStatus\", \"SBAGuaranteedApproval\"]\ndf = df[subset_cols]\n\n\nfilter_PIF = (df.LoanStatus == \"PIF\")\nfilter_CHGOFF = (df.LoanStatus == \"CHGOFF\")\nfilter_criteria = filter_PIF | filter_CHGOFF\ndf = df[filter_criteria].reset_index(drop=True)\n\n\ndf_missing_vals = pd.DataFrame.from_dict({c:df[c].isnull().sum() for c in subset_cols if df[c].isnull().sum() &gt; 0},\\\n                                         orient=\"index\").reset_index()\ndf_missing_vals.columns = [\"Attribute\", \"Missing Value Count\"]\ndf_missing_vals\n\n\n\n\n\n\n\n\nAttribute\nMissing Value Count\n\n\n\n\n0\nBankFDICNumber\n2122\n\n\n1\nFranchiseCode\n19923\n\n\n2\nBusinessAge\n53\n\n\n\n\n\n\n\n\nfor a in df_missing_vals[\"Attribute\"]:\n    df[a] = df[a].fillna(\"Not Applicable\")\n\n\n{c:df[c].isnull().sum() for c in subset_cols if df[c].isnull().sum() &gt; 0}\n\n{}\n\n\n\ndf[\"BankFDICNumber\"] = df[\"BankFDICNumber\"].apply(lambda x: x if x == \"Not Applicable\" else int(x))\ndf[\"NaicsCode\"] = df[\"NaicsCode\"].apply(lambda x: x if x == \"Not Applicable\" else int(x))\ndtypes_toset = {\"BorrZip\": 'str', \"BankZip\": \"str\", \"BankFDICNumber\": 'str',\\\n                \"NaicsCode\": 'str', \"FranchiseCode\": 'str', \\\n                \"BusinessAge\" : 'str', \"LoanStatus\": 'str', \"SBAGuaranteedApproval\" : float}\ndf = df.astype(dtypes_toset)\n\n\ndf\n\n\n\n\n\n\n\n\nBorrName\nBankFDICNumber\nBankZip\nBorrZip\nNaicsCode\nFranchiseCode\nBusinessAge\nLoanStatus\nSBAGuaranteedApproval\n\n\n\n\n0\nAllen Foot and Ankle Medicine\n57512\n85004\n85212\n621391\nNot Applicable\nExisting or more than 2 years old\nPIF\n175000.0\n\n\n1\nTown Cleaners\n24170\n90010\n22201\n812320\nNot Applicable\nExisting or more than 2 years old\nCHGOFF\n11000.0\n\n\n2\nMoor Inc.\n628\n43240\n94061\n445110\nNot Applicable\nExisting or more than 2 years old\nPIF\n24500.0\n\n\n3\nMimo Lash And Skin LLC\n26610\n90010\n7030\n812199\nNot Applicable\nExisting or more than 2 years old\nPIF\n28500.0\n\n\n4\nMoor Inc.\n628\n43240\n94061\n445110\nNot Applicable\nExisting or more than 2 years old\nPIF\n50500.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22956\nJK Freight LLC\n6560\n43215\n46239\n484121\nNot Applicable\nNew Business or 2 years or less\nPIF\n12500.0\n\n\n22957\nMuirhead Group LLC\n12633\n65101\n65255\n722513\nNot Applicable\nExisting or more than 2 years old\nPIF\n38900.0\n\n\n22958\nJP ELECTRICAL CONTRACTOR\n34968\n918\n674\n238390\nNot Applicable\nExisting or more than 2 years old\nPIF\n25000.0\n\n\n22959\nANOUSHEH ASHOURI INC.\nNot Applicable\n7922\n92618\n621111\nNot Applicable\nExisting or more than 2 years old\nPIF\n22500.0\n\n\n22960\nFur Fluffs Sake LLC\nNot Applicable\n1752\n1752\n812910\nNot Applicable\nNew Business or 2 years or less\nPIF\n31450.0\n\n\n\n\n22961 rows × 9 columns\n\n\n\n\ngb = df.groupby([\"NaicsCode\", \"LoanStatus\"], as_index=False)\n\n\ndfnaics_summ = gb.size()\n\n\ndfnaics_summ = dfnaics_summ.sort_values(by=[\"NaicsCode\"])\n\n\n#dfnaics_summ[(dfnaics_summ[\"NaicsCode\"].value_counts() == 1)]\n\n\nnaics_filter = (dfnaics_summ[\"NaicsCode\"].value_counts() == 1)\n\n\ndfnaics_vc = dfnaics_summ[\"NaicsCode\"].value_counts().reset_index()\n\n\ndfnaics_vc.columns = [\"NaicsCode\", \"LoanStatusCard\"]\n\n\nfilter_ss_naics_codes = (dfnaics_vc.LoanStatusCard == 1)\nss_naics_codes = dfnaics_vc[filter_ss_naics_codes][\"NaicsCode\"]\n\n\ndfnaics_vc[~filter_ss_naics_codes]\n\n\n\n\n\n\n\n\nNaicsCode\nLoanStatusCard\n\n\n\n\n0\n442291\n2\n\n\n1\n541370\n2\n\n\n2\n532310\n2\n\n\n3\n532490\n2\n\n\n4\n541110\n2\n\n\n...\n...\n...\n\n\n272\n611699\n2\n\n\n273\n621310\n2\n\n\n274\n611691\n2\n\n\n275\n321999\n2\n\n\n276\n621999\n2\n\n\n\n\n277 rows × 2 columns\n\n\n\n\ndf[df[\"NaicsCode\"].isin(ss_naics_codes)][\"LoanStatus\"].value_counts()\n\nPIF       4757\nCHGOFF       8\nName: LoanStatus, dtype: int64\n\n\n\ndf[df[\"NaicsCode\"].isin(ss_naics_codes)][\"NaicsCode\"].value_counts()\n\n447110    250\n531130    144\n722410    124\n623312     92\n541512     90\n         ... \n111419      1\n336411      1\n561591      1\n111140      1\n459120      1\nName: NaicsCode, Length: 583, dtype: int64\n\n\n\ngb = df.groupby([\"BankZip\", \"LoanStatus\"], as_index=False)\n\n\ndfbank_zip_summ = gb.size()\ndfbank_zip_summ = dfbank_zip_summ.sort_values(by=[\"BankZip\"])\n\n\nbankzip_filter = (dfbank_zip_summ[\"BankZip\"].value_counts() == 1)\n\n\ndfbank_zip_vc = dfbank_zip_summ[\"BankZip\"].value_counts().reset_index()\ndfbank_zip_vc.columns = [\"BankZip\", \"LoanStatusCard\"]\nfilter_ss_bankzip = (dfbank_zip_vc.LoanStatusCard == 1)\nss_bankzip_codes = dfbank_zip_vc[filter_ss_bankzip][\"BankZip\"]\n\n\ndf[df[\"BankZip\"].isin(ss_bankzip_codes)][\"LoanStatus\"].value_counts()\n\nPIF       8300\nCHGOFF       9\nName: LoanStatus, dtype: int64\n\n\n\ndf_trim = df[df[\"BankZip\"].isin(ss_bankzip_codes)].index.union(df[df[\"NaicsCode\"].isin(ss_naics_codes)].index)\n\n\ndf_trim.shape\n\n(10909,)\n\n\n\ngb = df.groupby([\"BorrZip\", \"LoanStatus\"], as_index=False)\ndfborr_zip_summ = gb.size()\ndfborr_zip_summ = dfborr_zip_summ.sort_values(by=[\"BorrZip\"])\n\n\nborrzip_filter = (dfborr_zip_summ[\"BorrZip\"].value_counts() == 1)\n\n\ndfborr_zip_vc = dfborr_zip_summ[\"BorrZip\"].value_counts().reset_index()\ndfborr_zip_vc.columns = [\"BorrZip\", \"LoanStatusCard\"]\nfilter_ss_borrzip = (dfborr_zip_vc.LoanStatusCard == 1)\nss_borrzip_codes = dfborr_zip_vc[filter_ss_borrzip][\"BorrZip\"]\n\n\ndf[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"LoanStatus\"].value_counts()\n\nPIF       19549\nCHGOFF      250\nName: LoanStatus, dtype: int64\n\n\n\ndf[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"BorrZip\"].value_counts()\n\n39428    35\n56601    27\n26508    22\n67601    22\n58102    21\n         ..\n48159     1\n65483     1\n12168     1\n29652     1\n65255     1\nName: BorrZip, Length: 8361, dtype: int64\n\n\n\ndf_borr_zip_counts_summ = df[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"BorrZip\"].value_counts().reset_index()\n\n\ndf_borr_zip_counts_summ.columns = [\"BorrZip\", \"num_recs\"]\n\n\ndf_borr_zip_counts_summ[df_borr_zip_counts_summ.num_recs == 1]\n\n\n\n\n\n\n\n\nBorrZip\nnum_recs\n\n\n\n\n4340\n14011\n1\n\n\n4341\n47441\n1\n\n\n4342\n66083\n1\n\n\n4343\n53522\n1\n\n\n4344\n64740\n1\n\n\n...\n...\n...\n\n\n8356\n48159\n1\n\n\n8357\n65483\n1\n\n\n8358\n12168\n1\n\n\n8359\n29652\n1\n\n\n8360\n65255\n1\n\n\n\n\n4021 rows × 2 columns\n\n\n\n\nlen(df[\"BorrZip\"].unique())\n\n9057"
  },
  {
    "objectID": "notebooks/R8_HMM_EM.html",
    "href": "notebooks/R8_HMM_EM.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import pandas as pd\nimport quantecon as qe\nimport warnings\nwarnings.filterwarnings('ignore')\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n\nfp = \"../data/regimed_coffee_prices.csv\"\ndf = pd.read_csv(fp)\nselect_r8 = (df.regime == \"R-8\")\ndfr8 = df[select_r8].copy().reset_index(drop=True)\ndel df\n\n\nLEARN_RNG = 40\n\n\ndfr8.shape[0]\n\n47\n\n\n\nfrom matplotlib import pyplot as plt\ndflearn = dfr8.iloc[:LEARN_RNG, :].reset_index(drop=True)\ndftest = dfr8.iloc[LEARN_RNG:, :].reset_index(drop=True)\n\n\ndflearn[\"cents_per_lb\"].plot.kde()\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom hmmlearn import hmm\nX = dflearn[\"cents_per_lb\"].values\nX = X.reshape((X.shape[0], 1))\n\n\nX.shape\n\n(40, 1)\n\n\n\ncomp_range = [ 2,3]\n\n\ncomp_range\n\n[2, 3]\n\n\n\nnp.random.randint(500)\n\n383\n\n\n\nscores = list()\nmodels = list()\nbic = list()\nnp.random.seed(10)\nfor n_components in comp_range:\n    for idx in range(10):\n        # define our hidden Markov model\n        model = hmm.GaussianHMM(n_components=n_components,\n                                covariance_type='full',\n                                random_state=np.random.randint(10,500))\n        model.fit(X)  # 50/50 \n        models.append(model)\n        scores.append(model.score(X))\n        bic.append(model.bic(X))\n        print(f'Converged: {model.monitor_.converged}'\n              f'\\tScore: {scores[-1]}')\n\n# get the best model\nmodel = models[np.argmin(bic)]\nn_states = model.n_components\nprint(f'The best model had a score of {max(scores)} and {n_states} '\n      'states')\n\n# use the Viterbi algorithm to predict the most likely sequence of states\n# given the model\nstates = model.predict(X)\n\nConverged: True Score: -193.01424017511735\nConverged: True Score: -173.4905746732474\nConverged: True Score: -173.50308099381377\nConverged: True Score: -173.4440605514749\nConverged: True Score: -193.00533548374705\nConverged: True Score: -173.2950245063382\nConverged: True Score: -192.69477463018066\nConverged: True Score: -173.49555242391432\nConverged: True Score: -193.02331693449096\nConverged: True Score: -173.29226497857934\nConverged: True Score: -165.47664941343598\nConverged: True Score: -178.86754302496482\nConverged: True Score: -166.65183108643694\nConverged: True Score: -165.4549050271745\nConverged: True Score: -171.8040837555721\nConverged: True Score: -167.82937741455203\nConverged: True Score: -177.08936192571053\nConverged: True Score: -169.99930610147587\nConverged: True Score: -173.16803000558758\nConverged: True Score: -165.29478475018215\nThe best model had a score of -165.29478475018215 and 2 states\n\n\n\nXt = dfr8.loc[LEARN_RNG:, \"cents_per_lb\"].values\nXt = Xt.reshape((Xt.shape[0], 1))\n\n\ndftest[\"cluster\"] = model.predict(Xt)\n\n\ndflearn[\"cluster\"] = model.predict(X)\n\n\ndflearn.groupby(\"cluster\").agg( size = pd.NamedAgg(column=\"cluster\", aggfunc=\"size\"),\\\n                         min_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"min\"),\\\n                        max_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"max\"),\\\n                        mean_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"mean\"),\\\n                        std_dev_price =  pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"std\")).round(2)\n\n\n\n\n\n\n\n\nsize\nmin_price\nmax_price\nmean_price\nstd_dev_price\n\n\ncluster\n\n\n\n\n\n\n\n\n\n0\n27\n168.65\n241.07\n209.79\n19.25\n\n\n1\n13\n248.40\n279.83\n264.17\n8.50\n\n\n\n\n\n\n\n\nmodel.covars_\n\narray([[[355.45947763]],\n\n       [[ 88.55697321]]])\n\n\n\nmodel.means_\n\narray([[209.41982256],\n       [263.24063538]])\n\n\n\nselect_c0 = (dflearn.cluster == 0)\nselect_c1 = (dflearn.cluster == 1)\n\n\nfrom matplotlib import pyplot as plt\ndflearn[select_c0][\"cents_per_lb\"].plot.kde()\nplt.grid(True)\n\n\n\n\n\n\n\n\n\ndflearn[select_c1][\"cents_per_lb\"].plot.kde()\nplt.grid(True)\n\n\n\n\n\n\n\n\n\ndfr8 = pd.concat([dflearn, dftest], axis=0).reset_index(drop=True)\n\n\ndfr8[\"cluster\"] = dfr8[\"cluster\"].astype(str)\n\n\nimport plotly.express as px\nfig = px.scatter(dfr8, x='date', y='cents_per_lb', color='cluster')\nfig.add_vline(x='2024-08-01', line_width=3, line_dash=\"dash\", line_color=\"green\")\nfig.show()"
  },
  {
    "objectID": "notebooks/download_coffee.html",
    "href": "notebooks/download_coffee.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import fredapi as fa\n\n\nfrom dotenv import dotenv_values\n\nconfig = dotenv_values(\".env\")\n\n\nimport pandas as pd\nfred = fa.Fred(config[\"FRED_API_KEY\"])\n\n\ncp = fred.get_series(\"PCOFFOTMUSDM\")\n\n\ncp = cp.dropna()\ncp_df = pd.DataFrame({\"date\": cp.index, \"cents_per_lb\": cp.values})\ncp_df[\"cents_per_lb\"] = cp_df[\"cents_per_lb\"].round(3)\n\n\ncp_df.to_csv(config[\"CP_DATA_FILE\"], index=True)"
  },
  {
    "objectID": "notebooks/markov_chains_coffee_prices.html",
    "href": "notebooks/markov_chains_coffee_prices.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import pandas as pd\nimport quantecon as qe\nimport warnings\nwarnings.filterwarnings('ignore')\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n\nfp = \"../data/stochastic_matrix_coffee_price-regime-R-8.csv\"\ndf = pd.read_csv(fp, usecols = [\"L\", \"M\", \"H\"])\ncol_order = [\"L\", \"M\", \"H\"]\ndf = df[col_order]\n\n\nsm = df.values\nsm\n\narray([[0.867, 0.133, 0.   ],\n       [0.2  , 0.667, 0.133],\n       [0.   , 0.188, 0.812]])\n\n\n\nfrom quantecon import MarkovChain\n\nmc = qe.MarkovChain(sm, (\"L\", \"M\", \"H\"))\n\n\nmc.is_irreducible\n\nTrue\n\n\n\nmc.communication_classes\n\n[array(['L', 'M', 'H'], dtype='&lt;U1')]\n\n\n\nmc.is_aperiodic\n\nTrue\n\n\n\nmc.stationary_distributions\n\narray([[0.46828491, 0.31140946, 0.22030563]])\n\n\n\nimport plotly.express as px\n\nfig = px.imshow(sm, text_auto=True,  labels=dict(x=\"Previous Month Price\", y=\"Current Month Price\"),\n                x=['Low', 'Medium', 'High'],\n                y=['Low', 'Medium', 'High'])\nfig.update_layout(\n    title={\n        'text': \"Stochastic Matrix for Region 8\",\n        'y':.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nd = mc.stationary_distributions.flatten().tolist()\nstationary_dist = {\"Low\": d[0], \"Medium\": d[1], \"High\": d[2]}\ndfp = pd.DataFrame.from_dict(stationary_dist, orient='index').round(3)\ndfp = dfp.reset_index()\ndfp.columns = [\"Price\", \"Probability\"]\ndfp\n\n\n\n\n\n\n\n\nPrice\nProbability\n\n\n\n\n0\nLow\n0.468\n\n\n1\nMedium\n0.311\n\n\n2\nHigh\n0.220"
  },
  {
    "objectID": "notebooks/generated_code_retail.html",
    "href": "notebooks/generated_code_retail.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n# Step 1: Read a pandas DataFrame\n# For demonstration, we will create a sample dataframe. You can replace this with your own CSV or data source.\nfp = \"../data/Retail_Transactions_Dataset.csv\"\ndf = pd.read_csv(fp)\n\n# Step 2: Define lists for categorical columns\ncategorical_columns = ['Payment_Method', 'City', 'Store_Type',\\\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion']\n\n# Step 3: Define your timestamp column\ntimestamp_column = 'Date'\n\n# Step 4: Set the type of the categorical columns to 'category'\nfor col in categorical_columns:\n    df[col] = df[col].astype('category')\n\n# Step 5: Set the type of the timestamp column to datetime\ndf[timestamp_column] = pd.to_datetime(df[timestamp_column])\n\n# Step 6: Function to check for a sentinel value (for this example, let's assume the sentinel value is 0)\ndef contains_sentinel_value(string_list):\n    # Convert the string representation of a list back to an actual list\n    try:\n        actual_list = eval(string_list)\n        return 0 in actual_list  # Check for sentinel value\n    except:\n        return False  # In case of any errors, return False\n\n# Step 7: Filter the DataFrame to rows that only contain the sentinel value\ndf['is_ice_cream'] = df[\"Product\"].apply(contains_sentinel_value)\nfiltered_df = df[df['ice_cream']]\n\n# Step 8: Drop all columns except the timestamp column in the filtered DataFrame\nfiltered_df = filtered_df[[timestamp_column]]\n\n# Step 9: Define a new column in the filtered DataFrame that is set to the value 1\nfiltered_df['is_ice_cream'] = 1\n\n# Step 10: Set the index of the filtered DataFrame to the timestamp column\nfiltered_df.set_index(timestamp_column, inplace=True)\n\n# Step 11: Resample the DataFrame on the timestamp column and sum the new column\n# Assuming we want to sum by minute, you can change the frequency as needed\nresampled_df = filtered_df.resample('T').sum()\n\n# Display the final resampled DataFrame\nprint(resampled_df)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'ice_cream'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[1], line 34\n     32 # Step 7: Filter the DataFrame to rows that only contain the sentinel value\n     33 df['is_ice_cream'] = df[\"Product\"].apply(contains_sentinel_value)\n---&gt; 34 filtered_df = df[df['ice_cream']]\n     36 # Step 8: Drop all columns except the timestamp column in the filtered DataFrame\n     37 filtered_df = filtered_df[[timestamp_column]]\n\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'ice_cream'"
  }
]