[
  {
    "objectID": "notebooks/download_7a_data_2025.html",
    "href": "notebooks/download_7a_data_2025.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "from dotenv import dotenv_values\nfp = \"../data/.env\"\nconfig = dotenv_values(fp)\n\n\nimport requests\nimport pandas as pd\nimport io\n\n\nfetch_string = config[\"DATA_GOV_7A_LOANS_URL\"] + \"?api-key=\" + config[\"DATA_GOV_USA_API_KEY\"] + \"&format=csv\"\n\n\nurlData = requests.get(fetch_string).content\n\n\nfp_write = \"../data/7aloans2025.csv\"\nrawData = pd.read_csv(io.StringIO(urlData.decode('utf-8')))\nrawData.to_csv(fp_write)\n\n\nfetch_string = config[\"DATA_GOV_7A_LOANS_DATA_DICT_URL\"] + \"?api-key=\" + config[\"DATA_GOV_USA_API_KEY\"] + \"&format=csv\"\nurlData = requests.get(fetch_string).content\n\n\nwith io.BytesIO(urlData) as fh:\n    df = pd.io.excel.read_excel(fh, sheet_name=0)\n\n\nfp = \"../data/dictionary/7a_loans_2025_data_dictionary.csy\"\ndf.to_csv(fp, index=False)"
  },
  {
    "objectID": "notebooks/generated_code_retail.html",
    "href": "notebooks/generated_code_retail.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n# Step 1: Read a pandas DataFrame\n# For demonstration, we will create a sample dataframe. You can replace this with your own CSV or data source.\nfp = \"../data/Retail_Transactions_Dataset.csv\"\ndf = pd.read_csv(fp)\n\n# Step 2: Define lists for categorical columns\ncategorical_columns = ['Payment_Method', 'City', 'Store_Type',\\\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion']\n\n# Step 3: Define your timestamp column\ntimestamp_column = 'Date'\n\n# Step 4: Set the type of the categorical columns to 'category'\nfor col in categorical_columns:\n    df[col] = df[col].astype('category')\n\n# Step 5: Set the type of the timestamp column to datetime\ndf[timestamp_column] = pd.to_datetime(df[timestamp_column])\n\n# Step 6: Function to check for a sentinel value (for this example, let's assume the sentinel value is 0)\ndef contains_sentinel_value(string_list):\n    # Convert the string representation of a list back to an actual list\n    try:\n        actual_list = eval(string_list)\n        return 0 in actual_list  # Check for sentinel value\n    except:\n        return False  # In case of any errors, return False\n\n# Step 7: Filter the DataFrame to rows that only contain the sentinel value\ndf['is_ice_cream'] = df[\"Product\"].apply(contains_sentinel_value)\nfiltered_df = df[df['ice_cream']]\n\n# Step 8: Drop all columns except the timestamp column in the filtered DataFrame\nfiltered_df = filtered_df[[timestamp_column]]\n\n# Step 9: Define a new column in the filtered DataFrame that is set to the value 1\nfiltered_df['is_ice_cream'] = 1\n\n# Step 10: Set the index of the filtered DataFrame to the timestamp column\nfiltered_df.set_index(timestamp_column, inplace=True)\n\n# Step 11: Resample the DataFrame on the timestamp column and sum the new column\n# Assuming we want to sum by minute, you can change the frequency as needed\nresampled_df = filtered_df.resample('T').sum()\n\n# Display the final resampled DataFrame\nprint(resampled_df)"
  },
  {
    "objectID": "notebooks/markov_chains_coffee_prices.html",
    "href": "notebooks/markov_chains_coffee_prices.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "import pandas as pd\nimport quantecon as qe\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfp = \"../data/stochastic_matrix_coffee_price-regime-R-8.csv\"\ndf = pd.read_csv(fp, usecols = [\"L\", \"M\", \"H\"])\ncol_order = [\"L\", \"M\", \"H\"]\ndf = df[col_order]\n\n\nsm = df.values\nsm\n\n\nfrom quantecon import MarkovChain\n\nmc = qe.MarkovChain(sm, (\"L\", \"M\", \"H\"))\n\n\nmc.is_irreducible\n\n\nmc.communication_classes\n\n\nmc.is_aperiodic\n\n\nmc.stationary_distributions\n\n\nimport plotly.express as px\n\nfig = px.imshow(sm, text_auto=True,  labels=dict(x=\"Previous Month Price\", y=\"Current Month Price\"),\n                x=['Low', 'Medium', 'High'],\n                y=['Low', 'Medium', 'High'])\nfig.update_layout(\n    title={\n        'text': \"Stochastic Matrix for Region 8\",\n        'y':.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show()\n\n\nd = mc.stationary_distributions.flatten().tolist()\nstationary_dist = {\"Low\": d[0], \"Medium\": d[1], \"High\": d[2]}\ndfp = pd.DataFrame.from_dict(stationary_dist, orient='index').round(3)\ndfp = dfp.reset_index()\ndfp.columns = [\"Price\", \"Probability\"]\ndfp"
  },
  {
    "objectID": "notebooks/download_coffee.html",
    "href": "notebooks/download_coffee.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "import fredapi as fa\n\n\nfrom dotenv import dotenv_values\n\nconfig = dotenv_values(\".env\")\n\n\nimport pandas as pd\nfred = fa.Fred(config[\"FRED_API_KEY\"])\n\n\ncp = fred.get_series(\"PCOFFOTMUSDM\")\n\n\ncp = cp.dropna()\ncp_df = pd.DataFrame({\"date\": cp.index, \"cents_per_lb\": cp.values})\ncp_df[\"cents_per_lb\"] = cp_df[\"cents_per_lb\"].round(3)\n\n\ncp_df.to_csv(config[\"CP_DATA_FILE\"], index=True)"
  },
  {
    "objectID": "notebooks/R8_HMM_EM.html",
    "href": "notebooks/R8_HMM_EM.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "import pandas as pd\nimport quantecon as qe\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfp = \"../data/regimed_coffee_prices.csv\"\ndf = pd.read_csv(fp)\nselect_r8 = (df.regime == \"R-8\")\ndfr8 = df[select_r8].copy().reset_index(drop=True)\ndel df\n\n\nLEARN_RNG = 40\n\n\ndfr8.shape[0]\n\n\nfrom matplotlib import pyplot as plt\ndflearn = dfr8.iloc[:LEARN_RNG, :].reset_index(drop=True)\ndftest = dfr8.iloc[LEARN_RNG:, :].reset_index(drop=True)\n\n\ndflearn[\"cents_per_lb\"].plot.kde()\nplt.grid(True)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom hmmlearn import hmm\nX = dflearn[\"cents_per_lb\"].values\nX = X.reshape((X.shape[0], 1))\n\n\nX.shape\n\n\ncomp_range = [ 2,3]\n\n\ncomp_range\n\n\nnp.random.randint(500)\n\n\nscores = list()\nmodels = list()\nbic = list()\nnp.random.seed(10)\nfor n_components in comp_range:\n    for idx in range(10):\n        # define our hidden Markov model\n        model = hmm.GaussianHMM(n_components=n_components,\n                                covariance_type='full',\n                                random_state=np.random.randint(10,500))\n        model.fit(X)  # 50/50 \n        models.append(model)\n        scores.append(model.score(X))\n        bic.append(model.bic(X))\n        print(f'Converged: {model.monitor_.converged}'\n              f'\\tScore: {scores[-1]}')\n\n# get the best model\nmodel = models[np.argmin(bic)]\nn_states = model.n_components\nprint(f'The best model had a score of {max(scores)} and {n_states} '\n      'states')\n\n# use the Viterbi algorithm to predict the most likely sequence of states\n# given the model\nstates = model.predict(X)\n\n\nXt = dfr8.loc[LEARN_RNG:, \"cents_per_lb\"].values\nXt = Xt.reshape((Xt.shape[0], 1))\n\n\ndftest[\"cluster\"] = model.predict(Xt)\n\n\ndflearn[\"cluster\"] = model.predict(X)\n\n\ndflearn.groupby(\"cluster\").agg( size = pd.NamedAgg(column=\"cluster\", aggfunc=\"size\"),\\\n                         min_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"min\"),\\\n                        max_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"max\"),\\\n                        mean_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"mean\"),\\\n                        std_dev_price =  pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"std\")).round(2)\n\n\nmodel.covars_\n\n\nmodel.means_\n\n\nselect_c0 = (dflearn.cluster == 0)\nselect_c1 = (dflearn.cluster == 1)\n\n\nfrom matplotlib import pyplot as plt\ndflearn[select_c0][\"cents_per_lb\"].plot.kde()\nplt.grid(True)\n\n\ndflearn[select_c1][\"cents_per_lb\"].plot.kde()\nplt.grid(True)\n\n\ndfr8 = pd.concat([dflearn, dftest], axis=0).reset_index(drop=True)\n\n\ndfr8[\"cluster\"] = dfr8[\"cluster\"].astype(str)\n\n\nimport plotly.express as px\nfig = px.scatter(dfr8, x='date', y='cents_per_lb', color='cluster')\nfig.add_vline(x='2024-08-01', line_width=3, line_dash=\"dash\", line_color=\"green\")\nfig.show()"
  },
  {
    "objectID": "notebooks/bakery_sales_last_40_weeks.html",
    "href": "notebooks/bakery_sales_last_40_weeks.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom hmmlearn import hmm\n\n\nfp = \"../data/daily_baugette_sales.csv\"\ndf = pd.read_csv(fp)\n\n\nCUT_OFF = df.CWOY.max() - 40\n\n\nselect_cut_off = df.CWOY &gt;= CUT_OFF\ndf = df[select_cut_off]\n\n\ncols_needed = [\"datetime\", \"Quantity\"]\ndf = df[cols_needed]\n\n\ndf[\"Quantity\"].plot.kde()\nplt.grid(True)\n\n\nfig = plt.figure()\nfig.set_size_inches(10, 6, forward=True)\nfig.subplots_adjust(hspace=0.3, wspace=0.5)\n\nplt.subplot(121)\ndf[\"Quantity\"].plot.kde()\nplt.grid(True)\nplt.title(\"Daily Baguette Sales - KDE\")\nplt.subplot(122)\nplt.title(\"Daily Baguette Sales - Histogram\")\ndf[\"Quantity\"].plot.hist()\nfig.tight_layout()\nplt.grid(True)\n\n\nfp = \"../data/daily_baguette_last_40_weeks.csv\"\ndf.to_csv(fp, index=True)\n\n\nimport unicodedata as ud\n\nsummary_stats = {\"mean\": df.Quantity.mean().round(3), \"var\": df.Quantity.var().round(3)}\n\n\ndf_summ_stats = pd.DataFrame.from_dict(summary_stats, orient=\"index\")\n\n\ndf_summ_stats = df_summ_stats.reset_index()\ndf_summ_stats.columns = [\"Parameter\", \"Value\"]\n\n\ndf_summ_stats\n\n\nimport numpy as np\nimport scipy.stats as stats\n\n\ndf_fit = df.iloc[:-14,:]\n\n\n# Fit the gamma distribution\nparams = stats.gamma.fit(df_fit[\"Quantity\"])\n\n\n# Extract the fitted parameters\nfitted_alpha, fitted_loc, fitted_beta = params\n\n\nprint(\"Fitted shape parameter (alpha):\", fitted_alpha)\nprint(\"Fitted location parameter (loc):\", fitted_loc)\nprint(\"Fitted scale parameter (beta):\", fitted_beta)\n\n\nfrom scipy.stats import nbinom\n\nCheck out this wikipedia link and then use the fact that \\(n\\) is the shape parameter, and the scale parameter \\(\\beta = \\frac{p}{(1-p)}\\), solve for \\(p\\) which is \\(\\frac{1}{(\\beta + 1)}\\), \\(\\beta = 5.97\\) from the above estimation, substituting into the above equation, we have \\(p=\\frac{1}{6.97}\\).\nThe specific content of interest is: “That is, we can view the negative binomial as a Poisson(λ) distribution, where λ is itself a random variable, distributed as a gamma distribution with shape r and scale θ = (1 − p)/p” - so we get \\(\\lambda\\) and p from fitting the gamma distribution. These values are shown above.\n\nn= 9.42\np = 1/6.97\nNUM_SAMPLES = 2000\n\n\nr1 = nbinom.rvs(n,p,size=NUM_SAMPLES)\nr2 = nbinom.rvs(n,p,size=NUM_SAMPLES)\n\n\ndf_gen = pd.DataFrame.from_records({\"xi\": r1, \"yi\": r2})\n\n\ndf_gen.columns = [\"xi\", \"yi\"]\n\n\nfp = \"../data/samples_for_stoch_estimation.csv\"\ndf_gen.to_csv(fp, index=False)\n\n\nsample = {\"actual\": df_fit[\"Quantity\"], \"fitted\": nbinom.rvs(n,p,size=df_fit[\"Quantity\"].shape[0])}\ndf_sample = pd.DataFrame.from_records(sample)\n\n\nfig = plt.figure()\nfig.set_size_inches(10, 6, forward=True)\nfig.subplots_adjust(hspace=0.3, wspace=0.5)\n\nplt.subplot(121)\ndf_sample[\"actual\"].plot.kde()\nplt.grid(True)\nplt.title(\"actual\")\nplt.subplot(122)\nplt.title(\"fitted (from samples drawn)\")\ndf_sample[\"fitted\"].plot.kde()\nfig.tight_layout()\nplt.grid(True)"
  },
  {
    "objectID": "notebooks/bakery_stochastic.html",
    "href": "notebooks/bakery_stochastic.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "from dotenv import dotenv_values\nfp = \"../notebooks/.env\"\nconfig = dotenv_values(fp)\n\n\nSOLVER = \"cbc\"\n\nfrom amplpy import AMPL, ampl_notebook\n\nampl = ampl_notebook(\n    modules=[\"cbc\"],  # modules to install\n    license_uuid=config[\"AMPL_UUID\"],  # license to use\n)  # instantiate AMPL object and register magics\n\n\n%%writefile bakery.mod\nparam N; # number of samples\n\nparam c;\nparam p;\nparam h;\n\nset indices := 1..N;\nparam xi{indices};\n\n# first stage variable: x (amount of baguettes baked)\nvar x integer &gt;=0;\n\nvar first_stage_profit = -c * x;\n\n# second stage variables: y (sold) and z (unsold)\nvar y{indices} integer &gt;=0;\nvar z{indices} integer &gt;=0;\n\n# second stage constraints\ns.t. cantsellbaguettedonthave {i in indices}: y[i] &lt;= xi[i];\ns.t. baguettedonotdisappear {i in indices}: y[i] + z[i] == x;\n\nvar second_stage_profit = sum{i in indices}(p * y[i] - h * z[i]) / N;\n\n# objective\nmaximize total_expected_profit: first_stage_profit + second_stage_profit;\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport scipy.stats as stats\n\n\n\nfrom scipy.stats import nbinom\n# Two-stage stochastic LP using SAA\n# Setting parameters\n## TL;DR version on pricing Profit Index. Costs = Selling Price, selling price is known from data, profit index is 1.7 \nc = 52\np = 90\nh = 1\n# Two-stage stochastic LP using SAA\n\nnp.random.seed(1)\nN = 5000\nshape = 9.42\nintensity = (1/6.97)\nsamples = nbinom.rvs(shape,intensity,size=N)\nampl = AMPL()\nampl.read(\"bakery.mod\")\n\n# Load the data\nampl.param[\"N\"] = int(N)\nampl.param[\"xi\"] = samples\nampl.param[\"c\"] = c\nampl.param[\"p\"] = p\nampl.param[\"h\"] = h\n\nampl.solve(solver=SOLVER)\nassert ampl.solve_result == \"solved\", ampl.solve_result\n\nxval = ampl.var[\"x\"].value()\ntotal_expected_profit = (ampl.obj[\"total_expected_profit\"].value()/100) # The divide by 100 is to convert from cent to euro\n\n\nprint(\n    f\"Approximate solution with fitted deman distribution using N={N:.0f} samples\"\n)\nprint(f\"Approximate optimal solution: x = {xval:.2f} baguettes\")\nprint(f\"Approximate expected daily profit: {total_expected_profit:.2f}€\")\n\n\n\nzs = ampl.var[\"z\"]\n\nys = ampl.var[\"y\"]\n\nny = ys.numInstances()\n\nyvals = [ ys[i].value() for i in range(1, ny +1)]\n\nzvals = [ zs[i].value() for i in range(1, ny +1)]\n\ndf_soln = pd.DataFrame.from_records({\"yi\": yvals, \"zi\": zvals})\n\n\ndf_soln"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I spent a long time developing software for business tasks in various domains, and I have a strong interest in the intersection of data science and software engineering. I am available for remote consulting work. I can work with your data science team on the following tasks:\n\nApplying data science to understand and extract insights from operational data.\nWorking with your data science team on framing business problems in terms of data science and machine learning. Translating business KPIs into model metrics and vice versa takes experience and practice. I have worked with data science teams in the past to help them with this translation.\nSummarizing and visualizing data science results for business stakeholders. This again requires experience and practice.\nModel and complexity management. What is the right level of model complexity for a given business problem? How do we refine and improve models over time? Doing this with a satisficing perspective takes experience and practice.\nSummarizing your data assets to inform the problems that you are interested in solving. Organizations collect vast quantitites of data over time. Summarizing these data assets so that key insights make their way into model iterations is a continuos process. There have been many developments in this area in the past decade (see this playlist ). However, picking the right approach for your data assets and business problem is not trivial. I can help you with this.\nWorking with your optimization team on interfacing machine learning and statistical models with optimization models. This is a very interesting area of research and has gained traction in the past decade. Many optimization and planning problems are stochastic in nature. They have to take uncertainty into account. This uncertainty can be modeled with machine learning and statistical models. For example, the weather can be a stochastic variable in a planning problem. If you are an agricultural company, you may want to use machine learning models to predict the weather. You can then use these predictions in your optimization models. If you are a retailer with perishable goods, you may want to use machine learning models to predict demand. You can then use these predictions to plan your inventory. Stochastic optimization is not esoteric anymore, thanks to the quality of optimization tools that are available today."
  },
  {
    "objectID": "posts/stochastic_opt/index.html",
    "href": "posts/stochastic_opt/index.html",
    "title": "Stochastic Optimization",
    "section": "",
    "text": "Synergy and complementarity are terms that aptly describe the relationship between machine learning and optimization. In machine learning, optimization is often the tool used to determine the best parameters for a model. For instance, when fitting a straight line to data, optimization helps identify the slope that best represents the relationship.\nMy interest in decision-making under cost constraints led me to explore stochastic optimization. This field is particularly relevant for those aiming to make data-driven decisions while explicitly accounting for:\n\nUncertainty in the decision variables of an optimization problem.\nThe sequential or hierarchical nature of decision variables, where some decisions influence others.\n\nStochastic optimization encompasses various approaches, but I will focus on one here. In this approach, problems are structured sequentially, with uncertainty typically associated with the primary decision variables, which are treated as random variables. Once these variables are realized, the optimal values for the dependent variables are determined accordingly.\nInterestingly, the interplay between statistics and optimization in this context contrasts with what we observe in machine learning. Here, statistical estimation informs and drives the optimization process. This synergy between the two fields highlights their potential to complement each other. As data collection and analysis techniques continue to evolve, I anticipate that stochastic optimization will gain prominence, fostering collaboration between machine learning and optimization practitioners.\n\n\nTo illustrate the role of statistical estimation in a simplified stochastic optimization problem, consider a bakery that sells baguettes. Each morning, the bakery must decide how many baguettes to bake, knowing that daily demand is uncertain. The challenge lies in understanding this uncertainty and fitting an appropriate probability distribution to model the demand. This step is crucial for making informed decisions that balance the risks of overproduction and underproduction.\nHere is the cost data. The cost of baking a baguette is 53 cents, and the selling price is 90 cents. I used this article to arrive at pricing information. The gist of the price information is: \\[\n\\text{Costs} \\times \\text{Profit Index} = \\text{Selling Price}\n\\]\nThe article uses a profit index of \\(1.7\\), the selling price is available from the data. I am going with the version that you can use baguette’s for up to a day, so what you need to bake each day is what is the sum of what is held (unsold) from yesterday and what you expect to sell today. This is based on an adaptation of this model.\nHere are the basic steps to solve the problem:\n\nProfile the Data I am going to build on the preview I talked about in my previous post. The data for this example comes from kaggle. The code for downloading the data and preliminary data exploration is available in this notebook. As discussed in the previous post, there are two trends in this data. If you restrict the time period to the last \\(40\\) weeks then the data is unimodal with a mean of \\(38\\) and a variance of \\(341\\). The variable of interest, the number of baguettes sold everyday, is a count. The histogram and KDE are shown below.\n\n\n\n\nDaily Sales of Baguettes in a bakery\n\n\nThe code for the detailed data profile for the last \\(40\\) weeks of data is available here.\n\nStatistical Estimation: The demand is a count variable that is over dispersed, that is, the variance is larger than the mean. We cannot use a Poisson distribution for the demand because of this over dispersion. For a Poisson distributed random variable, the mean and variance need to be identical, we can probably live with similar, but this is an order of magnitude difference. We can use a Negative Binomial distribution instead. This can work with over dispersed data. One interpretation of a Negative Binomial distribution is as a compound distribution. It is a distribution where one of the parameters is also a distribution, so you can think of it as a mixture of Poisson random variables where the rate parameter is a Gamma distribution. We have the daily arrival rates here (which happen to be integral, but in general, they do not need to be), we fit a Gamma distribution to the daily purchase rates using maximum likelihood estimation. This is available in scipy.stats package. We can then use the relationship between the Negative Binomial and its associated Gamma distribution to work out the details of the parameters of the Negative Binomial distribution. For the relationship between the Gamma and the Negative Binomial arameters, please see this wikipedia article. Please see this notebook for the details of the estimation. The fitted Negative Binomial distribution and the actual demand data are shown below.\n\n\n\n\nEstimated Negative Binomial Demand Fit\n\n\n\n\n\n\n\n\nOptimizing the Gamma Likelihood, why?\n\n\n\nIf you are wondering why I am not doing maximum likelihood estimation on the Negative Binomial, but instead I am choosing to do it on the associated Gamma distribution, it is because the Gamma distribution is continuous. Once you have fitted the Gamma to the data, you can work out the associated (optimal) Negative Binomial. The scipy.stats package does not provide a fit method for the Negative Binomial, probably for this reason - the fact that you can always use the Gamma MLE to work this out.\n\n\n\nStochastic Optimization: Here, I will outline the procedure, highlight the intuition behind it, and provide a link to the implementation. The primary decision variable is the number of baguettes to bake each day. This decision is influenced by the demand distribution and any unsold stock from the previous day. In essence, the total stock available (unsold stock plus freshly baked baguettes) must not exceed the expected demand for the day.\n\nThe demand distribution plays a pivotal role in guiding the optimization process. The implementation of this narrative leverages a mathematical modeling language for optimization, specifically AMPL (“AMPL: Advanced Modeling for Optimization Solutions — Ampl.com”). For details, refer to the bakery.mod section in the model implementation notebook. The book “Introduction to Stochastic Programming” (Birge and Louveaux 2011) served as an excellent resource, offering clear explanations and practical examples of real-world business problems that can be effectively addressed using stochastic optimization techniques.\nSo what are the results of doing all of this. To run an efficient business, when holding some inventory is allowed, with the price and cost data assumed, we should expect to bake about \\(50\\) baguettes a day and make \\(14.73\\) euro of average profit a day.\n\n\n\nOptimization Results\n\n\n\n\n\n\n\n\nNote:\n\n\n\n\nThe primary goal of this post is to highlight how statistical estimation serves as a critical component of stochastic optimization. While the newsvendor model could be a better fit compared to the stock optimization model used here —the idea was to show that statistical estimation primes the optimization process.\nIf you are wondering why not just use the mean and what really do we get by working with the distribution, please check out the AMPL seafood stock optimization example. It provides the difference in profits between using the expected value and using the distribution. Retail margins are sometimes small. So doing this may matter."
  },
  {
    "objectID": "posts/stochastic_opt/index.html#practical-example",
    "href": "posts/stochastic_opt/index.html#practical-example",
    "title": "Stochastic Optimization",
    "section": "",
    "text": "To illustrate the role of statistical estimation in a simplified stochastic optimization problem, consider a bakery that sells baguettes. Each morning, the bakery must decide how many baguettes to bake, knowing that daily demand is uncertain. The challenge lies in understanding this uncertainty and fitting an appropriate probability distribution to model the demand. This step is crucial for making informed decisions that balance the risks of overproduction and underproduction.\nHere is the cost data. The cost of baking a baguette is 53 cents, and the selling price is 90 cents. I used this article to arrive at pricing information. The gist of the price information is: \\[\n\\text{Costs} \\times \\text{Profit Index} = \\text{Selling Price}\n\\]\nThe article uses a profit index of \\(1.7\\), the selling price is available from the data. I am going with the version that you can use baguette’s for up to a day, so what you need to bake each day is what is the sum of what is held (unsold) from yesterday and what you expect to sell today. This is based on an adaptation of this model.\nHere are the basic steps to solve the problem:\n\nProfile the Data I am going to build on the preview I talked about in my previous post. The data for this example comes from kaggle. The code for downloading the data and preliminary data exploration is available in this notebook. As discussed in the previous post, there are two trends in this data. If you restrict the time period to the last \\(40\\) weeks then the data is unimodal with a mean of \\(38\\) and a variance of \\(341\\). The variable of interest, the number of baguettes sold everyday, is a count. The histogram and KDE are shown below.\n\n\n\n\nDaily Sales of Baguettes in a bakery\n\n\nThe code for the detailed data profile for the last \\(40\\) weeks of data is available here.\n\nStatistical Estimation: The demand is a count variable that is over dispersed, that is, the variance is larger than the mean. We cannot use a Poisson distribution for the demand because of this over dispersion. For a Poisson distributed random variable, the mean and variance need to be identical, we can probably live with similar, but this is an order of magnitude difference. We can use a Negative Binomial distribution instead. This can work with over dispersed data. One interpretation of a Negative Binomial distribution is as a compound distribution. It is a distribution where one of the parameters is also a distribution, so you can think of it as a mixture of Poisson random variables where the rate parameter is a Gamma distribution. We have the daily arrival rates here (which happen to be integral, but in general, they do not need to be), we fit a Gamma distribution to the daily purchase rates using maximum likelihood estimation. This is available in scipy.stats package. We can then use the relationship between the Negative Binomial and its associated Gamma distribution to work out the details of the parameters of the Negative Binomial distribution. For the relationship between the Gamma and the Negative Binomial arameters, please see this wikipedia article. Please see this notebook for the details of the estimation. The fitted Negative Binomial distribution and the actual demand data are shown below.\n\n\n\n\nEstimated Negative Binomial Demand Fit\n\n\n\n\n\n\n\n\nOptimizing the Gamma Likelihood, why?\n\n\n\nIf you are wondering why I am not doing maximum likelihood estimation on the Negative Binomial, but instead I am choosing to do it on the associated Gamma distribution, it is because the Gamma distribution is continuous. Once you have fitted the Gamma to the data, you can work out the associated (optimal) Negative Binomial. The scipy.stats package does not provide a fit method for the Negative Binomial, probably for this reason - the fact that you can always use the Gamma MLE to work this out.\n\n\n\nStochastic Optimization: Here, I will outline the procedure, highlight the intuition behind it, and provide a link to the implementation. The primary decision variable is the number of baguettes to bake each day. This decision is influenced by the demand distribution and any unsold stock from the previous day. In essence, the total stock available (unsold stock plus freshly baked baguettes) must not exceed the expected demand for the day.\n\nThe demand distribution plays a pivotal role in guiding the optimization process. The implementation of this narrative leverages a mathematical modeling language for optimization, specifically AMPL (“AMPL: Advanced Modeling for Optimization Solutions — Ampl.com”). For details, refer to the bakery.mod section in the model implementation notebook. The book “Introduction to Stochastic Programming” (Birge and Louveaux 2011) served as an excellent resource, offering clear explanations and practical examples of real-world business problems that can be effectively addressed using stochastic optimization techniques.\nSo what are the results of doing all of this. To run an efficient business, when holding some inventory is allowed, with the price and cost data assumed, we should expect to bake about \\(50\\) baguettes a day and make \\(14.73\\) euro of average profit a day.\n\n\n\nOptimization Results\n\n\n\n\n\n\n\n\nNote:\n\n\n\n\nThe primary goal of this post is to highlight how statistical estimation serves as a critical component of stochastic optimization. While the newsvendor model could be a better fit compared to the stock optimization model used here —the idea was to show that statistical estimation primes the optimization process.\nIf you are wondering why not just use the mean and what really do we get by working with the distribution, please check out the AMPL seafood stock optimization example. It provides the difference in profits between using the expected value and using the distribution. Retail margins are sometimes small. So doing this may matter."
  },
  {
    "objectID": "posts/descriptive_analytics/index.html",
    "href": "posts/descriptive_analytics/index.html",
    "title": "Descriptive Analytics, the lost idea in the current AI hype",
    "section": "",
    "text": "In the current wave of excitement around AI and generative technologies, the value of descriptive analytics as a foundation for predictive modeling is often overlooked. Descriptive analytics goes beyond traditional exploratory data analysis; it leverages statistical and machine learning methods to systematically summarize, interpret, and explain the essential patterns and concepts within your business data. This process provides critical insights that should inform and guide subsequent predictive modeling efforts.\nAs datasets grow in size, summarizing them becomes increasingly essential. Traditional visual inspection methods, effective for small datasets, quickly become impractical as data scales to thousands or millions of rows. To extract meaningful insights, it is crucial to identify the most informative subsets of data, distinguishing them from less relevant portions. Both the optimization and machine learning communities have developed innovative techniques to efficiently summarize and interpret large, complex datasets.\nMany organizations are eager to implement machine learning in their business operations. However, effective application requires careful due diligence to ensure a thorough understanding of the data generated by underlying business processes. Ideally, the characteristics of the data should inform the choice of modeling approach. In practice, though, modeling techniques are often predetermined for various reasons, and the focus shifts to adapting the data to fit the chosen model, rather than the other way around.\nI started a repository with recipes for descriptive analytics. I hope to add to this incrementally. To begin with I picked a transactions dataset from a national retailer in Brazil. You can check out the narrative and notebook."
  },
  {
    "objectID": "posts/descriptive_analytics/index.html#descriptive-analytics",
    "href": "posts/descriptive_analytics/index.html#descriptive-analytics",
    "title": "Descriptive Analytics, the lost idea in the current AI hype",
    "section": "",
    "text": "In the current wave of excitement around AI and generative technologies, the value of descriptive analytics as a foundation for predictive modeling is often overlooked. Descriptive analytics goes beyond traditional exploratory data analysis; it leverages statistical and machine learning methods to systematically summarize, interpret, and explain the essential patterns and concepts within your business data. This process provides critical insights that should inform and guide subsequent predictive modeling efforts.\nAs datasets grow in size, summarizing them becomes increasingly essential. Traditional visual inspection methods, effective for small datasets, quickly become impractical as data scales to thousands or millions of rows. To extract meaningful insights, it is crucial to identify the most informative subsets of data, distinguishing them from less relevant portions. Both the optimization and machine learning communities have developed innovative techniques to efficiently summarize and interpret large, complex datasets.\nMany organizations are eager to implement machine learning in their business operations. However, effective application requires careful due diligence to ensure a thorough understanding of the data generated by underlying business processes. Ideally, the characteristics of the data should inform the choice of modeling approach. In practice, though, modeling techniques are often predetermined for various reasons, and the focus shifts to adapting the data to fit the chosen model, rather than the other way around.\nI started a repository with recipes for descriptive analytics. I hope to add to this incrementally. To begin with I picked a transactions dataset from a national retailer in Brazil. You can check out the narrative and notebook."
  },
  {
    "objectID": "posts/the_magic_pill/index.html",
    "href": "posts/the_magic_pill/index.html",
    "title": "The Magic Pill Approach",
    "section": "",
    "text": "This post is more of a rant, really. I’m fine with asking ChatGPT for movie recommendations on Netflix or Prime, but when it comes to answering complex, nuanced questions that demand rigor and careful validation of assumptions, I draw the line. Yet, it seems impossible to escape the ubiquitous question: “But what does ChatGPT say?”\nFoundational models for everything? Really? Let’s break this down at a basic level. Assuming I’m even solving the right problem for my use case—which is far from trivial—here’s what I aim to do when tackling a problem:\n\nIdentify sources of variation: For tasks like regression, I want to pinpoint every factor that contributes to variation in the data.\nValidate these sources: This requires rigor and due diligence to ensure they are legitimate contributors.\nAccount for unexplained variance: Ideally, this should be minimal. If it’s significant, I need to investigate further and determine how to address it. Even then, I want to ensure the model remains useful—better than having no model or sticking with the status quo.\n\nI’m skeptical of the notion that all problems can be reduced to the same sources of variation or that there’s a universal method to identify them. If such a method exists, it must be demonstrated rigorously—not through cherry-picked examples.\nSure, I understand that certain approaches work well within specific problem domains. However, what I often encounter is this: “This approach worked for situation A, so let’s try it for situation B,” without any solid justification that A and B are meaningfully similar."
  },
  {
    "objectID": "posts/graph_learning/index.html",
    "href": "posts/graph_learning/index.html",
    "title": "Unsupervised Learning and Graphs",
    "section": "",
    "text": "Graph problems that you solve routinely fall into one of the following categories:\n\nPrediction with Partial Graphs: Given a graph that is partially observed, can you predict node property values on unobserved nodes? Can you predict if an unobserved pair of nodes are connected by an edge?\nGraph Structure Inference from Node Signals: Given values or signals assigned to the nodes, can you infer the underlying graph structure? Here, you lack explicit examples of which nodes should be connected, and the challenge is to determine the connections that best explain the observed node signals.\n\nSo if you are like me, you are probably thinking that the second example is a candidate for unsupervised learning and I agree. The problem is that you will run into the problem of selecting a threshold value in your learning task. For example, if you choose to cluster nodes using the notion of similarity or distance, then you need to decide that nodes that are closer or more similar than a threshold value are connected. If you have access to domain knowledge or have a good understanding of the problem you are trying to solve, you can come up with reasonable solutions.\nWhat I wanted to talk about in this post is that you can appeal to Graph Signal Processing for help too. I am not going into the details, rather, I am going to discuss the intuition and the motivation and point you to some references if you want to find out more. The ideas are from this paper (Dong et al. 2019). There is an excellent youtube talk that covers the ideas in this paper (Dong 2019).\nGiven observations on the nodes of a graph the paper (and talk) discuss three approaches to learning the associated graph.\n\nBy choosing how smooth you want your signal to be. When a signal is observed on the nodes of a graph, the eigen decomposition of the laplacian of the graph tells us how smoothly the signal will vary over the nodes of the graph. We can control the smoothness with a regularization term, i.e., you pick a regularization term that makes the signal smooth enough for your particular problem. This is your choice as a modeler. Learning the graph laplacian translates to learning the graph. The graphical lasso (Friedman, Hastie, and Tibshirani 2008) is used to learn the laplacian.\nBy selecting how a signal should diffuse through a graph. Diffusion on graphs, associated kernels is an extensive topic. If the diffusion perspective is right for your application, for example in disease modeling, then this might appeal to you.\nBy assuming a causal structure. If you have a causal model associated with the variables of the graph, you can use this approach to learning the graph.\n\nThe smoothness idea appeals to me. If you are wondering “what did we actually solve, we just traded threshold selection for smoothness selection?”, here is connecting the dots. It turns out that the covariance matrix that we can compute from the data that we observe can be used to estimate the graph laplacian through the Graphical LASSO. You will need to select a regularization constant. Estimating this is not too difficult, for example if you are working with say temperature data recorded at physical locations, you can note the temperature readings at a set of cities and you can adjust the regularization constant so that the variation produced by the GLASSO model matches the actual observed variation. Tuning the regularization parameter to reduce the model error is a familiar task for a lot of modelers.\nThe olist geographical sales by cities in Sau Paulo for 2017 is a good candidate to explore the first approach. So I might try that."
  },
  {
    "objectID": "posts/graph_learning/index.html#unsupervised-learning-and-graphs",
    "href": "posts/graph_learning/index.html#unsupervised-learning-and-graphs",
    "title": "Unsupervised Learning and Graphs",
    "section": "",
    "text": "Graph problems that you solve routinely fall into one of the following categories:\n\nPrediction with Partial Graphs: Given a graph that is partially observed, can you predict node property values on unobserved nodes? Can you predict if an unobserved pair of nodes are connected by an edge?\nGraph Structure Inference from Node Signals: Given values or signals assigned to the nodes, can you infer the underlying graph structure? Here, you lack explicit examples of which nodes should be connected, and the challenge is to determine the connections that best explain the observed node signals.\n\nSo if you are like me, you are probably thinking that the second example is a candidate for unsupervised learning and I agree. The problem is that you will run into the problem of selecting a threshold value in your learning task. For example, if you choose to cluster nodes using the notion of similarity or distance, then you need to decide that nodes that are closer or more similar than a threshold value are connected. If you have access to domain knowledge or have a good understanding of the problem you are trying to solve, you can come up with reasonable solutions.\nWhat I wanted to talk about in this post is that you can appeal to Graph Signal Processing for help too. I am not going into the details, rather, I am going to discuss the intuition and the motivation and point you to some references if you want to find out more. The ideas are from this paper (Dong et al. 2019). There is an excellent youtube talk that covers the ideas in this paper (Dong 2019).\nGiven observations on the nodes of a graph the paper (and talk) discuss three approaches to learning the associated graph.\n\nBy choosing how smooth you want your signal to be. When a signal is observed on the nodes of a graph, the eigen decomposition of the laplacian of the graph tells us how smoothly the signal will vary over the nodes of the graph. We can control the smoothness with a regularization term, i.e., you pick a regularization term that makes the signal smooth enough for your particular problem. This is your choice as a modeler. Learning the graph laplacian translates to learning the graph. The graphical lasso (Friedman, Hastie, and Tibshirani 2008) is used to learn the laplacian.\nBy selecting how a signal should diffuse through a graph. Diffusion on graphs, associated kernels is an extensive topic. If the diffusion perspective is right for your application, for example in disease modeling, then this might appeal to you.\nBy assuming a causal structure. If you have a causal model associated with the variables of the graph, you can use this approach to learning the graph.\n\nThe smoothness idea appeals to me. If you are wondering “what did we actually solve, we just traded threshold selection for smoothness selection?”, here is connecting the dots. It turns out that the covariance matrix that we can compute from the data that we observe can be used to estimate the graph laplacian through the Graphical LASSO. You will need to select a regularization constant. Estimating this is not too difficult, for example if you are working with say temperature data recorded at physical locations, you can note the temperature readings at a set of cities and you can adjust the regularization constant so that the variation produced by the GLASSO model matches the actual observed variation. Tuning the regularization parameter to reduce the model error is a familiar task for a lot of modelers.\nThe olist geographical sales by cities in Sau Paulo for 2017 is a good candidate to explore the first approach. So I might try that."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html",
    "href": "posts/ice_cream_sales/index.html",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "For a while now I have been meaning to explore how Copilot and Chat GPT are useful for data analysis. I have also been meaning to transition to Quarto for a while, so I grabbed a dataset from Kaggle, picked some interesting questions to analyze, and threw it at Copilot and Chat GPT. Here’s my experience. I used Chat GPT to generate the content initially, so what you see here is my own writing interlaced with generated content. I used VSCode with extensions for quarto and copilot for this article.\nBy leveraging Chat GPT, we can automate and enhance various aspects of time series analysis, including data preprocessing, model selection, and interpretation of results.\n\n\n\n\n\n\nNote\n\n\n\nWhile the above statement is true, it can be somewhat misleading. The assistance we get for the tasks like data preprocessing in comparison to tasks like result interpretation and model selection is very different. Code generation for manipulating and transforming pandas dataframes was useful, I did’t really find anything useful for interpretation or model selection.\n\n\n\n\nThe data analyzed in this post is a collection of transactions from a national retailer. The transactions span major metropolitan cities in the US and cover a four and half year period. The dataset is available on Kaggle. The objective was to analyze this data to get answers to a representative set of questions. The questions addressed in this post represent a subset of a comprehensive analysis exercise, and it is not intended to be exhaustive or rigorous. Some example questions include:\n\nWhat does the plot of daily ice cream sales nationwide over the analysis period look like?\nHow do ice cream sales vary across the four years within the analysis period? Are there significant differences in the summary statistics year over year?\nIs there variability in ice cream sales based on location? In other words, is there heterogeneity in consumption patterns based on location?\nWhat does the autocorrelation function of the ice cream sales indicate? Is the time series stationary?\nWhat are the key components of the time series (trend, seasonality, etc.)?\nHow does the plot of the maximum daily ice cream sales for each week of the year look? Can we model these maximum values using a Gumbel distribution to understand the distribution of peak daily demand nationally?\n\n\n\n\n\n\n\nNote\n\n\n\nThe relevant analysis tasks (the list above) is something the analyst has to generate. In my view, this comes with experience with similar data and use cases. Asking Chat GPT on how to do time series analysis got me very general guidelines. I did not find the response very useful (see below). The ACF suggests a white noise process (which is the ground truth), Chat GPT was trying find patterns in it. A notebook implementation of most of these questions is available\n\n\n\n\n\nChat GPT Analysis\n\n\n\n\n\nTransforming the raw dataset to a format that was suitable to answer the analysis questions required an understanding of the various fields in the data. To get the daily ice cream sales from the raw transaction data, I had the identify the transactions that contained ice creams and then aggregate the transactions on a daily basis. Doing this required an analysis of how the line items for each transaction is encoded. Once the data description was analyzed, providing the prompts to Chat GPT generated code that was pretty close to what I developed manually.\n\n\n\n\n\n\nNote\n\n\n\nThe real effort in the data preparation task went into developing an understanding of the various elements of the raw dataset and then coming up with the process of identifying the transactions containing ice cream. Aggregating these transactions to a daily or weekly cadence was fairly straight forward. Chat GPT was helpful in generating the specific pandas code for each step. The manual and the generated were pretty close.\n\n\n\n\n\n\nChat GPT was definitely very helpful in developing and organizing the content.\nIdentifying the key questions that are relevant to the analysis requires formal training and experience - I don’t really see this replacing a data scientist at this point.\nChat GPT was definitely useful for various code generation tasks.\nIn summary, it was impressive as an assistant, but at this point, it is an assistant not a data scientist replacement."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#the-time-series-analysis-task",
    "href": "posts/ice_cream_sales/index.html#the-time-series-analysis-task",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "The data analyzed in this post is a collection of transactions from a national retailer. The transactions span major metropolitan cities in the US and cover a four and half year period. The dataset is available on Kaggle. The objective was to analyze this data to get answers to a representative set of questions. The questions addressed in this post represent a subset of a comprehensive analysis exercise, and it is not intended to be exhaustive or rigorous. Some example questions include:\n\nWhat does the plot of daily ice cream sales nationwide over the analysis period look like?\nHow do ice cream sales vary across the four years within the analysis period? Are there significant differences in the summary statistics year over year?\nIs there variability in ice cream sales based on location? In other words, is there heterogeneity in consumption patterns based on location?\nWhat does the autocorrelation function of the ice cream sales indicate? Is the time series stationary?\nWhat are the key components of the time series (trend, seasonality, etc.)?\nHow does the plot of the maximum daily ice cream sales for each week of the year look? Can we model these maximum values using a Gumbel distribution to understand the distribution of peak daily demand nationally?\n\n\n\n\n\n\n\nNote\n\n\n\nThe relevant analysis tasks (the list above) is something the analyst has to generate. In my view, this comes with experience with similar data and use cases. Asking Chat GPT on how to do time series analysis got me very general guidelines. I did not find the response very useful (see below). The ACF suggests a white noise process (which is the ground truth), Chat GPT was trying find patterns in it. A notebook implementation of most of these questions is available\n\n\n\n\n\nChat GPT Analysis"
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#data-preparation",
    "href": "posts/ice_cream_sales/index.html#data-preparation",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "Transforming the raw dataset to a format that was suitable to answer the analysis questions required an understanding of the various fields in the data. To get the daily ice cream sales from the raw transaction data, I had the identify the transactions that contained ice creams and then aggregate the transactions on a daily basis. Doing this required an analysis of how the line items for each transaction is encoded. Once the data description was analyzed, providing the prompts to Chat GPT generated code that was pretty close to what I developed manually.\n\n\n\n\n\n\nNote\n\n\n\nThe real effort in the data preparation task went into developing an understanding of the various elements of the raw dataset and then coming up with the process of identifying the transactions containing ice cream. Aggregating these transactions to a daily or weekly cadence was fairly straight forward. Chat GPT was helpful in generating the specific pandas code for each step. The manual and the generated were pretty close."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#summary-of-chat-gpts-utility-for-this-particular-exercise",
    "href": "posts/ice_cream_sales/index.html#summary-of-chat-gpts-utility-for-this-particular-exercise",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "Chat GPT was definitely very helpful in developing and organizing the content.\nIdentifying the key questions that are relevant to the analysis requires formal training and experience - I don’t really see this replacing a data scientist at this point.\nChat GPT was definitely useful for various code generation tasks.\nIn summary, it was impressive as an assistant, but at this point, it is an assistant not a data scientist replacement."
  },
  {
    "objectID": "posts/coffee_prices/index.html",
    "href": "posts/coffee_prices/index.html",
    "title": "Predictive Modeling without Supporting Data Analysis",
    "section": "",
    "text": "In my opinion, relying on models without prior data analysis is problematic. This is particularly true for business applications that work with regular tabular data. While it’s common practice to develop separate models for prediction and explanation, preliminary data analysis is crucial for justifying: 1. Specific modeling approaches 2. The necessity of certain features\nThis isn’t just about generating summary statistics or assessing data quality; it’s about understanding the sources of variation relevant to our predictions or estimates.\nAs a coffee connoisseur, you may have noticed the rising costs of your favorite brew. Imagine you own a small restaurant chain and need to plan your coffee purchases to manage production costs effectively. To forecast coffee prices for the next six months, you hire a data science team to build a predictive model. However, it’s essential to first understand the primary sources of price variation and any recent changes in price patterns before developing an accurate model.\nOver the past decade, the quality of libraries for statistical modeling and optimization has significantly improved. Coupled with code generation features like those provided by GitHub Copilot, the effort and cost of conducting data analysis have decreased substantially—provided you have the data analysis skills. Unfortunately, it is quite common to run into conversations today where people believe that throwing the data at an “automatic modeling” tool will give you the answer and there is really no need to understand how the result was obtained.\nFor example, here is how easy tools make this for the coffee example. Here is the raw series  Here is the decomposition of this series with LOWESS. The details of the notebook with the observations are here."
  },
  {
    "objectID": "posts/coffee_prices/index.html#the-black-box-model",
    "href": "posts/coffee_prices/index.html#the-black-box-model",
    "title": "Predictive Modeling without Supporting Data Analysis",
    "section": "",
    "text": "In my opinion, relying on models without prior data analysis is problematic. This is particularly true for business applications that work with regular tabular data. While it’s common practice to develop separate models for prediction and explanation, preliminary data analysis is crucial for justifying: 1. Specific modeling approaches 2. The necessity of certain features\nThis isn’t just about generating summary statistics or assessing data quality; it’s about understanding the sources of variation relevant to our predictions or estimates.\nAs a coffee connoisseur, you may have noticed the rising costs of your favorite brew. Imagine you own a small restaurant chain and need to plan your coffee purchases to manage production costs effectively. To forecast coffee prices for the next six months, you hire a data science team to build a predictive model. However, it’s essential to first understand the primary sources of price variation and any recent changes in price patterns before developing an accurate model.\nOver the past decade, the quality of libraries for statistical modeling and optimization has significantly improved. Coupled with code generation features like those provided by GitHub Copilot, the effort and cost of conducting data analysis have decreased substantially—provided you have the data analysis skills. Unfortunately, it is quite common to run into conversations today where people believe that throwing the data at an “automatic modeling” tool will give you the answer and there is really no need to understand how the result was obtained.\nFor example, here is how easy tools make this for the coffee example. Here is the raw series  Here is the decomposition of this series with LOWESS. The details of the notebook with the observations are here."
  },
  {
    "objectID": "posts/graph_from_relations/index.html",
    "href": "posts/graph_from_relations/index.html",
    "title": "Graph From Relations",
    "section": "",
    "text": "If you have had a chance to look at the Olist Case Study, you can see that even within one geographic region, IID (Independent Identically Distributed) is not a reasonable assumption to analyze purchasing patterns of customers. For some cities it is, but for others it is not. As discussed in the post, a reasonable approach in such scenarios is to use methods used to analyze IID data for the group of cities for which IID is a reasonable assumption. For the remainder, we can use Graph based analysis methods.\nThis pattern of finding different behaviors for different subsets of the data is a very frequent finding in real life datasets. Using a monolithic modeling approach can be sub-optimal. If we use an IID assumption for all the data, then we have a model that is not correctly specified for some parts of the data. We could use a graph based approach for the entire dataset, but we don’t need the neighborhood features for some parts of the data (the IID parts). So we have a lot of redundant computation. Doing a descriptive analytics exercise lets us pick the right modeling approach for each subset of the data.\nIf you look at all the examples, the raw dataset is tabular. This can be viewed as a mathematical relation. Conceptually, each row in the input dataset can be viewed as a set an interaction between one or more entities. In the Olist case, it is a customer’s interaction with the store’s inventory. For some subset of this input dataset, a graph representation is useful to analyze it. Since non-IID (dependent) data does occur frequently, learning a graph from a set of relations is a frequent problem.\nI have a write up about this problem and a proposed solution sketch in the descriptive analytics repository: Learning Graphs from Relations."
  },
  {
    "objectID": "posts/graph_from_relations/index.html#graphs-as-a-descriptive-analytics-tool",
    "href": "posts/graph_from_relations/index.html#graphs-as-a-descriptive-analytics-tool",
    "title": "Graph From Relations",
    "section": "",
    "text": "If you have had a chance to look at the Olist Case Study, you can see that even within one geographic region, IID (Independent Identically Distributed) is not a reasonable assumption to analyze purchasing patterns of customers. For some cities it is, but for others it is not. As discussed in the post, a reasonable approach in such scenarios is to use methods used to analyze IID data for the group of cities for which IID is a reasonable assumption. For the remainder, we can use Graph based analysis methods.\nThis pattern of finding different behaviors for different subsets of the data is a very frequent finding in real life datasets. Using a monolithic modeling approach can be sub-optimal. If we use an IID assumption for all the data, then we have a model that is not correctly specified for some parts of the data. We could use a graph based approach for the entire dataset, but we don’t need the neighborhood features for some parts of the data (the IID parts). So we have a lot of redundant computation. Doing a descriptive analytics exercise lets us pick the right modeling approach for each subset of the data.\nIf you look at all the examples, the raw dataset is tabular. This can be viewed as a mathematical relation. Conceptually, each row in the input dataset can be viewed as a set an interaction between one or more entities. In the Olist case, it is a customer’s interaction with the store’s inventory. For some subset of this input dataset, a graph representation is useful to analyze it. Since non-IID (dependent) data does occur frequently, learning a graph from a set of relations is a frequent problem.\nI have a write up about this problem and a proposed solution sketch in the descriptive analytics repository: Learning Graphs from Relations."
  },
  {
    "objectID": "posts/hmm_r8/index.html",
    "href": "posts/hmm_r8/index.html",
    "title": "Using Hidden Markov Models with the Coffee Prices Data",
    "section": "",
    "text": "I started writing about coffee prices kind of on a whim—mostly because I wanted to try out this tool called quarto that I’d heard good things about. At the same time, I was curious to see how useful ChatGPT would be when dealing with tabular data, since most of the hype is around text and image/video stuff.\nAlong the way, I ended up realizing just how much the open source community has given to the data science world—there’s a ton of amazing tools out there. For most projects, I’ve found that the bulk of my time goes into exploring the data and evaluating different ways to frame the problem. Trying out different approaches and prepping the data for each one—that’s what really eats up modeling time.\nI’m not really in the “one model to rule them all” camp—first it was deep learning, now it’s generative AI. I’m definitely curious to see what gen AI can do, but I’m keeping a healthy dose of skepticism.\nAnyway, while I was digging into all this, I stumbled across some pretty neat libraries—no regrets. One of them was hmmlearn, which is great for working with Hidden Markov Models. This post is really just me wrapping up what I started—gotta close the loop!\nHere’s a brief recap of the story so far: we can use time series decomposition to break down the coffee price data into three main components — trend-cycle, seasonal, and noise.\nIt’s worth noting that people often confuse cycles with seasonality. The key difference is that cycles don’t follow a fixed or precisely defined period, whereas seasonal patterns do. For example, a weekly seasonal pattern repeats exactly every week.\nThis distinction is why some authors — such as Hyndman (Hyndman and Athanasopoulos 2018) — prefer the term trend-cycle instead of simply trend. The decomposition process typically captures both long-term trends and irregular cycles as a single trend-cycle component. In contrast, the seasonal component represents consistent, repeating patterns and is treated separately.\n\n\n\n\n\n\n\n\n\nRaw Prices (monthly)\n\n\n\n\n\n\n\n\n\nTrend-Cycle\n\n\n\n\n\n\n\n\n\nSeasonality\n\n\n\n\n\nThe decomposition of the coffee time series using LOESS(Cleveland et al. 1990) is shown in the panel above. A review of the trend-cycle component reveals approximately \\(4\\) major cycles. The seasonal component reflects noticeable shifts beginning around 2022, likely due to the impact of COVID. I was able to identify at least \\(3\\) distinct seasonal patterns during the period.\nAltogether, this led me to expect around \\(7\\) significant changes in the series — \\(4\\) from the trend-cycle and \\(3\\)–\\(4\\) from seasonal variations — dating back to the start of the series in 1990.\n\n\n\n\n\n\nNote\n\n\n\nOne of the benefits of analysis prior to model building is that we get to understand the components that account for the variation in the data. A review of the plots for trend-cycle and seasonality shows that the trend-cycle component is what accounts for the majority of the variation in prices.\n\n\nIf you’re a commodities pricing expert and you see additional changes beyond these, I’d be very interested in learning more about your interpretation.\nThe residuals — the remaining variation after removing both trend and seasonal effects — are shown in the panel below.\n\n\n\nErrors\n\n\nRunning change point analysis gave us the changepoints represented in the series below\n\n\n\nChange Points\n\n\nIn the previous post, I mentioned that Markov Models are powerful tools for modeling stochastic processes. By discretizing regime prices into three levels — (low, medium, high) — we can apply the theory of Markov chains to estimate the probability of being in each state, known as the stationary probability.\nIn this post, I’d like to complete the loop by introducing a practical extension for monitoring sequential data like this: the Hidden Markov Model.\nInstead of discretizing prices into fixed categories like (low, medium, high), we can let the data guide the analysis using a kernel density estimator (or a histogram — here, I’ve used a KDE). In other words, the discretization is data-driven rather than predefined.\nThis analysis focuses on Regime 8, which corresponds to the current period in coffee prices. A kernel density plot of these prices is shown below.\n\n\n\nCoffee Prices, R8\n\n\nA look at the KDE above reveals that coffee prices tend to concentrate around specific values. In this case, the distribution appears bi-modal — suggesting two distinct clusters of prices. Rather than arbitrarily discretizing prices into three levels, it’s more effective to first histogram the data and then estimate the number of underlying components.\nWhat we uncover here is a 2-state Hidden Markov Model. Coffee prices are clustering around two primary levels, but the cluster membership is latent — we don’t observe it directly. Instead, what we observe at each time step is a noisy sample from one of these clusters.\nYou can think of this like monitoring patients in a hospital ward where only two health conditions are possible: condition A or condition B. As a healthcare worker doing daily rounds, you don’t get to see a label revealing a patient’s condition — instead, you observe noisy indicators like temperature and vital signs, and from those, you infer the likely condition.\nSimilarly, each month we observe a single coffee price reported by the time series publisher. Based on that value — a noisy sample — we can infer which price cluster (or state) the market is in.\nI’m not going into the details of how the Hidden Markov Model is estimated from the data here. For that, I referred to Bishop’s book (Bishop 2006), which offers a solid conceptual introduction. For those eager to dive deeper, I recommend (Rabiner 1989) and (Bilmes et al. 1998).\nBy the way, if you’re interested in Graphical Models, Bishop’s book and Jeff Bilmes’ lecture series on YouTube are both excellent resources.\nThe HMM model estimates these two clusters for us. The kernel density estimates are shown below\n\n\n\n\n\n\n\n\ncluster 0, size = 27\n\n\n\n\n\n\n\ncluster 1, size = 13\n\n\n\n\n\n\nFigure 1: Clusters from HMM Estimation\n\n\n\nThe density plots reveal that cluster 0 is characterized by a lower mean and higher variance, while cluster 1 has a higher mean and lower variance.\nIn total, there are \\(47\\) data points in the current regime (Regime 8). The first \\(40\\) points were used to estimate the Hidden Markov Model, and the fitted model was then applied to predict the cluster membership for the remaining \\(7\\) data points.\nThis is illustrated in the figure below.\n\n\n\nHMM Prediction of Price Clusters\n\n\nWhile Hidden Markov Models may not be state-of-the-art for prediction tasks, they remain incredibly useful for monitoring and understanding the underlying structure of a time series. Personally, I believe the insights they offer make them well worth the time investment.\nThat’s a wrap on the coffee prices series — thanks for following along!\n\n\n\n\n\nReferences\n\nBilmes, Jeff A et al. 1998. “A Gentle Tutorial of the EM Algorithm and Its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models.” International Computer Science Institute 4 (510): 126.\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning. Vol. 4. Springer New York. http://scholar.google.com/scholar.bib?q=info:jYxggZ6Ag1YJ:scholar.google.com/&output=citation&hl=en&as_sdt=0,5&as_vis=1&ct=citation&cd=0.\n\n\nCleveland, Robert B, William S Cleveland, Jean E McRae, Irma Terpenning, et al. 1990. “STL: A Seasonal-Trend Decomposition.” J. Off. Stat 6 (1): 3–73.\n\n\nDiebold, F. X. 2007. Elements of Forecasting. Thomson/South-Western. https://books.google.co.in/books?id=j2_HtQEACAAJ.\n\n\nDiebold, Francis X, and Glenn D Rudebusch. 2020. “Business Cycles: Durations, Dynamics, and Forecasting.”\n\n\nHyndman, Rob J, and George Athanasopoulos. 2018. Forecasting: Principles and Practice. OTexts.\n\n\nRabiner, Lawrence R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” Proceedings of the IEEE 77 (2): 257–86.\n\n\nShumway, Robert H, David S Stoffer, and David S Stoffer. 2000. Time Series Analysis and Its Applications. Vol. 3. Springer.\n\nCitationBibTeX citation:@online{sambasivan2025,\n  author = {Sambasivan, Rajiv},\n  title = {Using {Hidden} {Markov} {Models} with the {Coffee} {Prices}\n    {Data}},\n  date = {2025-04-26},\n  url = {https://rajivsam.github.io/r2ds-blog/posts/markov_analysis_coffee_prices/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSambasivan, Rajiv. 2025. “Using Hidden Markov Models with the\nCoffee Prices Data.” April 26, 2025. https://rajivsam.github.io/r2ds-blog/posts/markov_analysis_coffee_prices/."
  },
  {
    "objectID": "posts/heterogeneity_in_models/index.html",
    "href": "posts/heterogeneity_in_models/index.html",
    "title": "Heterogeneity in Modeling",
    "section": "",
    "text": "In previous posts, I have frequently mentioned the term heterogeneity. Since it is a recurring concept, I wanted to explain what I was talking about\nHeterogeneity refers to the variability we observe in what we are analyzing—either across different observation units or within the same unit over time. Let’s break this down with some examples.\nAnalyses typically fall into one of the following categories:\n\nCross-Sectional Analysis: Here, we examine multiple units at a single point in time. For example, analyzing average apartment rents across US cities in 2017 provides a snapshot comparison.\nLongitudinal Analysis: This involves tracking a single unit over time. For instance, monitoring an individual’s blood sugar levels daily for 3 years.\nPanel Data Analysis: This combines both approaches, observing multiple units over multiple time periods. For example, tracking monthly apartment rents in Chicago from 2015 to 2020.\n\nHeterogeneity appears differently in each type of analysis. In cross-sectional data, such as average rents in 2017, variability can arise from factors like: - City - Apartment features (e.g., size, amenities)\nThese are sources of heterogeneity. Model heterogeneity occurs when we observe a difference in how each of these sources relate to the rents for each city. For example if there is a linear relationship between apartment size and rent, the coefficient term in a New York State model may be different from Illinois. In some states, we may find that the relationship between size and rent is non-linear.\nIn a longitudinal setting, such as tracking an individual’s daily 7 am blood sugar over 3 years along with diet and lifestyle factors (e.g., carb intake, sleep, exercise), heterogeneity can arise over time. For instance, as the person becomes fitter, the effect of these lifestyle factors on blood sugar may change from year to year.\nIn a panel data setting, such as tracking apartment rents in Chicago over several years, heterogeneity can arise both across groups and over time. For example, the impact of free utilities on monthly rents may change from year to year, and this effect might differ between single-bedroom and two-bedroom apartments. These differences could be driven by factors like rising fuel costs, which affect heating and cooling expenses differently depending on apartment size.\nThe above discussion covers what heterogeneity is and how it manifests in different types of data. Let’s talk about how this is addressed. Heterogeneity is a well recognized problem in the statistics and economics communities. Random effects models is the standard approach to dealing with this. Please see (Faraway 2016) for a discussion of the method. Modeling your analysis with graphs is another approach to dealing heterogeneity.\nIn graph-based models, the neighborhood of a node—its most similar peers—is used to predict the value of a property at that node. This approach naturally accounts for heterogeneity, since predictions are informed by local structure and relationships. For example, the predicted rent for an apartment is influenced by the rents of apartments most similar to it; the definition of “similarity” and the size of the neighborhood are modeling choices.\nBy framing your analysis as a graph problem, you can leverage a rich body of theory and methods developed for network data. This not only provides a principled way to address heterogeneity, but also offers interpretable insights into how local relationships drive outcomes. Developing tools to map relational data into graph structures is an ongoing effort, and I am working on this."
  },
  {
    "objectID": "posts/heterogeneity_in_models/index.html#heterogeneity",
    "href": "posts/heterogeneity_in_models/index.html#heterogeneity",
    "title": "Heterogeneity in Modeling",
    "section": "",
    "text": "In previous posts, I have frequently mentioned the term heterogeneity. Since it is a recurring concept, I wanted to explain what I was talking about\nHeterogeneity refers to the variability we observe in what we are analyzing—either across different observation units or within the same unit over time. Let’s break this down with some examples.\nAnalyses typically fall into one of the following categories:\n\nCross-Sectional Analysis: Here, we examine multiple units at a single point in time. For example, analyzing average apartment rents across US cities in 2017 provides a snapshot comparison.\nLongitudinal Analysis: This involves tracking a single unit over time. For instance, monitoring an individual’s blood sugar levels daily for 3 years.\nPanel Data Analysis: This combines both approaches, observing multiple units over multiple time periods. For example, tracking monthly apartment rents in Chicago from 2015 to 2020.\n\nHeterogeneity appears differently in each type of analysis. In cross-sectional data, such as average rents in 2017, variability can arise from factors like: - City - Apartment features (e.g., size, amenities)\nThese are sources of heterogeneity. Model heterogeneity occurs when we observe a difference in how each of these sources relate to the rents for each city. For example if there is a linear relationship between apartment size and rent, the coefficient term in a New York State model may be different from Illinois. In some states, we may find that the relationship between size and rent is non-linear.\nIn a longitudinal setting, such as tracking an individual’s daily 7 am blood sugar over 3 years along with diet and lifestyle factors (e.g., carb intake, sleep, exercise), heterogeneity can arise over time. For instance, as the person becomes fitter, the effect of these lifestyle factors on blood sugar may change from year to year.\nIn a panel data setting, such as tracking apartment rents in Chicago over several years, heterogeneity can arise both across groups and over time. For example, the impact of free utilities on monthly rents may change from year to year, and this effect might differ between single-bedroom and two-bedroom apartments. These differences could be driven by factors like rising fuel costs, which affect heating and cooling expenses differently depending on apartment size.\nThe above discussion covers what heterogeneity is and how it manifests in different types of data. Let’s talk about how this is addressed. Heterogeneity is a well recognized problem in the statistics and economics communities. Random effects models is the standard approach to dealing with this. Please see (Faraway 2016) for a discussion of the method. Modeling your analysis with graphs is another approach to dealing heterogeneity.\nIn graph-based models, the neighborhood of a node—its most similar peers—is used to predict the value of a property at that node. This approach naturally accounts for heterogeneity, since predictions are informed by local structure and relationships. For example, the predicted rent for an apartment is influenced by the rents of apartments most similar to it; the definition of “similarity” and the size of the neighborhood are modeling choices.\nBy framing your analysis as a graph problem, you can leverage a rich body of theory and methods developed for network data. This not only provides a principled way to address heterogeneity, but also offers interpretable insights into how local relationships drive outcomes. Developing tools to map relational data into graph structures is an ongoing effort, and I am working on this."
  },
  {
    "objectID": "posts/accuracy_is_all/index.html",
    "href": "posts/accuracy_is_all/index.html",
    "title": "Accuracy, is that what we are after?",
    "section": "",
    "text": "Is accuracy the ultimate goal of a data scientist’s work? A recent WhatsApp forward I received portrayed data scientists as being singularly focused on model accuracy, as if it were the sole purpose of the profession. This stereotype caught me off guard because, in my experience, the reality couldn’t be further from the truth.\nFor data scientists working with business applications, particularly those dealing with tabular data, the challenges are uniquely complex. While colleagues in fields like computer vision, natural language processing, or signal processing may face their own hurdles, their datasets—images, videos, documents, or audio—often appear more structured, with well-defined tasks such as segmentation, object recognition, or speech recognition. In contrast, business data often requires grappling with ambiguous problem definitions, messy datasets, and evolving objectives. Data scientists working with business data face a unique set of challenges. Often, the task begins with a vague directive: “If you can predict X, we can improve Y. Build a model to predict X.” This raises numerous open-ended questions and assumptions that need to be clarified before meaningful progress can be made.\nIs there any preliminary evidence or pilot study suggesting that predicting X will indeed improve Y? While data science offers robust tools to validate such assumptions, in practice, we rarely get the opportunity to do so upfront. Often, the problem itself is framed incorrectly. Collaborating with your data science team to refine the problem statement and identify what truly drives Y can be far more impactful.\nOne of the most challenging aspects of data science is characterizing the dataset—understanding the factors that contribute to explaining the target variable. This complexity varies depending on the type of data you’re working with—be it longitudinal, cross-sectional, or panel data. Each type introduces unique sources of variation that must be accounted for to build meaningful models.\nComing to the accuracy question, say you have a noisy signal with which you are trying to predict something, let’s say your body temperature with a wearable device. The signal might have noise due to motion artifacts, environmental factors, or sensor limitations. In such cases, you may get a model that has wide confidence intervals for its predictions.\nThe real effort is in identifying what causes the errors - the motion artifacts, the variation in enviromental factors as well as sensor limitations. Building accurate models in this case may require sensors that are more reliable and techniques that reduce noise artifacts - the best you can do is explain why your predictions have a wide confidence interval. No amount of modeling sophistication if fixing this problem. You can throw your best neural network at it.\nAchieving good accuracy is often an iterative process. We frequently start with no baseline or baselines that obscure flawed modeling assumptions. Personally, I don’t view accuracy as the ultimate goal; instead, I focus on understanding the limitations of the current accuracy. Accuracy is a reflection of how well we understand the data in the context of the task and the engineering efforts supporting it. Improving accuracy doesn’t always mean refining the model—it might require enhancing the data itself. Data improvement often demands collaboration and buy-in from sponsors and stakeholders. So, no, a lack of high accuracy is not necessarily a reflection of your modeling skills.\nSwitching to another topic, I was thinking about what I mentioned about identifying changepoints in my post about coffee prices. In it I was talking about visually analyzing the trend and seasonalities and then counting the change points. Could I show this with more clarity? So here is take 2 on a new dataset.\nThe problem setting is as follows, we have daily transaction data from a Bakery in the UK. The dataset is from Kaggle. Suppose we are interested in the number of baguette’s that are sold daily, we have a time series associated with this.\n\n\n\nDaily Sales of Baguettes in a bakery\n\n\nAs with the coffee prices data, we run it through a time series decomposition algorithm and we get a trend cycle shown below:\n\n\n\nDaily Baguette Sales, trend-cycle\n\n\nThe seasonality is shown below:\n\n\n\nDaily Baguette Sales, seasonality\n\n\nVisual inspection definitely shows different trend-cycles. Similarly, the seasonality plot also shows different seasoanalities. The question is how do we find the number of distinct trend components from the figure? Similarly, how do we find the number of seasonality components from the figure? I subsequently realized we can estimate this by simply plotting the KDE’s for the trend and seasonality.\nHere is the KDE for the trend component:\n\n\n\nDaily Baguette Sales Trend Component\n\n\nHere is the KDE for the seasonality component:\n\n\n\nDaily Baguette Sales Seasonality Component\n\n\nExamining the trend KDE reveals two distinct clusters, indicating the presence of two separate trends during the observed period. Similarly, the seasonality KDE shows three overlapping clusters, suggesting three distinct seasonal patterns.\nIn essence, counting the peaks (modes) in the trend and seasonality KDE plots provides a straightforward way to estimate the number of changes in trends and seasonalities.\nAnd with that, we wrap up this post.\n\n\nIf you are interested in the code"
  },
  {
    "objectID": "posts/accuracy_is_all/index.html#note",
    "href": "posts/accuracy_is_all/index.html#note",
    "title": "Accuracy, is that what we are after?",
    "section": "",
    "text": "If you are interested in the code"
  },
  {
    "objectID": "posts/descriptive_analytics_repo/index.html",
    "href": "posts/descriptive_analytics_repo/index.html",
    "title": "Descriptive Analytics Repository",
    "section": "",
    "text": "I have a created a github repository for descriptive analytics. The goal of this reository is to illustrate the utility of decribing facets of your data in relation to analysis and modeling objectives. Except in a very small well defined set of scenarios, it takes a lot of contextual analysis to understand how business concepts are connected in your data. Often there are different sub-populations in your data. Achieving your objectives may need different approaches for each sub-population. LLM’s have certainly helped in code generation and in content development. However, for complex business tasks, you still need the human expert to develop the analysis approach. The repository contains an example directory with case studies from this blog as well independent examples. Each case study has a subdirectory and a markdown file that describes the case study.\nThe descriptive analytics repository"
  },
  {
    "objectID": "posts/descriptive_analytics_repo/index.html#descriptive-analytics",
    "href": "posts/descriptive_analytics_repo/index.html#descriptive-analytics",
    "title": "Descriptive Analytics Repository",
    "section": "",
    "text": "I have a created a github repository for descriptive analytics. The goal of this reository is to illustrate the utility of decribing facets of your data in relation to analysis and modeling objectives. Except in a very small well defined set of scenarios, it takes a lot of contextual analysis to understand how business concepts are connected in your data. Often there are different sub-populations in your data. Achieving your objectives may need different approaches for each sub-population. LLM’s have certainly helped in code generation and in content development. However, for complex business tasks, you still need the human expert to develop the analysis approach. The repository contains an example directory with case studies from this blog as well independent examples. Each case study has a subdirectory and a markdown file that describes the case study.\nThe descriptive analytics repository"
  },
  {
    "objectID": "posts/markov_analysis_coffee_prices/index.html",
    "href": "posts/markov_analysis_coffee_prices/index.html",
    "title": "Understanding Coffee Prices",
    "section": "",
    "text": "The question of whether I will continue paying these high prices for coffee was a “nerd itch” I simply had to scratch. Below is a plot showing the coffee prices (in cents per pound) reported at a monthly frequency.\n\n\n\nRaw Coffee Prices\n\n\nAs mentioned in the previous post on coffee prices, it’s evident that these prices follow distinct cycles in which prices rise to a peak and then decline. While some peaks are more pronounced than others, focusing on the major ones reveals seven to eight clear changepoints between 1990 and the present. To identify the beginning and end of these cycles, we can use a change point detection algorithm. In time series analysis, the process of dividing a series into segments with similar behavior is known as segmentation.\n\n\n\n\n\n\nNote\n\n\n\nI have not really explained how I arrive at the number of \\(7-8\\) changes in this post. This comes from an analysis of the decomposition of the time series into trend and seasonal components and a subsequent review of these components. If you are interested in the details, please check out the next post. An explaination is provided there.\n\n\nThe modeling approach we choose depends largely on how certain we are about the number of segments in the data. In some cases, this number is known with confidence; in others, we start with an estimate that must be refined through modeling. For the coffee price series, we have a fairly narrow estimate—between seven and eight segments. To determine the exact number and placement of these segments, we use a model selection technique. In essence, model selection helps us refine our estimate and identify the segmentation that best fits the data.\nTo identify segments in the coffee price time series, we use the PELT algorithm (Pruned Exact Linear Time)(Killick, Fearnhead, and and 2012). For a broader overview of change point detection methods, see (Truong, Oudre, and Vayatis 2020). The change point detection problem can be described as follows: we have a sequence of signal values—in this case, monthly coffee prices. Typically, prices from one month to the next are relatively consistent, but occasionally, a significant shift occurs, indicating the start of a new pattern. The goal of change point detection is to identify these moments of transition. Each segment between change points represents a distinct period of similar price behavior, also known as a price regime. When a change point is detected, it marks the beginning of a new regime.\nA statistical model is assigned to each segment—for example, a probabilistic model where each segment is assumed to be a Gaussian Process. To prevent overfitting, a regularization or penalty term is included in the model. The parameters, along with the optimal segmentation, are determined by solving an optimization problem. For a comprehensive overview of this approach, see (Truong, Oudre, and Vayatis 2020), and for details on the specific change point detection method used here, refer to (Killick, Fearnhead, and and 2012). The method is briefly summarized in the equation below.\n\\(\\sum_{i=1}^{i=m} \\text{segment cost}_i + \\beta * \\text{penalty}\\)\nIf we have \\(m\\) change points in the time series, then we have \\((m+1)\\) segments. Each seqment contributes a model component. This is the segment cost. To prevent overfitting we introduce a penalty, actually, a penalty function, \\(f(n)\\), where \\(n\\) is the length of the time series. One suggestion for \\(f(n)\\) is \\(\\log(n)\\). (Killick, Fearnhead, and and 2012) suggests \\(\\beta\\) to be a small number.\nWe need to adjust the penalty parameter to be consistent with our data. This is the crux of the model selection problem. The plot below shows the number of breakpoints (and consequently the number of cycles) as a function of the penalty. We need to use the plot below to see what value of penalty is consistent with the number of changes we see in reality. \n\n\n\n\n\n\nCorrection\n\n\n\nThere is an edit to the previous version of the post. In the previous version I had mentioned that a Gaussian kernel is used as model for the segments. This is not a reasonable choice because the points in the series are not IID (identical, independently distributed), a Gaussian Process model is a much more reasonable choice. This model captures the correlation of values between data points. As a result of this change, the results from the modeling are different from what resulted from the Gaussian kernel.\n\n\nUsing a Gaussian Process for each segment and a penalty value of \\(55\\), we get a segmentation that is shown below.\n\n\n\nChange Point Plot\n\n\nEach segment, or regime, is a sequence of months with similar prices. The summary statistics of the segments is shown below. The size column represents the length of the segment in months.\n\n\n\nRegime Summary\n\n\nA violin plot of the prices during each of the cycles is shown below\n\n\n\nViolin Plot of Prices\n\n\nIt is evident that each regime has a unimodal distribution and the Gaussian assumption seems reasonable. Strictly speaking, a Q-Q plot and a goodness of fit test are warranted.\nSegmentation gives us time periods where prices can be explained by a particular random component, say a straight line with a particular slope, or, as in this example, samples from a particular Gaussian family. The goal here is to understand behavior and identify characteristics of coffee prices, as apposed to an overt task like forecasting (Can we predict next month’s price accurately?). In other words, this is an analysis exercise as opposed to a strict model development exercise. We need a modeling method that summarize the stochastic characteristics of each regime or segment and in the process reveal insights about how prices behaved in that segment. Markov models can do this for us.\nThe approach taken here is to model prices as a markov chain. In fact, a simplification is applied. The prices for each regime are first quantized into three discrete levels: Low, Medium and High, by applying a n-tile function to the prices for that regime. As a consequence, the sequence for each regime is a sequence of discretized labels. These are the states of the markov chain for each regime. We can compute a transition matrix corresponding to these states for each regime. The transition matrix is a contingency table between successive states in each regime. Such a table can be represented as a matrix. The \\((i,j)^{\\text{th}}\\) entry of this table represents the count of transitions between predecessor state \\(i\\) and successor state \\(j\\). When we normalize the entries of this matrix by the row sum, we have the transition probablities for the regime. This is a stochastic matrix.\nMarkov chains are widely used to study, understand, and model stochastic processes across a range of fields, including economics, science, and engineering. Their versatility and broad applicability make them a powerful tool for analyzing systems governed by probabilistic behavior. For a comprehensive introduction to Markov chains, see:\n\nThe QuantEcon website(QuantEcon 2025)\nMatthew Aldridge’s course (Aldridge 2025), “Introduction to Markov Processes”\nJason Bramburger’s course (Bramburger 2025), “Introduction to Mathematical Modeling”\n\nThe stochastic matrix for the current period (now) is shown below.\n\n\n\nStochastic Matrix, R8\n\n\nIf the stochastic matrix is irreducible —meaning that it is possible to transition from any state to any other state in a finite number of steps—then the corresponding regime has a stationary distribution. This distribution represents the long-run probabilities of the prices being in each of the three states within that regime. For the current period, the stochastic matrix is indeed irreducible, and its associated stationary distribution is as follows:\n\n\n\nStationary Probablities, R8\n\n\nNow that the modeling has been described, what do the results suggest. Here is what I was able to pick.\n\nCycle Lengths Vary Widely: Coffee price cycles can range from as short as 25 months to as long as 70 months.\nRising Volatility Since 2005: The variability in prices within each cycle—measured by the standard deviation—has been increasing since around 2005 (beginning with regime R5). This trend aligns with major global disruptions such as the financial crisis, the COVID-19 pandemic, and the Ukraine conflict. The rise in variance is also clearly visible in the violin plots. Increased volatility implies that businesses could potentially reduce costs by stockpiling coffee when demand is stable, though practical constraints and regulations may limit this strategy.\nCoffee Is Currently Expensive: We are presently in a high-price regime—bad news for coffee drinkers.\nStationary Distributions Provide Insight: The stationary distribution of a regime captures the long-term probability of prices falling into different tiers. In the current regime, for example, prices are expected to be in the lowest third of the historical range 47% of the time, in the middle third 31% of the time, and in the highest third 22% of the time.\nLarge Price Swings Are Rare: Within a regime, large jumps between low and high price levels are uncommon. Transitions typically occur gradually—for example, from low to medium before reaching high. This pattern means sudden, sharp price changes from one day to the next are unlikely.\n\nThese are some of the observations I could make. If you can identify others, please let me know along with a justification and rationale for your suggestion.\nThe code for this post is available. These are grouped as follows:\n\nDownload Data\nIdentify Changepoints\nDiscretize Prices in Regimes\nMarkov Analysis"
  },
  {
    "objectID": "posts/markov_analysis_coffee_prices/index.html#analysis-of-coffee-prices-with-segmentation-and-markov-chains",
    "href": "posts/markov_analysis_coffee_prices/index.html#analysis-of-coffee-prices-with-segmentation-and-markov-chains",
    "title": "Understanding Coffee Prices",
    "section": "",
    "text": "The question of whether I will continue paying these high prices for coffee was a “nerd itch” I simply had to scratch. Below is a plot showing the coffee prices (in cents per pound) reported at a monthly frequency.\n\n\n\nRaw Coffee Prices\n\n\nAs mentioned in the previous post on coffee prices, it’s evident that these prices follow distinct cycles in which prices rise to a peak and then decline. While some peaks are more pronounced than others, focusing on the major ones reveals seven to eight clear changepoints between 1990 and the present. To identify the beginning and end of these cycles, we can use a change point detection algorithm. In time series analysis, the process of dividing a series into segments with similar behavior is known as segmentation.\n\n\n\n\n\n\nNote\n\n\n\nI have not really explained how I arrive at the number of \\(7-8\\) changes in this post. This comes from an analysis of the decomposition of the time series into trend and seasonal components and a subsequent review of these components. If you are interested in the details, please check out the next post. An explaination is provided there.\n\n\nThe modeling approach we choose depends largely on how certain we are about the number of segments in the data. In some cases, this number is known with confidence; in others, we start with an estimate that must be refined through modeling. For the coffee price series, we have a fairly narrow estimate—between seven and eight segments. To determine the exact number and placement of these segments, we use a model selection technique. In essence, model selection helps us refine our estimate and identify the segmentation that best fits the data.\nTo identify segments in the coffee price time series, we use the PELT algorithm (Pruned Exact Linear Time)(Killick, Fearnhead, and and 2012). For a broader overview of change point detection methods, see (Truong, Oudre, and Vayatis 2020). The change point detection problem can be described as follows: we have a sequence of signal values—in this case, monthly coffee prices. Typically, prices from one month to the next are relatively consistent, but occasionally, a significant shift occurs, indicating the start of a new pattern. The goal of change point detection is to identify these moments of transition. Each segment between change points represents a distinct period of similar price behavior, also known as a price regime. When a change point is detected, it marks the beginning of a new regime.\nA statistical model is assigned to each segment—for example, a probabilistic model where each segment is assumed to be a Gaussian Process. To prevent overfitting, a regularization or penalty term is included in the model. The parameters, along with the optimal segmentation, are determined by solving an optimization problem. For a comprehensive overview of this approach, see (Truong, Oudre, and Vayatis 2020), and for details on the specific change point detection method used here, refer to (Killick, Fearnhead, and and 2012). The method is briefly summarized in the equation below.\n\\(\\sum_{i=1}^{i=m} \\text{segment cost}_i + \\beta * \\text{penalty}\\)\nIf we have \\(m\\) change points in the time series, then we have \\((m+1)\\) segments. Each seqment contributes a model component. This is the segment cost. To prevent overfitting we introduce a penalty, actually, a penalty function, \\(f(n)\\), where \\(n\\) is the length of the time series. One suggestion for \\(f(n)\\) is \\(\\log(n)\\). (Killick, Fearnhead, and and 2012) suggests \\(\\beta\\) to be a small number.\nWe need to adjust the penalty parameter to be consistent with our data. This is the crux of the model selection problem. The plot below shows the number of breakpoints (and consequently the number of cycles) as a function of the penalty. We need to use the plot below to see what value of penalty is consistent with the number of changes we see in reality. \n\n\n\n\n\n\nCorrection\n\n\n\nThere is an edit to the previous version of the post. In the previous version I had mentioned that a Gaussian kernel is used as model for the segments. This is not a reasonable choice because the points in the series are not IID (identical, independently distributed), a Gaussian Process model is a much more reasonable choice. This model captures the correlation of values between data points. As a result of this change, the results from the modeling are different from what resulted from the Gaussian kernel.\n\n\nUsing a Gaussian Process for each segment and a penalty value of \\(55\\), we get a segmentation that is shown below.\n\n\n\nChange Point Plot\n\n\nEach segment, or regime, is a sequence of months with similar prices. The summary statistics of the segments is shown below. The size column represents the length of the segment in months.\n\n\n\nRegime Summary\n\n\nA violin plot of the prices during each of the cycles is shown below\n\n\n\nViolin Plot of Prices\n\n\nIt is evident that each regime has a unimodal distribution and the Gaussian assumption seems reasonable. Strictly speaking, a Q-Q plot and a goodness of fit test are warranted.\nSegmentation gives us time periods where prices can be explained by a particular random component, say a straight line with a particular slope, or, as in this example, samples from a particular Gaussian family. The goal here is to understand behavior and identify characteristics of coffee prices, as apposed to an overt task like forecasting (Can we predict next month’s price accurately?). In other words, this is an analysis exercise as opposed to a strict model development exercise. We need a modeling method that summarize the stochastic characteristics of each regime or segment and in the process reveal insights about how prices behaved in that segment. Markov models can do this for us.\nThe approach taken here is to model prices as a markov chain. In fact, a simplification is applied. The prices for each regime are first quantized into three discrete levels: Low, Medium and High, by applying a n-tile function to the prices for that regime. As a consequence, the sequence for each regime is a sequence of discretized labels. These are the states of the markov chain for each regime. We can compute a transition matrix corresponding to these states for each regime. The transition matrix is a contingency table between successive states in each regime. Such a table can be represented as a matrix. The \\((i,j)^{\\text{th}}\\) entry of this table represents the count of transitions between predecessor state \\(i\\) and successor state \\(j\\). When we normalize the entries of this matrix by the row sum, we have the transition probablities for the regime. This is a stochastic matrix.\nMarkov chains are widely used to study, understand, and model stochastic processes across a range of fields, including economics, science, and engineering. Their versatility and broad applicability make them a powerful tool for analyzing systems governed by probabilistic behavior. For a comprehensive introduction to Markov chains, see:\n\nThe QuantEcon website(QuantEcon 2025)\nMatthew Aldridge’s course (Aldridge 2025), “Introduction to Markov Processes”\nJason Bramburger’s course (Bramburger 2025), “Introduction to Mathematical Modeling”\n\nThe stochastic matrix for the current period (now) is shown below.\n\n\n\nStochastic Matrix, R8\n\n\nIf the stochastic matrix is irreducible —meaning that it is possible to transition from any state to any other state in a finite number of steps—then the corresponding regime has a stationary distribution. This distribution represents the long-run probabilities of the prices being in each of the three states within that regime. For the current period, the stochastic matrix is indeed irreducible, and its associated stationary distribution is as follows:\n\n\n\nStationary Probablities, R8\n\n\nNow that the modeling has been described, what do the results suggest. Here is what I was able to pick.\n\nCycle Lengths Vary Widely: Coffee price cycles can range from as short as 25 months to as long as 70 months.\nRising Volatility Since 2005: The variability in prices within each cycle—measured by the standard deviation—has been increasing since around 2005 (beginning with regime R5). This trend aligns with major global disruptions such as the financial crisis, the COVID-19 pandemic, and the Ukraine conflict. The rise in variance is also clearly visible in the violin plots. Increased volatility implies that businesses could potentially reduce costs by stockpiling coffee when demand is stable, though practical constraints and regulations may limit this strategy.\nCoffee Is Currently Expensive: We are presently in a high-price regime—bad news for coffee drinkers.\nStationary Distributions Provide Insight: The stationary distribution of a regime captures the long-term probability of prices falling into different tiers. In the current regime, for example, prices are expected to be in the lowest third of the historical range 47% of the time, in the middle third 31% of the time, and in the highest third 22% of the time.\nLarge Price Swings Are Rare: Within a regime, large jumps between low and high price levels are uncommon. Transitions typically occur gradually—for example, from low to medium before reaching high. This pattern means sudden, sharp price changes from one day to the next are unlikely.\n\nThese are some of the observations I could make. If you can identify others, please let me know along with a justification and rationale for your suggestion.\nThe code for this post is available. These are grouped as follows:\n\nDownload Data\nIdentify Changepoints\nDiscretize Prices in Regimes\nMarkov Analysis"
  },
  {
    "objectID": "posts/olist_SP_shopping/index.html",
    "href": "posts/olist_SP_shopping/index.html",
    "title": "Descriptive Analysis of Olist Customers in SP, 2017",
    "section": "",
    "text": "This post is the first sketch of a descriptive analysis task for the Olist dataset. Last week I had an incremental update. If you saw that, you’d see that this week’s increment has many simpilications. A lot of deletions. For those who do this, you know that this is typical. Sao Paulo turns out to be the biggest market for Olist. So this post covers just the description of the Sao Paulo market. The analysis for the other markets should be similar. Rio De Janero and Minas Gerias are other decent size geographic market segments. Customers from all other states in Brazil are small. So to get a complete picture you need analysis for the Rio De Janero and the Mias Gerias states.\nA couple of thoughts worth sharing. To go from a dataset with say 100 K records to say 10 to 20 facts is a logarithmic reduction. Of these 10 to 20 facts, some of them are irrelevant to what you are interested in analyzing the data for, some are redundant (other facts convey the same information), say 5 facts truly shape your analysis. Descriptive analytics helps us get this picture.\nHere is the summary"
  },
  {
    "objectID": "posts/olist_SP_shopping/index.html#shopping-on-olist-by-customers-in-sau-paulo-in-2017",
    "href": "posts/olist_SP_shopping/index.html#shopping-on-olist-by-customers-in-sau-paulo-in-2017",
    "title": "Descriptive Analysis of Olist Customers in SP, 2017",
    "section": "",
    "text": "This post is the first sketch of a descriptive analysis task for the Olist dataset. Last week I had an incremental update. If you saw that, you’d see that this week’s increment has many simpilications. A lot of deletions. For those who do this, you know that this is typical. Sao Paulo turns out to be the biggest market for Olist. So this post covers just the description of the Sao Paulo market. The analysis for the other markets should be similar. Rio De Janero and Minas Gerias are other decent size geographic market segments. Customers from all other states in Brazil are small. So to get a complete picture you need analysis for the Rio De Janero and the Mias Gerias states.\nA couple of thoughts worth sharing. To go from a dataset with say 100 K records to say 10 to 20 facts is a logarithmic reduction. Of these 10 to 20 facts, some of them are irrelevant to what you are interested in analyzing the data for, some are redundant (other facts convey the same information), say 5 facts truly shape your analysis. Descriptive analytics helps us get this picture.\nHere is the summary"
  },
  {
    "objectID": "posts/feature_engg_graph/index.html",
    "href": "posts/feature_engg_graph/index.html",
    "title": "Feature Engineering with Graphs",
    "section": "",
    "text": "Real estate is all about location. Graphs make it possible to do this with your learning problems. Imbalanced datasets are pervasive in real life: Modeling rare diseases and conditions, fraud detection, loan defaults, are all examples of datasets where the event of interest is rare. Using graphs to model such data make it possible to identify neighborhoods of these events and use them as features in your model. This is a simple idea, but it works. Check out an implementation of this idea to predict the performance of loans gauranteed by the SBA. The SBA 7a loan performance data is publicly available. The implementation and results are available in the descriptive analytics repository"
  },
  {
    "objectID": "posts/feature_engg_graph/index.html#feature-engineering-using-graphs",
    "href": "posts/feature_engg_graph/index.html#feature-engineering-using-graphs",
    "title": "Feature Engineering with Graphs",
    "section": "",
    "text": "Real estate is all about location. Graphs make it possible to do this with your learning problems. Imbalanced datasets are pervasive in real life: Modeling rare diseases and conditions, fraud detection, loan defaults, are all examples of datasets where the event of interest is rare. Using graphs to model such data make it possible to identify neighborhoods of these events and use them as features in your model. This is a simple idea, but it works. Check out an implementation of this idea to predict the performance of loans gauranteed by the SBA. The SBA 7a loan performance data is publicly available. The implementation and results are available in the descriptive analytics repository"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rajiv’s Data Science Blog",
    "section": "",
    "text": "Feature Engineering with Graphs\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning and Graphs\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nHeterogeneity in Modeling\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 3, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nGraph From Relations\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 27, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Coffee Prices\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 21, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Analytics Repository\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 20, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Analysis of Olist Customers in SP, 2017\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 9, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Analytics, the lost idea in the current AI hype\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 30, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Optimization\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 25, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy, is that what we are after?\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nThe Magic Pill Approach\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Hidden Markov Models with the Coffee Prices Data\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 26, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Modeling without Supporting Data Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT and Data Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data/sba_heterogeneity_analysis.html",
    "href": "data/sba_heterogeneity_analysis.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "import pandas as pd\nfp = \"../data/sba_7a_loans.csv\"\ndf = pd.read_csv(fp)\n\n/var/folders/fb/t_m5qpcj6qq85rvkh73vbxh40000gn/T/ipykernel_61763/3705498770.py:3: DtypeWarning: Columns (34,35,39) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(fp)\n\n\n\nsubset_cols = [\"BorrName\", \"BankFDICNumber\", \"BankZip\", \"BorrZip\", \"NaicsCode\", \"FranchiseCode\",\\\n               \"BusinessAge\", \"LoanStatus\", \"SBAGuaranteedApproval\"]\ndf = df[subset_cols]\n\n\nfilter_PIF = (df.LoanStatus == \"PIF\")\nfilter_CHGOFF = (df.LoanStatus == \"CHGOFF\")\nfilter_criteria = filter_PIF | filter_CHGOFF\ndf = df[filter_criteria].reset_index(drop=True)\n\n\ndf_missing_vals = pd.DataFrame.from_dict({c:df[c].isnull().sum() for c in subset_cols if df[c].isnull().sum() &gt; 0},\\\n                                         orient=\"index\").reset_index()\ndf_missing_vals.columns = [\"Attribute\", \"Missing Value Count\"]\ndf_missing_vals\n\n\n\n\n\n\n\n\nAttribute\nMissing Value Count\n\n\n\n\n0\nBankFDICNumber\n2122\n\n\n1\nFranchiseCode\n19923\n\n\n2\nBusinessAge\n53\n\n\n\n\n\n\n\n\nfor a in df_missing_vals[\"Attribute\"]:\n    df[a] = df[a].fillna(\"Not Applicable\")\n\n\n{c:df[c].isnull().sum() for c in subset_cols if df[c].isnull().sum() &gt; 0}\n\n{}\n\n\n\ndf[\"BankFDICNumber\"] = df[\"BankFDICNumber\"].apply(lambda x: x if x == \"Not Applicable\" else int(x))\ndf[\"NaicsCode\"] = df[\"NaicsCode\"].apply(lambda x: x if x == \"Not Applicable\" else int(x))\ndtypes_toset = {\"BorrZip\": 'str', \"BankZip\": \"str\", \"BankFDICNumber\": 'str',\\\n                \"NaicsCode\": 'str', \"FranchiseCode\": 'str', \\\n                \"BusinessAge\" : 'str', \"LoanStatus\": 'str', \"SBAGuaranteedApproval\" : float}\ndf = df.astype(dtypes_toset)\n\n\ndf\n\n\n\n\n\n\n\n\nBorrName\nBankFDICNumber\nBankZip\nBorrZip\nNaicsCode\nFranchiseCode\nBusinessAge\nLoanStatus\nSBAGuaranteedApproval\n\n\n\n\n0\nAllen Foot and Ankle Medicine\n57512\n85004\n85212\n621391\nNot Applicable\nExisting or more than 2 years old\nPIF\n175000.0\n\n\n1\nTown Cleaners\n24170\n90010\n22201\n812320\nNot Applicable\nExisting or more than 2 years old\nCHGOFF\n11000.0\n\n\n2\nMoor Inc.\n628\n43240\n94061\n445110\nNot Applicable\nExisting or more than 2 years old\nPIF\n24500.0\n\n\n3\nMimo Lash And Skin LLC\n26610\n90010\n7030\n812199\nNot Applicable\nExisting or more than 2 years old\nPIF\n28500.0\n\n\n4\nMoor Inc.\n628\n43240\n94061\n445110\nNot Applicable\nExisting or more than 2 years old\nPIF\n50500.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22956\nJK Freight LLC\n6560\n43215\n46239\n484121\nNot Applicable\nNew Business or 2 years or less\nPIF\n12500.0\n\n\n22957\nMuirhead Group LLC\n12633\n65101\n65255\n722513\nNot Applicable\nExisting or more than 2 years old\nPIF\n38900.0\n\n\n22958\nJP ELECTRICAL CONTRACTOR\n34968\n918\n674\n238390\nNot Applicable\nExisting or more than 2 years old\nPIF\n25000.0\n\n\n22959\nANOUSHEH ASHOURI INC.\nNot Applicable\n7922\n92618\n621111\nNot Applicable\nExisting or more than 2 years old\nPIF\n22500.0\n\n\n22960\nFur Fluffs Sake LLC\nNot Applicable\n1752\n1752\n812910\nNot Applicable\nNew Business or 2 years or less\nPIF\n31450.0\n\n\n\n\n22961 rows × 9 columns\n\n\n\n\ngb = df.groupby([\"NaicsCode\", \"LoanStatus\"], as_index=False)\n\n\ndfnaics_summ = gb.size()\n\n\ndfnaics_summ = dfnaics_summ.sort_values(by=[\"NaicsCode\"])\n\n\n#dfnaics_summ[(dfnaics_summ[\"NaicsCode\"].value_counts() == 1)]\n\n\nnaics_filter = (dfnaics_summ[\"NaicsCode\"].value_counts() == 1)\n\n\ndfnaics_vc = dfnaics_summ[\"NaicsCode\"].value_counts().reset_index()\n\n\ndfnaics_vc.columns = [\"NaicsCode\", \"LoanStatusCard\"]\n\n\nfilter_ss_naics_codes = (dfnaics_vc.LoanStatusCard == 1)\nss_naics_codes = dfnaics_vc[filter_ss_naics_codes][\"NaicsCode\"]\n\n\ndfnaics_vc[~filter_ss_naics_codes]\n\n\n\n\n\n\n\n\nNaicsCode\nLoanStatusCard\n\n\n\n\n0\n442291\n2\n\n\n1\n541370\n2\n\n\n2\n532310\n2\n\n\n3\n532490\n2\n\n\n4\n541110\n2\n\n\n...\n...\n...\n\n\n272\n611699\n2\n\n\n273\n621310\n2\n\n\n274\n611691\n2\n\n\n275\n321999\n2\n\n\n276\n621999\n2\n\n\n\n\n277 rows × 2 columns\n\n\n\n\ndf[df[\"NaicsCode\"].isin(ss_naics_codes)][\"LoanStatus\"].value_counts()\n\nPIF       4757\nCHGOFF       8\nName: LoanStatus, dtype: int64\n\n\n\ndf[df[\"NaicsCode\"].isin(ss_naics_codes)][\"NaicsCode\"].value_counts()\n\n447110    250\n531130    144\n722410    124\n623312     92\n541512     90\n         ... \n111419      1\n336411      1\n561591      1\n111140      1\n459120      1\nName: NaicsCode, Length: 583, dtype: int64\n\n\n\ngb = df.groupby([\"BankZip\", \"LoanStatus\"], as_index=False)\n\n\ndfbank_zip_summ = gb.size()\ndfbank_zip_summ = dfbank_zip_summ.sort_values(by=[\"BankZip\"])\n\n\nbankzip_filter = (dfbank_zip_summ[\"BankZip\"].value_counts() == 1)\n\n\ndfbank_zip_vc = dfbank_zip_summ[\"BankZip\"].value_counts().reset_index()\ndfbank_zip_vc.columns = [\"BankZip\", \"LoanStatusCard\"]\nfilter_ss_bankzip = (dfbank_zip_vc.LoanStatusCard == 1)\nss_bankzip_codes = dfbank_zip_vc[filter_ss_bankzip][\"BankZip\"]\n\n\ndf[df[\"BankZip\"].isin(ss_bankzip_codes)][\"LoanStatus\"].value_counts()\n\nPIF       8300\nCHGOFF       9\nName: LoanStatus, dtype: int64\n\n\n\ndf_trim = df[df[\"BankZip\"].isin(ss_bankzip_codes)].index.union(df[df[\"NaicsCode\"].isin(ss_naics_codes)].index)\n\n\ndf_trim.shape\n\n(10909,)\n\n\n\ngb = df.groupby([\"BorrZip\", \"LoanStatus\"], as_index=False)\ndfborr_zip_summ = gb.size()\ndfborr_zip_summ = dfborr_zip_summ.sort_values(by=[\"BorrZip\"])\n\n\nborrzip_filter = (dfborr_zip_summ[\"BorrZip\"].value_counts() == 1)\n\n\ndfborr_zip_vc = dfborr_zip_summ[\"BorrZip\"].value_counts().reset_index()\ndfborr_zip_vc.columns = [\"BorrZip\", \"LoanStatusCard\"]\nfilter_ss_borrzip = (dfborr_zip_vc.LoanStatusCard == 1)\nss_borrzip_codes = dfborr_zip_vc[filter_ss_borrzip][\"BorrZip\"]\n\n\ndf[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"LoanStatus\"].value_counts()\n\nPIF       19549\nCHGOFF      250\nName: LoanStatus, dtype: int64\n\n\n\ndf[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"BorrZip\"].value_counts()\n\n39428    35\n56601    27\n26508    22\n67601    22\n58102    21\n         ..\n48159     1\n65483     1\n12168     1\n29652     1\n65255     1\nName: BorrZip, Length: 8361, dtype: int64\n\n\n\ndf_borr_zip_counts_summ = df[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"BorrZip\"].value_counts().reset_index()\n\n\ndf_borr_zip_counts_summ.columns = [\"BorrZip\", \"num_recs\"]\n\n\ndf_borr_zip_counts_summ[df_borr_zip_counts_summ.num_recs == 1]\n\n\n\n\n\n\n\n\nBorrZip\nnum_recs\n\n\n\n\n4340\n14011\n1\n\n\n4341\n47441\n1\n\n\n4342\n66083\n1\n\n\n4343\n53522\n1\n\n\n4344\n64740\n1\n\n\n...\n...\n...\n\n\n8356\n48159\n1\n\n\n8357\n65483\n1\n\n\n8358\n12168\n1\n\n\n8359\n29652\n1\n\n\n8360\n65255\n1\n\n\n\n\n4021 rows × 2 columns\n\n\n\n\nlen(df[\"BorrZip\"].unique())\n\n9057"
  },
  {
    "objectID": "notebooks/bakery_sales_data.html",
    "href": "notebooks/bakery_sales_data.html",
    "title": "Dependency Check",
    "section": "",
    "text": "import kagglehub\nfrom kagglehub import KaggleDatasetAdapter\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"matthieugimbert/french-bakery-daily-sales\")\n\nprint(\"Path to dataset files:\", path)\nimport pandas as pd\n# kagglehub, please make this simpler, provide some example, really poor documentation for the api\"\nfp = path + \"/Bakery Sales.csv\"\ndf = pd.read_csv(fp)\ncols = df.columns.tolist()\ncols.remove(\"Unnamed: 0\")\ndf = df[cols]\ndf.head()\nfilter_baguette = (df.article == \"BAGUETTE\")\ndf[\"datetime\"] = pd.to_datetime(df[\"date\"]  + \" \" + df[\"time\"])\ndf_baguette = df[filter_baguette]\ncols = [\"datetime\", \"Quantity\"]\ndf_baguette = df_baguette[cols]\ndf_daily_baugette = df_baguette.set_index(\"datetime\").resample(\"D\").sum()\ndf_weekly_baugette = df_baguette.set_index(\"datetime\").resample(\"W\").sum()\ndf_weekly_baugette = df_weekly_baugette.reset_index()\ndf_weekly_baugette\nzero_counts = (df_daily_baugette.Quantity == 0)\ndf_daily_baugette[zero_counts].shape\ndf_daily_baugette = df_daily_baugette.reset_index()\ndf_daily_baugette[\"Day\"] = df_daily_baugette.index\ndf_daily_baugette[\"DOW\"] = df_daily_baugette.datetime.dt.day_of_week\ndf_daily_baugette[\"month\"] = df_daily_baugette.datetime.dt.month\ndf_daily_baugette[\"WOY\"] = df_daily_baugette.datetime.dt.isocalendar().week\ncols = [\"Day\",\"datetime\", \"DOW\", \"month\", \"WOY\", \"Quantity\"]\ndf_daily_baugette = df_daily_baugette[cols]\ndf_daily_baugette.shape\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(12, 6))\ndf_daily_baugette[\"Quantity\"].plot()\nThe purpose of the STL decomposition is to identify underlying components of the series. The purpose of the auto-correlation plots is to get another perspective on the dependence structure. This tells us the variance components that we have to account for if we are going to build a forecasting model. Note that for demand estimation, we are not actually forecasting anything, we need a statistical characterization of the demand independent of the sequence order of demand arrival, we only care that the net demand is right, we don’t care how it arrives. In forecasting, we do and we have to account for it in the model.\nfrom statsmodels.tsa.seasonal import STL\ndaily_baugette_sales = pd.Series(df_daily_baugette[\"Quantity\"].values, index=df_daily_baugette[\"datetime\"])\nstl = STL(daily_baugette_sales, period=7)\nres = stl.fit()\ndecomp_res = {\"Trend\": res._trend, \"Seasonality\": res._seasonal, \"Noise\": res._resid}\ndf_res = pd.DataFrame.from_dict(decomp_res, orient=\"columns\")\ndf_res = df_res.reset_index()\ndf_res[\"Day\"] = df_res.index + 1\ndf_res"
  },
  {
    "objectID": "notebooks/bakery_sales_data.html#zero-check",
    "href": "notebooks/bakery_sales_data.html#zero-check",
    "title": "Dependency Check",
    "section": "Zero Check",
    "text": "Zero Check\nJust want to check if it is a count process dominated by zeros, or if zeros are small in number.\n\nzero_counts = (df_daily_baugette.Quantity == 0)\ndf_daily_baugette[zero_counts].shape\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.line(df_daily_baugette, x='datetime', y=\"Quantity\")\nfig.update_layout(\n    autosize=False,\n    width=1100,\n    height=800,\n)\nfig.show()\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.line(df_res, x='datetime', y=\"Trend\", title=\"Trend Cycle Component of Daily Baugette Sales\",\n             labels = {\"Trend\": \"Number of Baugettes\", \"datetime\": \"date\"})# Using plotly.express\nfig.update_layout(\n    autosize=False,\n    width=1100,\n    height=800,\n)\nfig.show()\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.line(df_res, x='datetime', y=\"Seasonality\", title=\"Seasonality Component of Daily Baugette Sales\",\n             labels = {\"Trend\": \"Number of Baugettes\", \"datetime\": \"date\"})# Using plotly.express\nfig.update_layout(\n    autosize=False,\n    width=1100,\n    height=800,\n)\nfig.show()\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.line(df_res, x='datetime', y=\"Noise\", title=\"Noise Component of Daily Baugette Sales\",\n             labels = {\"Noise\": \"Number of Baugettes\", \"datetime\": \"date\"})# Using plotly.express\nfig.update_layout(\n    autosize=False,\n    width=1100,\n    height=800,\n)\nfig.show()\n\n\ndf_res[\"Trend\"].plot.kde()\nplt.grid(True)\n\n\ndf_res[\"Seasonality\"].plot.kde()\nplt.grid(True)\n\n\nimport statsmodels.api as sm\n\n\nplt.rc(\"figure\", figsize=(12,8))\nacf_plot = sm.graphics.tsa.plot_acf(df_daily_baugette[\"Quantity\"] , lags=40)\nplt.grid(True)\n\n\nplt.rc(\"figure\", figsize=(12,8))\npacf_plot = sm.graphics.tsa.plot_pacf(df_daily_baugette[\"Quantity\"], lags=40, method=\"ywm\")\nplt.grid(True)\n\n\ndf_baguette = df_baguette.reset_index(drop=True)\n\n\ndf_baguette = df_baguette.rename(columns={\"index\": \"Day\"})\n\n\ndf_baguette[\"Day\"] = df_baguette.index + 1\ndf_baguette[\"DOW\"] = df_baguette.datetime.dt.day_of_week\ndf_baguette[\"month\"] = df_baguette.datetime.dt.month\n\n\ndf_daily_baugette[\"CWOY\"] = df_daily_baugette.apply(lambda x: x[\"WOY\"] + 53 if x.datetime.year == 2022 else x[\"WOY\"], axis=1)\n\n\ndf_daily_baugette[\"CWOY\"].max()\n\n\ndf_daily_baugette\n\n\nfp = \"../data/daily_baugette_sales.csv\"\ndf_daily_baugette.to_csv(fp, index=False)\n\n\ndf_period_perf  = pd.pivot_table(df_daily_baugette, index= \"CWOY\", columns= \"DOW\", values = \"Quantity\", fill_value=0) \n\n\nHOURS_OPEN = 1\ndf_period_perf = df_period_perf.apply(lambda x: x.div(HOURS_OPEN), axis=1).round(3).reset_index()\n\n\ndf_period_perf = df_period_perf.melt(id_vars=[\"CWOY\"], value_vars=[i for i in range(7)], value_name=\"Quantity\")\n\n\ndf_period_perf\n\n\ndf_period_perf.Quantity.plot.kde()\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.scatter(df_period_perf, x='CWOY', y=\"Quantity\")\nfig.update_layout(\n    autosize=False,\n    width=1100,\n    height=800,\n)\nfig.show()"
  },
  {
    "objectID": "notebooks/coffee_prices.html",
    "href": "notebooks/coffee_prices.html",
    "title": "Observations",
    "section": "",
    "text": "import pandas as pd\nfp = \"../data/coffee_prices_index.csv\"\ndf = pd.read_csv(fp)\ndf.columns = [\"date\", \"cindex\"]\ndf[\"date\"] = pd.to_datetime(df.date)\ndf[\"cindex\"] = df[\"cindex\"].astype(float).round(3)\n%matplotlib inline\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nplt.plot(df.date,df.cindex)\nplt.grid(True)\nplt.xlabel(\"Year\")\nplt.ylabel(\"cents per pound\")\nplt.title (\"Price of mild arabica coffee\") # from https://fred.stlouisfed.org/series/PCOFFOTMUSDM\n#plt.xlabel(df[\"date\"])\nsm.graphics.tsa.plot_acf(df.cindex, lags=24);\ncoffee_prices = pd.Series(df.cindex.values, index=df.date)\ncoffee_prices"
  },
  {
    "objectID": "notebooks/coffee_prices.html#observations",
    "href": "notebooks/coffee_prices.html#observations",
    "title": "Observations",
    "section": "Observations",
    "text": "Observations\nPost-covid, there has been a stronger trend and larger seasonal variations. We have been experiencing this at the grocery stores and every where else. It looks like pre-covid, though prices did have trend-cycles and seasonality, they were gradual and similar. I am not economist, so I don’t know the answers. The point here is that the right data tools can surface the problems that need analysis. It also provides a basis for determing the right characteristics we need to account for in downstream analysis like forecasting. Building predictive models without rigorous data analysis to document evidence for sources of variation we need to account for is like carpet bombing or driving blind. You are either using too much of computational sophistication, or, if you get a reasonable answer, you are just lucky that you picked a model that had the right features.\n\nfrom statsmodels.tsa.seasonal import STL\n\nstl = STL(coffee_prices, period=12)\nres = stl.fit()\n#fig = res.plot()\n\n\n\ndecomp_res = {\"Trend\": res._trend, \"Seasonality\": res._seasonal, \"Noise\": res._resid}\ndf_res = pd.DataFrame.from_dict(decomp_res, orient=\"columns\")\n\n\ndf_res = df_res.reset_index()\ndf_res\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.line(df_res, x='date', y=\"Trend\", title=\"Trend Cycle Component of Coffee Prices\",\n             labels = {\"Trend\": \"cents per pound\", \"date\": \"date\"})# Using plotly.express\nfig.show()\n\n\nfig = px.line(df_res, x='date', y=\"Seasonality\", title=\"Seasonality Component of Coffee Prices\",\n             labels = {\"Seasonality\": \"cents per pound\", \"date\": \"date\"})# Using plotly.express\nfig.show()\n\n\nfig = px.line(df_res, x='date', y=\"Noise\", title=\"Noise Component of Coffee Prices\",\n             labels = {\"Noise\": \"cents per pound\", \"date\": \"date\"})# Using plotly.express\nfig.show()"
  },
  {
    "objectID": "notebooks/coffee_prices.html#errors-post-decomposition",
    "href": "notebooks/coffee_prices.html#errors-post-decomposition",
    "title": "Observations",
    "section": "Errors Post Decomposition",
    "text": "Errors Post Decomposition\n\nfrom matplotlib import pyplot as plt\nres.resid.plot.kde() # looks reasonable\nplt.grid(True)"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html",
    "href": "notebooks/daily_ice_cream_sales_exploration.html",
    "title": "Analysis Notebook",
    "section": "",
    "text": "The purpose of this notebook is to analyze the daily sales of ice creams at metro areas in the USA. Subsets corresponding to yearly sales are also profiled. Plots of auto-correlation and maximum daily sales per week are also provided.\nimport pandas as pd\nimport numpy as np\nfp = \"../data/daily_ice_cream_sales.csv\"\ndf = pd.read_csv(fp)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"log_daily_sales\"] = np.log(df[\"ice_cream_purchases\"])\ndf[\"Date\"].max()\ndf[\"Date\"].min()\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#extract-yearly-data-subsets",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#extract-yearly-data-subsets",
    "title": "Analysis Notebook",
    "section": "Extract yearly data subsets",
    "text": "Extract yearly data subsets\n\ndf_2020 = df[df.Date.dt.year == 2020]\ndf_2021 = df[df.Date.dt.year == 2021]\ndf_2022 = df[df.Date.dt.year == 2022]\ndf_2023 = df[df.Date.dt.year == 2023]\n\n\n%matplotlib inline\nimport statsmodels.api as sm\n\n\ndf_2020[\"log_daily_sales\"].plot()"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#plot-autocorrelation",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#plot-autocorrelation",
    "title": "Analysis Notebook",
    "section": "Plot Autocorrelation",
    "text": "Plot Autocorrelation\nThe auto correlation plots are shown below.\n\nsm.graphics.tsa.plot_acf(df_2020[\"log_daily_sales\"], lags=40)\n\n\nsm.graphics.tsa.plot_pacf(df_2020[\"log_daily_sales\"], lags=40, method=\"ywm\")\n\n\nsm.graphics.tsa.plot_pacf(df[\"log_daily_sales\"], lags=40, method=\"ywm\")"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#observation",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#observation",
    "title": "Analysis Notebook",
    "section": "Observation",
    "text": "Observation\nIt appears that daily sales are not correlated, but independent draws from a distribution. This looks like a white noise process. This is consistent with synthetically generated data. A dickey fuller test with no regression (constant, this is the default) also suggests the same.\n\nfrom statsmodels.tsa.stattools import adfuller\nresult = adfuller(df[\"log_daily_sales\"])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\n\n\ndf_2020\n\n\n\ndf[\"log_daily_sales\"] = np.log(df[\"ice_cream_purchases\"])\n\n\ndf_yearly_ice_cream_sales = df.set_index(\"Date\").resample(\"Y\").sum()\n\n\ndf_yearly_ice_cream_sales\n\n\ndf[\"ice_cream_purchases\"].plot()\n\n\nfpmeanweekly = \"../data/mean_weekly_ice_cream_sales.csv\"\ndfwmics = pd.read_csv(fpmeanweekly)\n\n\ndfwmics[\"ice_cream_purchases\"].plot()\n\n\ndfwmics[\"weekno\"] = range(1,dfwmics.shape[0] + 1)\n\n\ndfwmics"
  },
  {
    "objectID": "notebooks/coffee_prices_change_point.html",
    "href": "notebooks/coffee_prices_change_point.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nfp = \"../data/cp_data.csv\"\ndf = pd.read_csv(fp)\ncols = [\"date\", \"cents_per_lb\"]\ndf = df[cols]\n\n\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 422 entries, 0 to 421\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   date          422 non-null    datetime64[ns]\n 1   cents_per_lb  422 non-null    float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 6.7 KB\n\n\n\ndf\n\n\n\n\n\n\n\n\ndate\ncents_per_lb\n\n\n\n\n0\n1990-01-01\n75.830\n\n\n1\n1990-02-01\n84.010\n\n\n2\n1990-03-01\n93.960\n\n\n3\n1990-04-01\n93.730\n\n\n4\n1990-05-01\n92.020\n\n\n...\n...\n...\n\n\n417\n2024-10-01\n276.777\n\n\n418\n2024-11-01\n304.953\n\n\n419\n2024-12-01\n344.119\n\n\n420\n2025-01-01\n353.933\n\n\n421\n2025-02-01\n409.516\n\n\n\n\n422 rows × 2 columns\n\n\n\n\n# Using plotly.express\nimport plotly.express as px\nfig = px.line(df, x='date', y=\"cents_per_lb\")\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\ncoffee prices have cycles as is evident from the above chart.\nEach hill like pattern we observe in the graph is a cycle. From visual inspection we can count 6 to 7 cycles in coffee prices between 1990 and the present.\nWe will have to apply rigorous statistical methodology to determine the end points of each cycle. The visual data exploration provides us a basic estimate of how many cycles we can expect to see in this data. The statistical modeling technique provides us the number of cycles.\nThe statistical methodology applied assumes that prices are a Gaussian Process. It applies the change point detection method developed by Killick and available in the ruptures python package. Please see this page for the gaussian process cost function implementation.\n\n\nimport ruptures as rpt\n#model = \"l1\"  # \"l2\", \"rbf\"\nalgo = rpt.Pelt(custom_cost=rpt.costs.CostNormal(), min_size=10).fit(df[\"cents_per_lb\"].values)\nmy_bkps = algo.predict(pen=200)\n\nConduct an experiment to determine how the number of change points changes with regularization penalty parameter. Killick suggests that this is ideally \\(k \\log(n)\\) where \\(n\\) is the number of data points (which is 422 here) and \\(k\\) is a small number. By running the experiment below and visually inspecting the number of cycles, a penalty value of 55 seems about right for this dataset. Edit: Now use a GP to model\n\nimport numpy as np\npen = np.arange(2,62,2)\npen_res = {}\n\nmodel = \"rbf\"  # \"l2\", \"rbf\"\nfor p in pen:\n    algo = rpt.Pelt(custom_cost=rpt.costs.CostNormal(), min_size=12).fit(df[\"cents_per_lb\"].values)\n    my_bkps = algo.predict(pen=p)\n    pen_res[p] = my_bkps\n\n    \n\n\nnum_bks = { int(p): len(pen_res[p]) for p in pen}\n\n\ndf_bkps = pd.DataFrame.from_dict(num_bks,orient='index').reset_index()\ndf_bkps.columns = [\"penalty\", \"num_breaks\"]\n\n\nfig = px.line(df_bkps, x='penalty', y='num_breaks', markers=True)\nfig.show()\n\n                            \n                                            \n\n\n\nnp.log2(df.shape[0])\n\nnp.float64(8.721099188707186)\n\n\nKillick’s paper recommends \\(k \\log(n)\\) for the penalty. Here \\(k\\) is a small constant and \\(n\\) is the number of datapoints. A plot of penalty versus the number of break points detected is shown in the above plot. A penalty of about \\(6\\) is consistent with what we observe visually in the plot of the time series.\n\nif 'pen_res' in globals():\n    del pen_res\nalgo = rpt.Pelt(custom_cost=rpt.costs.CostNormal(), min_size=12).fit(df[\"cents_per_lb\"].values)\nmy_bkps = algo.predict(pen=55)\n\n\nlen(my_bkps)\n\n8\n\n\n\nmy_bkps\n\n[50, 100, 125, 180, 235, 305, 375, 422]\n\n\n\nfor i in range(len(my_bkps)):\n    if i == 0:\n        df.loc[: my_bkps[i], \"regime\"] = \"R-\" + str(i+1)\n    elif ( ( i &gt; 0) and (i &lt;= (len(my_bkps) - 1))):\n        df.loc[ my_bkps[ (i - 1)] : my_bkps[i], \"regime\"] = \"R-\" + str(i+1)\n    else:\n        print(\"last breakpoint, do nothing!\")\n\n\ndf\n\n\n\n\n\n\n\n\ndate\ncents_per_lb\nregime\n\n\n\n\n0\n1990-01-01\n75.830\nR-1\n\n\n1\n1990-02-01\n84.010\nR-1\n\n\n2\n1990-03-01\n93.960\nR-1\n\n\n3\n1990-04-01\n93.730\nR-1\n\n\n4\n1990-05-01\n92.020\nR-1\n\n\n...\n...\n...\n...\n\n\n417\n2024-10-01\n276.777\nR-8\n\n\n418\n2024-11-01\n304.953\nR-8\n\n\n419\n2024-12-01\n344.119\nR-8\n\n\n420\n2025-01-01\n353.933\nR-8\n\n\n421\n2025-02-01\n409.516\nR-8\n\n\n\n\n422 rows × 3 columns\n\n\n\nA line plot that marks each regime with a separate color\n\nfig = px.line(df, x='date', y='cents_per_lb', color='regime', markers=True)\nfig.show()\n\n                            \n                                            \n\n\nA violin plot to confirm that each regime corresponds to a single density (not too multi modal) and summarize the ranges for each regime.\n\nfig = px.violin(df, y=\"cents_per_lb\", x=\"regime\", color=\"regime\", box=True, points=\"all\",\n          hover_data=df.columns)\nfig.show()\n\n                            \n                                            \n\n\nWrite the prepared datafile to disk.\n\ndf.groupby(\"regime\").agg( size = pd.NamedAgg(column=\"regime\", aggfunc=\"size\"),\\\n                         min_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"min\"),\\\n                        max_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"max\"),\\\n                        mean_price = pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"mean\"),\\\n                        std_dev_price =  pd.NamedAgg(column=\"cents_per_lb\", aggfunc=\"std\")).round(2)\n\n\n\n\n\n\n\n\nsize\nmin_price\nmax_price\nmean_price\nstd_dev_price\n\n\nregime\n\n\n\n\n\n\n\n\n\nR-1\n50\n50.83\n94.92\n76.94\n12.83\n\n\nR-2\n50\n87.02\n265.61\n156.20\n37.78\n\n\nR-3\n25\n84.52\n138.94\n108.44\n12.72\n\n\nR-4\n55\n54.36\n104.39\n68.45\n9.94\n\n\nR-5\n55\n99.49\n157.29\n124.97\n14.43\n\n\nR-6\n70\n122.02\n300.48\n193.37\n46.80\n\n\nR-7\n70\n120.55\n184.12\n147.23\n15.16\n\n\nR-8\n47\n168.65\n409.52\n241.02\n47.27\n\n\n\n\n\n\n\n\nfp = \"../data/regimed_coffee_prices.csv\"\ndf.to_csv(fp, index=False, header=True)"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html",
    "href": "notebooks/retail_transaction_data.html",
    "title": "Data Preparation Notebook",
    "section": "",
    "text": "The data for this notebook is from Kaggle. The data looks like it is synthetic, the characteristics observed in the dataset also seem to suggest that. The objectives for this notebook are to:\nimport pandas as pd\nfp = \"../data/Retail_Transactions_Dataset.csv\"\ndf = pd.read_csv(fp)\ndf.head()"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#profile-the-categorical-columns",
    "href": "notebooks/retail_transaction_data.html#profile-the-categorical-columns",
    "title": "Data Preparation Notebook",
    "section": "Profile the categorical columns",
    "text": "Profile the categorical columns\n\ndf.columns\n\n\ncategory_cols = ['Payment_Method', 'City', 'Store_Type',\\\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion']\n\n\ndf[category_cols] = df[category_cols].astype('category')\n\n\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n\ndf.dtypes\n\n\ndf[\"Customer_Category\"].value_counts()\n\n\ndf[\"City\"].value_counts()"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#observation",
    "href": "notebooks/retail_transaction_data.html#observation",
    "title": "Data Preparation Notebook",
    "section": "Observation",
    "text": "Observation\nThe counts for each of the metro areas are very similar, the counts for each of the customer categories are very similar, so this dataset was probably synthetically generated.\n\ndf[\"Product\"] = df[\"Product\"].apply(eval)\n\n\npurchase_summ = {}\nfor index, row in df[\"Product\"].items():\n    for p in row:\n        if p in purchase_summ:\n            purchase_summ[p] += 1\n        else:\n            purchase_summ[p] = 1"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#extract-the-ice-cream-transactions",
    "href": "notebooks/retail_transaction_data.html#extract-the-ice-cream-transactions",
    "title": "Data Preparation Notebook",
    "section": "Extract the ice cream transactions",
    "text": "Extract the ice cream transactions\n\ndef is_ice_cream(row):\n    for p in row:\n        if p == \"Ice Cream\":\n            return True\n    return False\ndf[\"is_ice_cream\"] = df[\"Product\"].apply(is_ice_cream)\n\n\ndf_ice_cream_trans = df[df[\"is_ice_cream\"]].reset_index()\nreq_cols = [\"Date\"]\ndf_ice_cream_trans = df_ice_cream_trans[req_cols]\n\n\ndf_ice_cream_trans[\"ice_cream_purchases\"] = 1\n\n\ndf_daily_ice_cream_sales = df_ice_cream_trans.set_index(\"Date\").resample(\"D\").sum()\n\n\ndf_weekly_max_ice_cream_sales = df_daily_ice_cream_sales.assign(year=df_daily_ice_cream_sales.index.year,\\\n                                week = df_daily_ice_cream_sales.index.isocalendar().week).groupby(['year','week']).max()\n\n\ndf_weekly_mean_ice_cream_sales = df_daily_ice_cream_sales.assign(year=df_daily_ice_cream_sales.index.year,\\\n                                week = df_daily_ice_cream_sales.index.isocalendar().week).groupby(['year','week']).mean()\n\n\ndf_weekly_max_ice_cream_sales = pd.DataFrame(df_weekly_max_ice_cream_sales.to_records()) \n\n\ndf_weekly_max_ice_cream_sales\n\n\ndf_weekly_mean_ice_cream_sales"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#write-the-extracted-files-for-further-analysis",
    "href": "notebooks/retail_transaction_data.html#write-the-extracted-files-for-further-analysis",
    "title": "Data Preparation Notebook",
    "section": "Write the extracted files for further analysis",
    "text": "Write the extracted files for further analysis\n\nfpdaily = \"../data/daily_ice_cream_sales.csv\"\nfpmaxweekly = \"../data/max_weekly_ice_cream_sales.csv\"\nfpmeanweekly = \"../data/mean_weekly_ice_cream_sales.csv\"\ndf_daily_ice_cream_sales.to_csv(fpdaily, index=True)\ndf_weekly_max_ice_cream_sales.to_csv(fpmaxweekly, index=True)\ndf_weekly_mean_ice_cream_sales.to_csv(fpmeanweekly, index=True)"
  },
  {
    "objectID": "notebooks/coffee_prices_discretization.html",
    "href": "notebooks/coffee_prices_discretization.html",
    "title": "Rajiv's blog about data science",
    "section": "",
    "text": "import pandas as pd\nfp = \"../data/regimed_coffee_prices.csv\"\ndf = pd.read_csv(fp)\ndf\n\n\nregimes = df[\"regime\"].unique().tolist()\n\nfor r in regimes:\n    reg_select = (df[\"regime\"] == r) # select each regime\n    df_reg = df[reg_select]\n    # the index contains the indices of each regime, discretize the prices with qcut and set\n    df.loc[df_reg.index, \"price\"] = pd.qcut(df_reg[\"cents_per_lb\"], 3, labels=[\"L\", \"M\", \"H\"])\n\n    \n\n    \n\n\nfor r in regimes:\n    reg_select = (df[\"regime\"] == r) # select each regime\n    df_reg = df[reg_select]\n    # maintain regime point count to mark the previous price for the first entry in each regime as na\n    rpc = 0\n    for ri, row in df_reg.iterrows():\n        if rpc == 0 :\n            rpc += 1\n            continue\n        else:\n            df.loc[ri, \"previous_price\"] = df.loc[ (ri -1), \"price\"]\n            rpc += 1\n\n\nsum(df[\"previous_price\"].isna()) \n\n\nfp = \"../data/regimed_coffee_prices.csv\"\ndf.to_csv(fp, index=False)\n\n\nmatrix_dict = {}\nfor r in regimes:\n    reg_select = (df[\"regime\"] == r) # select each regime\n    df_reg = df[reg_select]\n    # the index contains the indices of each regime, discretize the prices with qcut and set\n    df_sm = pd.crosstab(df_reg.price, df_reg.previous_price)\n    # the next step normalizes the entry in each row by the row sum\n    df_sm = df_sm.div(df_sm.sum(axis=1), axis=0).round(3)\n    matrix_dict[r] = df_sm\n    fp = \"../data/stochastic_matrix_coffee_price-regime-\" + r + \".csv\"\n    df_sm.to_csv(fp, index=True)\n    \n\n\nmatrix_dict[\"R-5\"]\n\n\nimport plotly.express as px\n\nfig = px.imshow(matrix_dict[\"R-5\"], text_auto=True)\nfig.update_layout(\n    title={\n        'text': \"Stochastic Matrix for Region 5\",\n        'y':.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show()"
  }
]