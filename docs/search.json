[
  {
    "objectID": "notebooks/generated_code_retail.html",
    "href": "notebooks/generated_code_retail.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n# Step 1: Read a pandas DataFrame\n# For demonstration, we will create a sample dataframe. You can replace this with your own CSV or data source.\nfp = \"../data/Retail_Transactions_Dataset.csv\"\ndf = pd.read_csv(fp)\n\n# Step 2: Define lists for categorical columns\ncategorical_columns = ['Payment_Method', 'City', 'Store_Type',\\\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion']\n\n# Step 3: Define your timestamp column\ntimestamp_column = 'Date'\n\n# Step 4: Set the type of the categorical columns to 'category'\nfor col in categorical_columns:\n    df[col] = df[col].astype('category')\n\n# Step 5: Set the type of the timestamp column to datetime\ndf[timestamp_column] = pd.to_datetime(df[timestamp_column])\n\n# Step 6: Function to check for a sentinel value (for this example, let's assume the sentinel value is 0)\ndef contains_sentinel_value(string_list):\n    # Convert the string representation of a list back to an actual list\n    try:\n        actual_list = eval(string_list)\n        return 0 in actual_list  # Check for sentinel value\n    except:\n        return False  # In case of any errors, return False\n\n# Step 7: Filter the DataFrame to rows that only contain the sentinel value\ndf['is_ice_cream'] = df[\"Product\"].apply(contains_sentinel_value)\nfiltered_df = df[df['ice_cream']]\n\n# Step 8: Drop all columns except the timestamp column in the filtered DataFrame\nfiltered_df = filtered_df[[timestamp_column]]\n\n# Step 9: Define a new column in the filtered DataFrame that is set to the value 1\nfiltered_df['is_ice_cream'] = 1\n\n# Step 10: Set the index of the filtered DataFrame to the timestamp column\nfiltered_df.set_index(timestamp_column, inplace=True)\n\n# Step 11: Resample the DataFrame on the timestamp column and sum the new column\n# Assuming we want to sum by minute, you can change the frequency as needed\nresampled_df = filtered_df.resample('T').sum()\n\n# Display the final resampled DataFrame\nprint(resampled_df)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'ice_cream'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[1], line 34\n     32 # Step 7: Filter the DataFrame to rows that only contain the sentinel value\n     33 df['is_ice_cream'] = df[\"Product\"].apply(contains_sentinel_value)\n---&gt; 34 filtered_df = df[df['ice_cream']]\n     36 # Step 8: Drop all columns except the timestamp column in the filtered DataFrame\n     37 filtered_df = filtered_df[[timestamp_column]]\n\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'ice_cream'"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html",
    "href": "notebooks/daily_ice_cream_sales_exploration.html",
    "title": "Analysis Notebook",
    "section": "",
    "text": "The purpose of this notebook is to analyze the daily sales of ice creams at metro areas in the USA. Subsets corresponding to yearly sales are also profiled. Plots of auto-correlation and maximum daily sales per week are also provided.\nimport pandas as pd\nimport numpy as np\nfp = \"../data/daily_ice_cream_sales.csv\"\ndf = pd.read_csv(fp)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"log_daily_sales\"] = np.log(df[\"ice_cream_purchases\"])\ndf[\"Date\"].max()\n\nTimestamp('2024-05-18 00:00:00')\ndf[\"Date\"].min()\n\nTimestamp('2020-01-01 00:00:00')\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#extract-yearly-data-subsets",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#extract-yearly-data-subsets",
    "title": "Analysis Notebook",
    "section": "Extract yearly data subsets",
    "text": "Extract yearly data subsets\n\ndf_2020 = df[df.Date.dt.year == 2020]\ndf_2021 = df[df.Date.dt.year == 2021]\ndf_2022 = df[df.Date.dt.year == 2022]\ndf_2023 = df[df.Date.dt.year == 2023]\n\n\n%matplotlib inline\nimport statsmodels.api as sm\n\n\ndf_2020[\"log_daily_sales\"].plot()"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#plot-autocorrelation",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#plot-autocorrelation",
    "title": "Analysis Notebook",
    "section": "Plot Autocorrelation",
    "text": "Plot Autocorrelation\nThe auto correlation plots are shown below.\n\nsm.graphics.tsa.plot_acf(df_2020[\"log_daily_sales\"], lags=40)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsm.graphics.tsa.plot_pacf(df_2020[\"log_daily_sales\"], lags=40, method=\"ywm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsm.graphics.tsa.plot_pacf(df[\"log_daily_sales\"], lags=40, method=\"ywm\")"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#observation",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#observation",
    "title": "Analysis Notebook",
    "section": "Observation",
    "text": "Observation\nIt appears that daily sales are not correlated, but independent draws from a distribution. This looks like a white noise process. This is consistent with synthetically generated data. A dickey fuller test with no regression (constant, this is the default) also suggests the same.\n\nfrom statsmodels.tsa.stattools import adfuller\nresult = adfuller(df[\"log_daily_sales\"])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\n\nADF Statistic: -40.221392\np-value: 0.000000\n\n\n\ndf_2020\n\n\n\n\n\n\n\n\nDate\nice_cream_purchases\nlog_daily_sales\n\n\n\n\n0\n2020-01-01\n20\n2.995732\n\n\n1\n2020-01-02\n22\n3.091042\n\n\n2\n2020-01-03\n23\n3.135494\n\n\n3\n2020-01-04\n23\n3.135494\n\n\n4\n2020-01-05\n21\n3.044522\n\n\n...\n...\n...\n...\n\n\n361\n2020-12-27\n31\n3.433987\n\n\n362\n2020-12-28\n30\n3.401197\n\n\n363\n2020-12-29\n25\n3.218876\n\n\n364\n2020-12-30\n22\n3.091042\n\n\n365\n2020-12-31\n29\n3.367296\n\n\n\n\n366 rows × 3 columns\n\n\n\n\n\ndf[\"log_daily_sales\"] = np.log(df[\"ice_cream_purchases\"])\n\n\ndf_yearly_ice_cream_sales = df.set_index(\"Date\").resample(\"Y\").sum()\n\n\ndf_yearly_ice_cream_sales\n\n\n\n\n\n\n\n\nice_cream_purchases\nlog_daily_sales\n\n\nDate\n\n\n\n\n\n\n2020-12-31\n8488\n1142.662667\n\n\n2021-12-31\n8224\n1128.462655\n\n\n2022-12-31\n8206\n1127.813921\n\n\n2023-12-31\n8379\n1135.628790\n\n\n2024-12-31\n3188\n432.632764\n\n\n\n\n\n\n\n\ndf[\"ice_cream_purchases\"].plot()\n\n\n\n\n\n\n\n\n\nfpmeanweekly = \"../data/mean_weekly_ice_cream_sales.csv\"\ndfwmics = pd.read_csv(fpmeanweekly)\n\n\ndfwmics[\"ice_cream_purchases\"].plot()\n\n\n\n\n\n\n\n\n\ndfwmics[\"weekno\"] = range(1,dfwmics.shape[0] + 1)\n\n\ndfwmics\n\n\n\n\n\n\n\n\nyear\nweek\nice_cream_purchases\nweekno\n\n\n\n\n0\n2020\n1\n21.800000\n1\n\n\n1\n2020\n2\n21.857143\n2\n\n\n2\n2020\n3\n26.000000\n3\n\n\n3\n2020\n4\n26.142857\n4\n\n\n4\n2020\n5\n19.571429\n5\n\n\n...\n...\n...\n...\n...\n\n\n225\n2024\n16\n21.571429\n226\n\n\n226\n2024\n17\n23.000000\n227\n\n\n227\n2024\n18\n20.857143\n228\n\n\n228\n2024\n19\n23.285714\n229\n\n\n229\n2024\n20\n23.000000\n230\n\n\n\n\n230 rows × 4 columns"
  },
  {
    "objectID": "data/sba_heterogeneity_analysis.html",
    "href": "data/sba_heterogeneity_analysis.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import pandas as pd\nfp = \"../data/sba_7a_loans.csv\"\ndf = pd.read_csv(fp)\n\n/var/folders/fb/t_m5qpcj6qq85rvkh73vbxh40000gn/T/ipykernel_61763/3705498770.py:3: DtypeWarning: Columns (34,35,39) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(fp)\n\n\n\nsubset_cols = [\"BorrName\", \"BankFDICNumber\", \"BankZip\", \"BorrZip\", \"NaicsCode\", \"FranchiseCode\",\\\n               \"BusinessAge\", \"LoanStatus\", \"SBAGuaranteedApproval\"]\ndf = df[subset_cols]\n\n\nfilter_PIF = (df.LoanStatus == \"PIF\")\nfilter_CHGOFF = (df.LoanStatus == \"CHGOFF\")\nfilter_criteria = filter_PIF | filter_CHGOFF\ndf = df[filter_criteria].reset_index(drop=True)\n\n\ndf_missing_vals = pd.DataFrame.from_dict({c:df[c].isnull().sum() for c in subset_cols if df[c].isnull().sum() &gt; 0},\\\n                                         orient=\"index\").reset_index()\ndf_missing_vals.columns = [\"Attribute\", \"Missing Value Count\"]\ndf_missing_vals\n\n\n\n\n\n\n\n\nAttribute\nMissing Value Count\n\n\n\n\n0\nBankFDICNumber\n2122\n\n\n1\nFranchiseCode\n19923\n\n\n2\nBusinessAge\n53\n\n\n\n\n\n\n\n\nfor a in df_missing_vals[\"Attribute\"]:\n    df[a] = df[a].fillna(\"Not Applicable\")\n\n\n{c:df[c].isnull().sum() for c in subset_cols if df[c].isnull().sum() &gt; 0}\n\n{}\n\n\n\ndf[\"BankFDICNumber\"] = df[\"BankFDICNumber\"].apply(lambda x: x if x == \"Not Applicable\" else int(x))\ndf[\"NaicsCode\"] = df[\"NaicsCode\"].apply(lambda x: x if x == \"Not Applicable\" else int(x))\ndtypes_toset = {\"BorrZip\": 'str', \"BankZip\": \"str\", \"BankFDICNumber\": 'str',\\\n                \"NaicsCode\": 'str', \"FranchiseCode\": 'str', \\\n                \"BusinessAge\" : 'str', \"LoanStatus\": 'str', \"SBAGuaranteedApproval\" : float}\ndf = df.astype(dtypes_toset)\n\n\ndf\n\n\n\n\n\n\n\n\nBorrName\nBankFDICNumber\nBankZip\nBorrZip\nNaicsCode\nFranchiseCode\nBusinessAge\nLoanStatus\nSBAGuaranteedApproval\n\n\n\n\n0\nAllen Foot and Ankle Medicine\n57512\n85004\n85212\n621391\nNot Applicable\nExisting or more than 2 years old\nPIF\n175000.0\n\n\n1\nTown Cleaners\n24170\n90010\n22201\n812320\nNot Applicable\nExisting or more than 2 years old\nCHGOFF\n11000.0\n\n\n2\nMoor Inc.\n628\n43240\n94061\n445110\nNot Applicable\nExisting or more than 2 years old\nPIF\n24500.0\n\n\n3\nMimo Lash And Skin LLC\n26610\n90010\n7030\n812199\nNot Applicable\nExisting or more than 2 years old\nPIF\n28500.0\n\n\n4\nMoor Inc.\n628\n43240\n94061\n445110\nNot Applicable\nExisting or more than 2 years old\nPIF\n50500.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22956\nJK Freight LLC\n6560\n43215\n46239\n484121\nNot Applicable\nNew Business or 2 years or less\nPIF\n12500.0\n\n\n22957\nMuirhead Group LLC\n12633\n65101\n65255\n722513\nNot Applicable\nExisting or more than 2 years old\nPIF\n38900.0\n\n\n22958\nJP ELECTRICAL CONTRACTOR\n34968\n918\n674\n238390\nNot Applicable\nExisting or more than 2 years old\nPIF\n25000.0\n\n\n22959\nANOUSHEH ASHOURI INC.\nNot Applicable\n7922\n92618\n621111\nNot Applicable\nExisting or more than 2 years old\nPIF\n22500.0\n\n\n22960\nFur Fluffs Sake LLC\nNot Applicable\n1752\n1752\n812910\nNot Applicable\nNew Business or 2 years or less\nPIF\n31450.0\n\n\n\n\n22961 rows × 9 columns\n\n\n\n\ngb = df.groupby([\"NaicsCode\", \"LoanStatus\"], as_index=False)\n\n\ndfnaics_summ = gb.size()\n\n\ndfnaics_summ = dfnaics_summ.sort_values(by=[\"NaicsCode\"])\n\n\n#dfnaics_summ[(dfnaics_summ[\"NaicsCode\"].value_counts() == 1)]\n\n\nnaics_filter = (dfnaics_summ[\"NaicsCode\"].value_counts() == 1)\n\n\ndfnaics_vc = dfnaics_summ[\"NaicsCode\"].value_counts().reset_index()\n\n\ndfnaics_vc.columns = [\"NaicsCode\", \"LoanStatusCard\"]\n\n\nfilter_ss_naics_codes = (dfnaics_vc.LoanStatusCard == 1)\nss_naics_codes = dfnaics_vc[filter_ss_naics_codes][\"NaicsCode\"]\n\n\ndfnaics_vc[~filter_ss_naics_codes]\n\n\n\n\n\n\n\n\nNaicsCode\nLoanStatusCard\n\n\n\n\n0\n442291\n2\n\n\n1\n541370\n2\n\n\n2\n532310\n2\n\n\n3\n532490\n2\n\n\n4\n541110\n2\n\n\n...\n...\n...\n\n\n272\n611699\n2\n\n\n273\n621310\n2\n\n\n274\n611691\n2\n\n\n275\n321999\n2\n\n\n276\n621999\n2\n\n\n\n\n277 rows × 2 columns\n\n\n\n\ndf[df[\"NaicsCode\"].isin(ss_naics_codes)][\"LoanStatus\"].value_counts()\n\nPIF       4757\nCHGOFF       8\nName: LoanStatus, dtype: int64\n\n\n\ndf[df[\"NaicsCode\"].isin(ss_naics_codes)][\"NaicsCode\"].value_counts()\n\n447110    250\n531130    144\n722410    124\n623312     92\n541512     90\n         ... \n111419      1\n336411      1\n561591      1\n111140      1\n459120      1\nName: NaicsCode, Length: 583, dtype: int64\n\n\n\ngb = df.groupby([\"BankZip\", \"LoanStatus\"], as_index=False)\n\n\ndfbank_zip_summ = gb.size()\ndfbank_zip_summ = dfbank_zip_summ.sort_values(by=[\"BankZip\"])\n\n\nbankzip_filter = (dfbank_zip_summ[\"BankZip\"].value_counts() == 1)\n\n\ndfbank_zip_vc = dfbank_zip_summ[\"BankZip\"].value_counts().reset_index()\ndfbank_zip_vc.columns = [\"BankZip\", \"LoanStatusCard\"]\nfilter_ss_bankzip = (dfbank_zip_vc.LoanStatusCard == 1)\nss_bankzip_codes = dfbank_zip_vc[filter_ss_bankzip][\"BankZip\"]\n\n\ndf[df[\"BankZip\"].isin(ss_bankzip_codes)][\"LoanStatus\"].value_counts()\n\nPIF       8300\nCHGOFF       9\nName: LoanStatus, dtype: int64\n\n\n\ndf_trim = df[df[\"BankZip\"].isin(ss_bankzip_codes)].index.union(df[df[\"NaicsCode\"].isin(ss_naics_codes)].index)\n\n\ndf_trim.shape\n\n(10909,)\n\n\n\ngb = df.groupby([\"BorrZip\", \"LoanStatus\"], as_index=False)\ndfborr_zip_summ = gb.size()\ndfborr_zip_summ = dfborr_zip_summ.sort_values(by=[\"BorrZip\"])\n\n\nborrzip_filter = (dfborr_zip_summ[\"BorrZip\"].value_counts() == 1)\n\n\ndfborr_zip_vc = dfborr_zip_summ[\"BorrZip\"].value_counts().reset_index()\ndfborr_zip_vc.columns = [\"BorrZip\", \"LoanStatusCard\"]\nfilter_ss_borrzip = (dfborr_zip_vc.LoanStatusCard == 1)\nss_borrzip_codes = dfborr_zip_vc[filter_ss_borrzip][\"BorrZip\"]\n\n\ndf[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"LoanStatus\"].value_counts()\n\nPIF       19549\nCHGOFF      250\nName: LoanStatus, dtype: int64\n\n\n\ndf[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"BorrZip\"].value_counts()\n\n39428    35\n56601    27\n26508    22\n67601    22\n58102    21\n         ..\n48159     1\n65483     1\n12168     1\n29652     1\n65255     1\nName: BorrZip, Length: 8361, dtype: int64\n\n\n\ndf_borr_zip_counts_summ = df[df[\"BorrZip\"].isin(ss_borrzip_codes)][\"BorrZip\"].value_counts().reset_index()\n\n\ndf_borr_zip_counts_summ.columns = [\"BorrZip\", \"num_recs\"]\n\n\ndf_borr_zip_counts_summ[df_borr_zip_counts_summ.num_recs == 1]\n\n\n\n\n\n\n\n\nBorrZip\nnum_recs\n\n\n\n\n4340\n14011\n1\n\n\n4341\n47441\n1\n\n\n4342\n66083\n1\n\n\n4343\n53522\n1\n\n\n4344\n64740\n1\n\n\n...\n...\n...\n\n\n8356\n48159\n1\n\n\n8357\n65483\n1\n\n\n8358\n12168\n1\n\n\n8359\n29652\n1\n\n\n8360\n65255\n1\n\n\n\n\n4021 rows × 2 columns\n\n\n\n\nlen(df[\"BorrZip\"].unique())\n\n9057"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "Predictive Modeling without Supporting Data Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 13, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT and Data Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/coffee_prices/index.html",
    "href": "posts/coffee_prices/index.html",
    "title": "Predictive Modeling without Supporting Data Analysis",
    "section": "",
    "text": "In my opinion, relying on models without prior data analysis is problematic. This is particularly true for business applications that work with regular tabular data. While it’s common practice to develop separate models for prediction and explanation, preliminary data analysis is crucial for justifying: 1. Specific modeling approaches 2. The necessity of certain features\nThis isn’t just about generating summary statistics or assessing data quality; it’s about understanding the sources of variation relevant to our predictions or estimates.\nAs a coffee connoisseur, you may have noticed the rising costs of your favorite brew. Imagine you own a small restaurant chain and need to plan your coffee purchases to manage production costs effectively. To forecast coffee prices for the next six months, you hire a data science team to build a predictive model. However, it’s essential to first understand the primary sources of price variation and any recent changes in price patterns before developing an accurate model.\nOver the past decade, the quality of libraries for statistical modeling and optimization has significantly improved. Coupled with code generation features like those provided by GitHub Copilot, the effort and cost of conducting data analysis have decreased substantially—provided you have the data analysis skills. Unfortunately, it is quite common to run into conversations today where people believe that throwing the data at an “automatic modeling” tool will give you the answer and there is really no need to understand how the result was obtained.\nFor example, here is how easy tools make this for the coffee example. Here is the raw series  Here is the decomposition of this series with LOWESS. The details of the notebook with the observations are here."
  },
  {
    "objectID": "posts/coffee_prices/index.html#the-black-box-model",
    "href": "posts/coffee_prices/index.html#the-black-box-model",
    "title": "Predictive Modeling without Supporting Data Analysis",
    "section": "",
    "text": "In my opinion, relying on models without prior data analysis is problematic. This is particularly true for business applications that work with regular tabular data. While it’s common practice to develop separate models for prediction and explanation, preliminary data analysis is crucial for justifying: 1. Specific modeling approaches 2. The necessity of certain features\nThis isn’t just about generating summary statistics or assessing data quality; it’s about understanding the sources of variation relevant to our predictions or estimates.\nAs a coffee connoisseur, you may have noticed the rising costs of your favorite brew. Imagine you own a small restaurant chain and need to plan your coffee purchases to manage production costs effectively. To forecast coffee prices for the next six months, you hire a data science team to build a predictive model. However, it’s essential to first understand the primary sources of price variation and any recent changes in price patterns before developing an accurate model.\nOver the past decade, the quality of libraries for statistical modeling and optimization has significantly improved. Coupled with code generation features like those provided by GitHub Copilot, the effort and cost of conducting data analysis have decreased substantially—provided you have the data analysis skills. Unfortunately, it is quite common to run into conversations today where people believe that throwing the data at an “automatic modeling” tool will give you the answer and there is really no need to understand how the result was obtained.\nFor example, here is how easy tools make this for the coffee example. Here is the raw series  Here is the decomposition of this series with LOWESS. The details of the notebook with the observations are here."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html",
    "href": "posts/ice_cream_sales/index.html",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "For a while now I have been meaning to explore how Copilot and Chat GPT are useful for data analysis. I have also been meaning to transition to Quarto for a while, so I grabbed a dataset from Kaggle, picked some interesting questions to analyze, and threw it at Copilot and Chat GPT. Here’s my experience. I used Chat GPT to generate the content initially, so what you see here is my own writing interlaced with generated content. I used VSCode with extensions for quarto and copilot for this article.\nBy leveraging Chat GPT, we can automate and enhance various aspects of time series analysis, including data preprocessing, model selection, and interpretation of results.\n\n\n\n\n\n\nNote\n\n\n\nWhile the above statement is true, it can be somewhat misleading. The assistance we get for the tasks like data preprocessing in comparison to tasks like result interpretation and model selection is very different. Code generation for manipulating and transforming pandas dataframes was useful, I did’t really find anything useful for interpretation or model selection.\n\n\n\n\nThe data analyzed in this post is a collection of transactions from a national retailer. The transactions span major metropolitan cities in the US and cover a four and half year period. The dataset is available on Kaggle. The objective was to analyze this data to get answers to a representative set of questions. The questions addressed in this post represent a subset of a comprehensive analysis exercise, and it is not intended to be exhaustive or rigorous. Some example questions include:\n\nWhat does the plot of daily ice cream sales nationwide over the analysis period look like?\nHow do ice cream sales vary across the four years within the analysis period? Are there significant differences in the summary statistics year over year?\nIs there variability in ice cream sales based on location? In other words, is there heterogeneity in consumption patterns based on location?\nWhat does the autocorrelation function of the ice cream sales indicate? Is the time series stationary?\nWhat are the key components of the time series (trend, seasonality, etc.)?\nHow does the plot of the maximum daily ice cream sales for each week of the year look? Can we model these maximum values using a Gumbel distribution to understand the distribution of peak daily demand nationally?\n\n\n\n\n\n\n\nNote\n\n\n\nThe relevant analysis tasks (the list above) is something the analyst has to generate. In my view, this comes with experience with similar data and use cases. Asking Chat GPT on how to do time series analysis got me very general guidelines. I did not find the response very useful (see below). The ACF suggests a white noise process (which is the ground truth), Chat GPT was trying find patterns in it. A notebook implementation of most of these questions is available\n\n\n\n\n\nChat GPT Analysis\n\n\n\n\n\nTransforming the raw dataset to a format that was suitable to answer the analysis questions required an understanding of the various fields in the data. To get the daily ice cream sales from the raw transaction data, I had the identify the transactions that contained ice creams and then aggregate the transactions on a daily basis. Doing this required an analysis of how the line items for each transaction is encoded. Once the data description was analyzed, providing the prompts to Chat GPT generated code that was pretty close to what I developed manually.\n\n\n\n\n\n\nNote\n\n\n\nThe real effort in the data preparation task went into developing an understanding of the various elements of the raw dataset and then coming up with the process of identifying the transactions containing ice cream. Aggregating these transactions to a daily or weekly cadence was fairly straight forward. Chat GPT was helpful in generating the specific pandas code for each step. The manual and the generated were pretty close.\n\n\n\n\n\n\nChat GPT was definitely very helpful in developing and organizing the content.\nIdentifying the key questions that are relevant to the analysis requires formal training and experience - I don’t really see this replacing a data scientist at this point.\nChat GPT was definitely useful for various code generation tasks.\nIn summary, it was impressive as an assistant, but at this point, it is an assistant not a data scientist replacement."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#the-time-series-analysis-task",
    "href": "posts/ice_cream_sales/index.html#the-time-series-analysis-task",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "The data analyzed in this post is a collection of transactions from a national retailer. The transactions span major metropolitan cities in the US and cover a four and half year period. The dataset is available on Kaggle. The objective was to analyze this data to get answers to a representative set of questions. The questions addressed in this post represent a subset of a comprehensive analysis exercise, and it is not intended to be exhaustive or rigorous. Some example questions include:\n\nWhat does the plot of daily ice cream sales nationwide over the analysis period look like?\nHow do ice cream sales vary across the four years within the analysis period? Are there significant differences in the summary statistics year over year?\nIs there variability in ice cream sales based on location? In other words, is there heterogeneity in consumption patterns based on location?\nWhat does the autocorrelation function of the ice cream sales indicate? Is the time series stationary?\nWhat are the key components of the time series (trend, seasonality, etc.)?\nHow does the plot of the maximum daily ice cream sales for each week of the year look? Can we model these maximum values using a Gumbel distribution to understand the distribution of peak daily demand nationally?\n\n\n\n\n\n\n\nNote\n\n\n\nThe relevant analysis tasks (the list above) is something the analyst has to generate. In my view, this comes with experience with similar data and use cases. Asking Chat GPT on how to do time series analysis got me very general guidelines. I did not find the response very useful (see below). The ACF suggests a white noise process (which is the ground truth), Chat GPT was trying find patterns in it. A notebook implementation of most of these questions is available\n\n\n\n\n\nChat GPT Analysis"
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#data-preparation",
    "href": "posts/ice_cream_sales/index.html#data-preparation",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "Transforming the raw dataset to a format that was suitable to answer the analysis questions required an understanding of the various fields in the data. To get the daily ice cream sales from the raw transaction data, I had the identify the transactions that contained ice creams and then aggregate the transactions on a daily basis. Doing this required an analysis of how the line items for each transaction is encoded. Once the data description was analyzed, providing the prompts to Chat GPT generated code that was pretty close to what I developed manually.\n\n\n\n\n\n\nNote\n\n\n\nThe real effort in the data preparation task went into developing an understanding of the various elements of the raw dataset and then coming up with the process of identifying the transactions containing ice cream. Aggregating these transactions to a daily or weekly cadence was fairly straight forward. Chat GPT was helpful in generating the specific pandas code for each step. The manual and the generated were pretty close."
  },
  {
    "objectID": "posts/ice_cream_sales/index.html#summary-of-chat-gpts-utility-for-this-particular-exercise",
    "href": "posts/ice_cream_sales/index.html#summary-of-chat-gpts-utility-for-this-particular-exercise",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "Chat GPT was definitely very helpful in developing and organizing the content.\nIdentifying the key questions that are relevant to the analysis requires formal training and experience - I don’t really see this replacing a data scientist at this point.\nChat GPT was definitely useful for various code generation tasks.\nIn summary, it was impressive as an assistant, but at this point, it is an assistant not a data scientist replacement."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog, where I explore a variety of topics that pique my interest, including:\n\nAnalyzing, summarizing, and optimizing large datasets for business machine learning tasks.\nDeveloping discrete and continuous optimization models for business applications.\nLeveraging graph data models and algorithms to address business challenges.\nApplying statistical modeling techniques to solve business problems."
  },
  {
    "objectID": "notebooks/coffee_prices.html",
    "href": "notebooks/coffee_prices.html",
    "title": "Observations",
    "section": "",
    "text": "import pandas as pd\nfp = \"../data/coffee_prices_index.csv\"\ndf = pd.read_csv(fp)\ndf.columns = [\"date\", \"cindex\"]\ndf[\"date\"] = pd.to_datetime(df.date)\ndf[\"cindex\"] = df[\"cindex\"].astype(float).round(3)\n%matplotlib inline\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nplt.plot(df.date,df.cindex)\nplt.grid(True)\nplt.xlabel(\"Year\")\nplt.ylabel(\"cents per pound\")\nplt.title (\"Price of mild arabica coffee\") # from https://fred.stlouisfed.org/series/PCOFFOTMUSDM\n#plt.xlabel(df[\"date\"])\n\nText(0.5, 1.0, 'Price of mild arabica coffee')\nsm.graphics.tsa.plot_acf(df.cindex, lags=24);\ncoffee_prices = pd.Series(df.cindex.values, index=df.date)\ncoffee_prices\n\ndate\n2014-12-01    193.386\n2015-01-01    189.626\n2015-02-01    178.888\n2015-03-01    160.736\n2015-04-01    163.998\n               ...   \n2024-08-01    261.438\n2024-09-01    278.760\n2024-10-01    276.777\n2024-11-01    304.953\n2024-12-01    344.119\nLength: 121, dtype: float64"
  },
  {
    "objectID": "notebooks/coffee_prices.html#observations",
    "href": "notebooks/coffee_prices.html#observations",
    "title": "Observations",
    "section": "Observations",
    "text": "Observations\nPost-covid, there has been a stronger trend and larger seasonal variations. We have been experiencing this at the grocery stores and every where else. It looks like pre-covid, though prices did have trend-cycles and seasonality, they were gradual and similar. I am not economist, so I don’t know the answers. The point here is that the right data tools can surface the problems that need analysis. It also provides a basis for determing the right characteristics we need to account for in downstream analysis like forecasting. Building predictive models without rigorous data analysis to document evidence for sources of variation we need to account for is like carpet bombing or driving blind. You are either using too much of computational sophistication, or, if you get a reasonable answer, you are just lucky that you picked a model that had the right features.\n\nfrom statsmodels.tsa.seasonal import STL\n\nstl = STL(coffee_prices, period=12)\nres = stl.fit()\nfig = res.plot()"
  },
  {
    "objectID": "notebooks/coffee_prices.html#errors-post-decomposition",
    "href": "notebooks/coffee_prices.html#errors-post-decomposition",
    "title": "Observations",
    "section": "Errors Post Decomposition",
    "text": "Errors Post Decomposition\n\nres.resid.hist() # looks reasonable"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html",
    "href": "notebooks/retail_transaction_data.html",
    "title": "Data Preparation Notebook",
    "section": "",
    "text": "The data for this notebook is from Kaggle. The data looks like it is synthetic, the characteristics observed in the dataset also seem to suggest that. The objectives for this notebook are to:\nimport pandas as pd\nfp = \"../data/Retail_Transactions_Dataset.csv\"\ndf = pd.read_csv(fp)\ndf.head()\n\n\n\n\n\n\n\n\nTransaction_ID\nDate\nCustomer_Name\nProduct\nTotal_Items\nTotal_Cost\nPayment_Method\nCity\nStore_Type\nDiscount_Applied\nCustomer_Category\nSeason\nPromotion\n\n\n\n\n0\n1000000000\n2022-01-21 06:27:29\nStacey Price\n['Ketchup', 'Shaving Cream', 'Light Bulbs']\n3\n71.65\nMobile Payment\nLos Angeles\nWarehouse Club\nTrue\nHomemaker\nWinter\nNone\n\n\n1\n1000000001\n2023-03-01 13:01:21\nMichelle Carlson\n['Ice Cream', 'Milk', 'Olive Oil', 'Bread', 'P...\n2\n25.93\nCash\nSan Francisco\nSpecialty Store\nTrue\nProfessional\nFall\nBOGO (Buy One Get One)\n\n\n2\n1000000002\n2024-03-21 15:37:04\nLisa Graves\n['Spinach']\n6\n41.49\nCredit Card\nHouston\nDepartment Store\nTrue\nProfessional\nWinter\nNone\n\n\n3\n1000000003\n2020-10-31 09:59:47\nMrs. Patricia May\n['Tissues', 'Mustard']\n1\n39.34\nMobile Payment\nChicago\nPharmacy\nTrue\nHomemaker\nSpring\nNone\n\n\n4\n1000000004\n2020-12-10 00:59:59\nSusan Mitchell\n['Dish Soap']\n10\n16.42\nDebit Card\nHouston\nSpecialty Store\nFalse\nYoung Adult\nWinter\nDiscount on Selected Items"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#profile-the-categorical-columns",
    "href": "notebooks/retail_transaction_data.html#profile-the-categorical-columns",
    "title": "Data Preparation Notebook",
    "section": "Profile the categorical columns",
    "text": "Profile the categorical columns\n\ndf.columns\n\nIndex(['Transaction_ID', 'Date', 'Customer_Name', 'Product', 'Total_Items',\n       'Total_Cost', 'Payment_Method', 'City', 'Store_Type',\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion'],\n      dtype='object')\n\n\n\ncategory_cols = ['Payment_Method', 'City', 'Store_Type',\\\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion']\n\n\ndf[category_cols] = df[category_cols].astype('category')\n\n\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n\ndf.dtypes\n\nTransaction_ID                int64\nDate                 datetime64[ns]\nCustomer_Name                object\nProduct                      object\nTotal_Items                   int64\nTotal_Cost                  float64\nPayment_Method             category\nCity                       category\nStore_Type                 category\nDiscount_Applied           category\nCustomer_Category          category\nSeason                     category\nPromotion                  category\ndtype: object\n\n\n\ndf[\"Customer_Category\"].value_counts()\n\nSenior Citizen    125485\nHomemaker         125418\nTeenager          125319\nRetiree           125072\nStudent           124842\nProfessional      124651\nMiddle-Aged       124636\nYoung Adult       124577\nName: Customer_Category, dtype: int64\n\n\n\ndf[\"City\"].value_counts()\n\nBoston           100566\nDallas           100559\nSeattle          100167\nChicago          100059\nHouston          100050\nNew York         100007\nLos Angeles       99879\nMiami             99839\nSan Francisco     99808\nAtlanta           99066\nName: City, dtype: int64"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#observation",
    "href": "notebooks/retail_transaction_data.html#observation",
    "title": "Data Preparation Notebook",
    "section": "Observation",
    "text": "Observation\nThe counts for each of the metro areas are very similar, the counts for each of the customer categories are very similar, so this dataset was probably synthetically generated.\n\ndf[\"Product\"] = df[\"Product\"].apply(eval)\n\n\npurchase_summ = {}\nfor index, row in df[\"Product\"].items():\n    for p in row:\n        if p in purchase_summ:\n            purchase_summ[p] += 1\n        else:\n            purchase_summ[p] = 1"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#extract-the-ice-cream-transactions",
    "href": "notebooks/retail_transaction_data.html#extract-the-ice-cream-transactions",
    "title": "Data Preparation Notebook",
    "section": "Extract the ice cream transactions",
    "text": "Extract the ice cream transactions\n\ndef is_ice_cream(row):\n    for p in row:\n        if p == \"Ice Cream\":\n            return True\n    return False\ndf[\"is_ice_cream\"] = df[\"Product\"].apply(is_ice_cream)\n\n\ndf_ice_cream_trans = df[df[\"is_ice_cream\"]].reset_index()\nreq_cols = [\"Date\"]\ndf_ice_cream_trans = df_ice_cream_trans[req_cols]\n\n\ndf_ice_cream_trans[\"ice_cream_purchases\"] = 1\n\n\ndf_daily_ice_cream_sales = df_ice_cream_trans.set_index(\"Date\").resample(\"D\").sum()\n\n\ndf_weekly_max_ice_cream_sales = df_daily_ice_cream_sales.assign(year=df_daily_ice_cream_sales.index.year,\\\n                                week = df_daily_ice_cream_sales.index.isocalendar().week).groupby(['year','week']).max()\n\n\ndf_weekly_mean_ice_cream_sales = df_daily_ice_cream_sales.assign(year=df_daily_ice_cream_sales.index.year,\\\n                                week = df_daily_ice_cream_sales.index.isocalendar().week).groupby(['year','week']).mean()\n\n\ndf_weekly_max_ice_cream_sales = pd.DataFrame(df_weekly_max_ice_cream_sales.to_records()) \n\n\ndf_weekly_max_ice_cream_sales\n\n\n\n\n\n\n\n\nyear\nweek\nice_cream_purchases\n\n\n\n\n0\n2020\n1\n23\n\n\n1\n2020\n2\n26\n\n\n2\n2020\n3\n36\n\n\n3\n2020\n4\n31\n\n\n4\n2020\n5\n23\n\n\n...\n...\n...\n...\n\n\n225\n2024\n16\n33\n\n\n226\n2024\n17\n32\n\n\n227\n2024\n18\n30\n\n\n228\n2024\n19\n30\n\n\n229\n2024\n20\n30\n\n\n\n\n230 rows × 3 columns\n\n\n\n\ndf_weekly_mean_ice_cream_sales\n\n\n\n\n\n\n\n\n\nice_cream_purchases\n\n\nyear\nweek\n\n\n\n\n\n2020\n1\n21.800000\n\n\n2\n21.857143\n\n\n3\n26.000000\n\n\n4\n26.142857\n\n\n5\n19.571429\n\n\n...\n...\n...\n\n\n2024\n16\n21.571429\n\n\n17\n23.000000\n\n\n18\n20.857143\n\n\n19\n23.285714\n\n\n20\n23.000000\n\n\n\n\n230 rows × 1 columns"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#write-the-extracted-files-for-further-analysis",
    "href": "notebooks/retail_transaction_data.html#write-the-extracted-files-for-further-analysis",
    "title": "Data Preparation Notebook",
    "section": "Write the extracted files for further analysis",
    "text": "Write the extracted files for further analysis\n\nfpdaily = \"../data/daily_ice_cream_sales.csv\"\nfpmaxweekly = \"../data/max_weekly_ice_cream_sales.csv\"\nfpmeanweekly = \"../data/mean_weekly_ice_cream_sales.csv\"\ndf_daily_ice_cream_sales.to_csv(fpdaily, index=True)\ndf_weekly_max_ice_cream_sales.to_csv(fpmaxweekly, index=True)\ndf_weekly_mean_ice_cream_sales.to_csv(fpmeanweekly, index=True)"
  }
]