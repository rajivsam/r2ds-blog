[
  {
    "objectID": "notebooks/generated_code_retail.html",
    "href": "notebooks/generated_code_retail.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n# Step 1: Read a pandas DataFrame\n# For demonstration, we will create a sample dataframe. You can replace this with your own CSV or data source.\nfp = \"../data/Retail_Transactions_Dataset.csv\"\ndf = pd.read_csv(fp)\n\n# Step 2: Define lists for categorical columns\ncategorical_columns = ['Payment_Method', 'City', 'Store_Type',\\\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion']\n\n# Step 3: Define your timestamp column\ntimestamp_column = 'Date'\n\n# Step 4: Set the type of the categorical columns to 'category'\nfor col in categorical_columns:\n    df[col] = df[col].astype('category')\n\n# Step 5: Set the type of the timestamp column to datetime\ndf[timestamp_column] = pd.to_datetime(df[timestamp_column])\n\n# Step 6: Function to check for a sentinel value (for this example, let's assume the sentinel value is 0)\ndef contains_sentinel_value(string_list):\n    # Convert the string representation of a list back to an actual list\n    try:\n        actual_list = eval(string_list)\n        return 0 in actual_list  # Check for sentinel value\n    except:\n        return False  # In case of any errors, return False\n\n# Step 7: Filter the DataFrame to rows that only contain the sentinel value\ndf['is_ice_cream'] = df[\"Product\"].apply(contains_sentinel_value)\nfiltered_df = df[df['ice_cream']]\n\n# Step 8: Drop all columns except the timestamp column in the filtered DataFrame\nfiltered_df = filtered_df[[timestamp_column]]\n\n# Step 9: Define a new column in the filtered DataFrame that is set to the value 1\nfiltered_df['is_ice_cream'] = 1\n\n# Step 10: Set the index of the filtered DataFrame to the timestamp column\nfiltered_df.set_index(timestamp_column, inplace=True)\n\n# Step 11: Resample the DataFrame on the timestamp column and sum the new column\n# Assuming we want to sum by minute, you can change the frequency as needed\nresampled_df = filtered_df.resample('T').sum()\n\n# Display the final resampled DataFrame\nprint(resampled_df)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'ice_cream'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[1], line 34\n     32 # Step 7: Filter the DataFrame to rows that only contain the sentinel value\n     33 df['is_ice_cream'] = df[\"Product\"].apply(contains_sentinel_value)\n---&gt; 34 filtered_df = df[df['ice_cream']]\n     36 # Step 8: Drop all columns except the timestamp column in the filtered DataFrame\n     37 filtered_df = filtered_df[[timestamp_column]]\n\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile ~/Programming/change_utils/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'ice_cream'"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html",
    "href": "notebooks/daily_ice_cream_sales_exploration.html",
    "title": "Analysis Notebook",
    "section": "",
    "text": "The purpose of this notebook is to analyze the daily sales of ice creams at metro areas in the USA. Subsets corresponding to yearly sales are also profiled. Plots of auto-correlation and maximum daily sales per week are also provided.\nimport pandas as pd\nimport numpy as np\nfp = \"../data/daily_ice_cream_sales.csv\"\ndf = pd.read_csv(fp)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"log_daily_sales\"] = np.log(df[\"ice_cream_purchases\"])\ndf[\"Date\"].max()\n\nTimestamp('2024-05-18 00:00:00')\ndf[\"Date\"].min()\n\nTimestamp('2020-01-01 00:00:00')\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#extract-yearly-data-subsets",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#extract-yearly-data-subsets",
    "title": "Analysis Notebook",
    "section": "Extract yearly data subsets",
    "text": "Extract yearly data subsets\n\ndf_2020 = df[df.Date.dt.year == 2020]\ndf_2021 = df[df.Date.dt.year == 2021]\ndf_2022 = df[df.Date.dt.year == 2022]\ndf_2023 = df[df.Date.dt.year == 2023]\n\n\n%matplotlib inline\nimport statsmodels.api as sm\n\n\ndf_2020[\"log_daily_sales\"].plot()"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#plot-autocorrelation",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#plot-autocorrelation",
    "title": "Analysis Notebook",
    "section": "Plot Autocorrelation",
    "text": "Plot Autocorrelation\nThe auto correlation plots are shown below.\n\nsm.graphics.tsa.plot_acf(df_2020[\"log_daily_sales\"], lags=40)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsm.graphics.tsa.plot_pacf(df_2020[\"log_daily_sales\"], lags=40, method=\"ywm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsm.graphics.tsa.plot_pacf(df[\"log_daily_sales\"], lags=40, method=\"ywm\")"
  },
  {
    "objectID": "notebooks/daily_ice_cream_sales_exploration.html#observation",
    "href": "notebooks/daily_ice_cream_sales_exploration.html#observation",
    "title": "Analysis Notebook",
    "section": "Observation",
    "text": "Observation\nIt appears that daily sales are not correlated, but independent draws from a distribution. This looks like a white noise process. This is consistent with synthetically generated data. A dickey fuller test with no regression (constant, this is the default) also suggests the same.\n\nfrom statsmodels.tsa.stattools import adfuller\nresult = adfuller(df[\"log_daily_sales\"])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\n\nADF Statistic: -40.221392\np-value: 0.000000\n\n\n\ndf_2020\n\n\n\n\n\n\n\n\nDate\nice_cream_purchases\nlog_daily_sales\n\n\n\n\n0\n2020-01-01\n20\n2.995732\n\n\n1\n2020-01-02\n22\n3.091042\n\n\n2\n2020-01-03\n23\n3.135494\n\n\n3\n2020-01-04\n23\n3.135494\n\n\n4\n2020-01-05\n21\n3.044522\n\n\n...\n...\n...\n...\n\n\n361\n2020-12-27\n31\n3.433987\n\n\n362\n2020-12-28\n30\n3.401197\n\n\n363\n2020-12-29\n25\n3.218876\n\n\n364\n2020-12-30\n22\n3.091042\n\n\n365\n2020-12-31\n29\n3.367296\n\n\n\n\n366 rows × 3 columns\n\n\n\n\n\ndf[\"log_daily_sales\"] = np.log(df[\"ice_cream_purchases\"])\n\n\ndf_yearly_ice_cream_sales = df.set_index(\"Date\").resample(\"Y\").sum()\n\n\ndf_yearly_ice_cream_sales\n\n\n\n\n\n\n\n\nice_cream_purchases\nlog_daily_sales\n\n\nDate\n\n\n\n\n\n\n2020-12-31\n8488\n1142.662667\n\n\n2021-12-31\n8224\n1128.462655\n\n\n2022-12-31\n8206\n1127.813921\n\n\n2023-12-31\n8379\n1135.628790\n\n\n2024-12-31\n3188\n432.632764\n\n\n\n\n\n\n\n\ndf[\"ice_cream_purchases\"].plot()\n\n\n\n\n\n\n\n\n\nfpmeanweekly = \"../data/mean_weekly_ice_cream_sales.csv\"\ndfwmics = pd.read_csv(fpmeanweekly)\n\n\ndfwmics[\"ice_cream_purchases\"].plot()\n\n\n\n\n\n\n\n\n\ndfwmics[\"weekno\"] = range(1,dfwmics.shape[0] + 1)\n\n\ndfwmics\n\n\n\n\n\n\n\n\nyear\nweek\nice_cream_purchases\nweekno\n\n\n\n\n0\n2020\n1\n21.800000\n1\n\n\n1\n2020\n2\n21.857143\n2\n\n\n2\n2020\n3\n26.000000\n3\n\n\n3\n2020\n4\n26.142857\n4\n\n\n4\n2020\n5\n19.571429\n5\n\n\n...\n...\n...\n...\n...\n\n\n225\n2024\n16\n21.571429\n226\n\n\n226\n2024\n17\n23.000000\n227\n\n\n227\n2024\n18\n20.857143\n228\n\n\n228\n2024\n19\n23.285714\n229\n\n\n229\n2024\n20\n23.000000\n230\n\n\n\n\n230 rows × 4 columns"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "r2ds-blog",
    "section": "",
    "text": "Chat GPT and Data Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nRajiv Sambasivan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "For a while now I have been meaning to explore how Copilot and Chat GPT are useful for data analysis. I have also been meaning to transition to Quarto for a while, so I grabbed a dataset from Kaggle, picked some interesting questions to analyze, and threw it at Copilot and Chat GPT. Here’s my experience. I used Chat GPT to generate the content initially, so what you see here is my own writing interlaced with generated content. I used VSCode with extensions for quarto and copilot for this article.\nBy leveraging Chat GPT, we can automate and enhance various aspects of time series analysis, including data preprocessing, model selection, and interpretation of results.\n\n\n\n\n\n\nNote\n\n\n\nWhile the above statement is true, it can be somewhat misleading. The assistance we get for the tasks like data preprocessing in comparison to tasks like result interpretation and model selection is very different. Code generation for manipulating and transforming pandas dataframes was useful, I did’t really find anything useful for interpretation or model selection.\n\n\n\n\nThe data analyzed in this post is a collection of transactions from a national retailer. The transactions span major metropolitan cities in the US and cover a four and half year period. The dataset is available on Kaggle. The objective was to analyze this data to get answers to a representative set of questions. The questions addressed in this post represent a subset of a comprehensive analysis exercise, and it is not intended to be exhaustive or rigorous. Some example questions include:\n\nWhat does the plot of daily ice cream sales nationwide over the analysis period look like?\nHow do ice cream sales vary across the four years within the analysis period? Are there significant differences in the summary statistics year over year?\nIs there variability in ice cream sales based on location? In other words, is there heterogeneity in consumption patterns based on location?\nWhat does the autocorrelation function of the ice cream sales indicate? Is the time series stationary?\nWhat are the key components of the time series (trend, seasonality, etc.)?\nHow does the plot of the maximum daily ice cream sales for each week of the year look? Can we model these maximum values using a Gumbel distribution to understand the distribution of peak daily demand nationally?\n\n\n\n\n\n\n\nNote\n\n\n\nThe relevant analysis tasks (the list above) is something the analyst has to generate. In my view, this comes with experience with similar data and use cases. Asking Chat GPT on how to do time series analysis got me very general guidelines. I did not find the response very useful. A notebook implementation of most of these questions is available\n\n\n\n\n\nTransforming the raw dataset to a format that was suitable to answer the analysis questions required an understanding of the various fields in the data. To get the daily ice cream sales from the raw transaction data, I had the identify the transactions that contained ice creams and then aggregate the transactions on a daily basis. Doing this required an analysis of how the line items for each transaction is encoded. Once the data description was analyzed, providing the prompts to Chat GPT generated code that was pretty close to what I developed manually.\n\n\n\n\n\n\nNote\n\n\n\nThe real effort in the data preparation task went into developing an understanding of the various elements of the raw dataset and then coming up with the process of identifying the transactions containing ice cream. Aggregating these transactions to a daily or weekly cadence was fairly straight forward. Chat GPT was helpful in generating the specific pandas code for each step. The manual and the generated were pretty close.\n\n\n\n\n\n\nChat GPT was definitely very helpful in developing and organizing the content.\nIdentifying the key questions that are relevant to the analysis requires formal training and experience - I don’t really see this replacing a data scientist at this point.\nChat GPT was definitely useful for various code generation tasks.\nIn summary, it was impressive as an assistant, but at this point, it is an assistant not a data scientist replacement."
  },
  {
    "objectID": "posts/post-with-code/index.html#the-time-series-analysis-task",
    "href": "posts/post-with-code/index.html#the-time-series-analysis-task",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "The data analyzed in this post is a collection of transactions from a national retailer. The transactions span major metropolitan cities in the US and cover a four and half year period. The dataset is available on Kaggle. The objective was to analyze this data to get answers to a representative set of questions. The questions addressed in this post represent a subset of a comprehensive analysis exercise, and it is not intended to be exhaustive or rigorous. Some example questions include:\n\nWhat does the plot of daily ice cream sales nationwide over the analysis period look like?\nHow do ice cream sales vary across the four years within the analysis period? Are there significant differences in the summary statistics year over year?\nIs there variability in ice cream sales based on location? In other words, is there heterogeneity in consumption patterns based on location?\nWhat does the autocorrelation function of the ice cream sales indicate? Is the time series stationary?\nWhat are the key components of the time series (trend, seasonality, etc.)?\nHow does the plot of the maximum daily ice cream sales for each week of the year look? Can we model these maximum values using a Gumbel distribution to understand the distribution of peak daily demand nationally?\n\n\n\n\n\n\n\nNote\n\n\n\nThe relevant analysis tasks (the list above) is something the analyst has to generate. In my view, this comes with experience with similar data and use cases. Asking Chat GPT on how to do time series analysis got me very general guidelines. I did not find the response very useful. A notebook implementation of most of these questions is available"
  },
  {
    "objectID": "posts/post-with-code/index.html#data-preparation",
    "href": "posts/post-with-code/index.html#data-preparation",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "Transforming the raw dataset to a format that was suitable to answer the analysis questions required an understanding of the various fields in the data. To get the daily ice cream sales from the raw transaction data, I had the identify the transactions that contained ice creams and then aggregate the transactions on a daily basis. Doing this required an analysis of how the line items for each transaction is encoded. Once the data description was analyzed, providing the prompts to Chat GPT generated code that was pretty close to what I developed manually.\n\n\n\n\n\n\nNote\n\n\n\nThe real effort in the data preparation task went into developing an understanding of the various elements of the raw dataset and then coming up with the process of identifying the transactions containing ice cream. Aggregating these transactions to a daily or weekly cadence was fairly straight forward. Chat GPT was helpful in generating the specific pandas code for each step. The manual and the generated were pretty close."
  },
  {
    "objectID": "posts/post-with-code/index.html#summary-of-chat-gpts-utility-for-this-particular-exercise",
    "href": "posts/post-with-code/index.html#summary-of-chat-gpts-utility-for-this-particular-exercise",
    "title": "Chat GPT and Data Analysis",
    "section": "",
    "text": "Chat GPT was definitely very helpful in developing and organizing the content.\nIdentifying the key questions that are relevant to the analysis requires formal training and experience - I don’t really see this replacing a data scientist at this point.\nChat GPT was definitely useful for various code generation tasks.\nIn summary, it was impressive as an assistant, but at this point, it is an assistant not a data scientist replacement."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog, where I explore a variety of topics that pique my interest, including:\n\nAnalyzing, summarizing, and optimizing large datasets for business machine learning tasks.\nDeveloping discrete and continuous optimization models for business applications.\nLeveraging graph data models and algorithms to address business challenges.\nApplying statistical modeling techniques to solve business problems."
  },
  {
    "objectID": "notebooks/retail_transaction_data.html",
    "href": "notebooks/retail_transaction_data.html",
    "title": "Data Preparation Notebook",
    "section": "",
    "text": "The data for this notebook is from Kaggle. The data looks like it is synthetic, the characteristics observed in the dataset also seem to suggest that. The objectives for this notebook are to:\nimport pandas as pd\nfp = \"../data/Retail_Transactions_Dataset.csv\"\ndf = pd.read_csv(fp)\ndf.head()\n\n\n\n\n\n\n\n\nTransaction_ID\nDate\nCustomer_Name\nProduct\nTotal_Items\nTotal_Cost\nPayment_Method\nCity\nStore_Type\nDiscount_Applied\nCustomer_Category\nSeason\nPromotion\n\n\n\n\n0\n1000000000\n2022-01-21 06:27:29\nStacey Price\n['Ketchup', 'Shaving Cream', 'Light Bulbs']\n3\n71.65\nMobile Payment\nLos Angeles\nWarehouse Club\nTrue\nHomemaker\nWinter\nNone\n\n\n1\n1000000001\n2023-03-01 13:01:21\nMichelle Carlson\n['Ice Cream', 'Milk', 'Olive Oil', 'Bread', 'P...\n2\n25.93\nCash\nSan Francisco\nSpecialty Store\nTrue\nProfessional\nFall\nBOGO (Buy One Get One)\n\n\n2\n1000000002\n2024-03-21 15:37:04\nLisa Graves\n['Spinach']\n6\n41.49\nCredit Card\nHouston\nDepartment Store\nTrue\nProfessional\nWinter\nNone\n\n\n3\n1000000003\n2020-10-31 09:59:47\nMrs. Patricia May\n['Tissues', 'Mustard']\n1\n39.34\nMobile Payment\nChicago\nPharmacy\nTrue\nHomemaker\nSpring\nNone\n\n\n4\n1000000004\n2020-12-10 00:59:59\nSusan Mitchell\n['Dish Soap']\n10\n16.42\nDebit Card\nHouston\nSpecialty Store\nFalse\nYoung Adult\nWinter\nDiscount on Selected Items"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#profile-the-categorical-columns",
    "href": "notebooks/retail_transaction_data.html#profile-the-categorical-columns",
    "title": "Data Preparation Notebook",
    "section": "Profile the categorical columns",
    "text": "Profile the categorical columns\n\ndf.columns\n\nIndex(['Transaction_ID', 'Date', 'Customer_Name', 'Product', 'Total_Items',\n       'Total_Cost', 'Payment_Method', 'City', 'Store_Type',\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion'],\n      dtype='object')\n\n\n\ncategory_cols = ['Payment_Method', 'City', 'Store_Type',\\\n       'Discount_Applied', 'Customer_Category', 'Season', 'Promotion']\n\n\ndf[category_cols] = df[category_cols].astype('category')\n\n\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n\ndf.dtypes\n\nTransaction_ID                int64\nDate                 datetime64[ns]\nCustomer_Name                object\nProduct                      object\nTotal_Items                   int64\nTotal_Cost                  float64\nPayment_Method             category\nCity                       category\nStore_Type                 category\nDiscount_Applied           category\nCustomer_Category          category\nSeason                     category\nPromotion                  category\ndtype: object\n\n\n\ndf[\"Customer_Category\"].value_counts()\n\nSenior Citizen    125485\nHomemaker         125418\nTeenager          125319\nRetiree           125072\nStudent           124842\nProfessional      124651\nMiddle-Aged       124636\nYoung Adult       124577\nName: Customer_Category, dtype: int64\n\n\n\ndf[\"City\"].value_counts()\n\nBoston           100566\nDallas           100559\nSeattle          100167\nChicago          100059\nHouston          100050\nNew York         100007\nLos Angeles       99879\nMiami             99839\nSan Francisco     99808\nAtlanta           99066\nName: City, dtype: int64"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#observation",
    "href": "notebooks/retail_transaction_data.html#observation",
    "title": "Data Preparation Notebook",
    "section": "Observation",
    "text": "Observation\nThe counts for each of the metro areas are very similar, the counts for each of the customer categories are very similar, so this dataset was probably synthetically generated.\n\ndf[\"Product\"] = df[\"Product\"].apply(eval)\n\n\npurchase_summ = {}\nfor index, row in df[\"Product\"].items():\n    for p in row:\n        if p in purchase_summ:\n            purchase_summ[p] += 1\n        else:\n            purchase_summ[p] = 1"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#extract-the-ice-cream-transactions",
    "href": "notebooks/retail_transaction_data.html#extract-the-ice-cream-transactions",
    "title": "Data Preparation Notebook",
    "section": "Extract the ice cream transactions",
    "text": "Extract the ice cream transactions\n\ndef is_ice_cream(row):\n    for p in row:\n        if p == \"Ice Cream\":\n            return True\n    return False\ndf[\"is_ice_cream\"] = df[\"Product\"].apply(is_ice_cream)\n\n\ndf_ice_cream_trans = df[df[\"is_ice_cream\"]].reset_index()\nreq_cols = [\"Date\"]\ndf_ice_cream_trans = df_ice_cream_trans[req_cols]\n\n\ndf_ice_cream_trans[\"ice_cream_purchases\"] = 1\n\n\ndf_daily_ice_cream_sales = df_ice_cream_trans.set_index(\"Date\").resample(\"D\").sum()\n\n\ndf_weekly_max_ice_cream_sales = df_daily_ice_cream_sales.assign(year=df_daily_ice_cream_sales.index.year,\\\n                                week = df_daily_ice_cream_sales.index.isocalendar().week).groupby(['year','week']).max()\n\n\ndf_weekly_mean_ice_cream_sales = df_daily_ice_cream_sales.assign(year=df_daily_ice_cream_sales.index.year,\\\n                                week = df_daily_ice_cream_sales.index.isocalendar().week).groupby(['year','week']).mean()\n\n\ndf_weekly_max_ice_cream_sales = pd.DataFrame(df_weekly_max_ice_cream_sales.to_records()) \n\n\ndf_weekly_max_ice_cream_sales\n\n\n\n\n\n\n\n\nyear\nweek\nice_cream_purchases\n\n\n\n\n0\n2020\n1\n23\n\n\n1\n2020\n2\n26\n\n\n2\n2020\n3\n36\n\n\n3\n2020\n4\n31\n\n\n4\n2020\n5\n23\n\n\n...\n...\n...\n...\n\n\n225\n2024\n16\n33\n\n\n226\n2024\n17\n32\n\n\n227\n2024\n18\n30\n\n\n228\n2024\n19\n30\n\n\n229\n2024\n20\n30\n\n\n\n\n230 rows × 3 columns\n\n\n\n\ndf_weekly_mean_ice_cream_sales\n\n\n\n\n\n\n\n\n\nice_cream_purchases\n\n\nyear\nweek\n\n\n\n\n\n2020\n1\n21.800000\n\n\n2\n21.857143\n\n\n3\n26.000000\n\n\n4\n26.142857\n\n\n5\n19.571429\n\n\n...\n...\n...\n\n\n2024\n16\n21.571429\n\n\n17\n23.000000\n\n\n18\n20.857143\n\n\n19\n23.285714\n\n\n20\n23.000000\n\n\n\n\n230 rows × 1 columns"
  },
  {
    "objectID": "notebooks/retail_transaction_data.html#write-the-extracted-files-for-further-analysis",
    "href": "notebooks/retail_transaction_data.html#write-the-extracted-files-for-further-analysis",
    "title": "Data Preparation Notebook",
    "section": "Write the extracted files for further analysis",
    "text": "Write the extracted files for further analysis\n\nfpdaily = \"../data/daily_ice_cream_sales.csv\"\nfpmaxweekly = \"../data/max_weekly_ice_cream_sales.csv\"\nfpmeanweekly = \"../data/mean_weekly_ice_cream_sales.csv\"\ndf_daily_ice_cream_sales.to_csv(fpdaily, index=True)\ndf_weekly_max_ice_cream_sales.to_csv(fpmaxweekly, index=True)\ndf_weekly_mean_ice_cream_sales.to_csv(fpmeanweekly, index=True)"
  }
]